{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"AA5qxs9gnKoK"},"outputs":[{"ename":"ValueError","evalue":"mount failed","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--> 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wh58CzmIoPFX"},"outputs":[],"source":["%cd /content/drive/Othercomputers/Laptop/FINANCIAL_INFORMATION\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92035,"status":"ok","timestamp":1723206349354,"user":{"displayName":"임병완","userId":"16912505565738020902"},"user_tz":-540},"id":"3Pkt2FJlyH3P","outputId":"6127e9a3-442c-443e-a59e-c03bd0ddbdce"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.0/384.0 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.2/140.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q -r requirements.txt\n","!pip install --upgrade -q transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"_IwakG22opuw","outputId":"e357609a-b543-4fee-f0e3-e651f233ba19"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c76c1aca212477a878754b7d415929e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()\n","\n","# huggingface-cli login"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1232528,"status":"ok","timestamp":1723208154363,"user":{"displayName":"임병완","userId":"16912505565738020902"},"user_tz":-540},"id":"N4LuV0UGnMja","outputId":"eb64506e-c456-4253-d153-e1f7403fd526"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./train_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]},{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./test_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]},{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./fewshot_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","Loading checkpoint shards: 100%|██████████| 4/4 [00:58<00:00, 14.67s/it]\n","  0%|          | 0/98 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","Question:  2022년 혁신창업사업화자금(융자)의 예산은 얼마인가요?\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n","  1%|          | 1/98 [00:05<08:35,  5.31s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  2022년 혁신창업사업화자금(융자)의 예산은 2,007,800 백만원입니다.\n","\n","Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 무엇인가요?\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 2/98 [00:12<10:24,  6.50s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  중소벤처기업부의 혁신창업사업화자금(융자)의 사업목적은 기술력과 사업성이 우수하고 미래 성장 가능성이 높은 중소벤처기업의 창업을 활성화하고 고용 창출을 도모하는 것입니다.\n","\n","Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업근거는 어떤 법률에 근거하고 있나요?\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 3/98 [00:19<10:30,  6.63s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  중소벤처기업부의 혁신창업사업화자금(융자)의 사업근거는 중소기업진흥에 관한 법률 제66조, 제67조, 제74조와 중소기업창업지원법 제35조에 근거하고 있습니다.\n","\n","Question:  2010년에 신규 지원된 혁신창업사업화자금은 무엇인가요?\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 4/98 [00:23<08:29,  5.42s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  2010년에 신규 지원된 혁신창업사업화자금은 2,300,000 백만원입니다.\n","\n","Question:  혁신창업사업화자금 중 2020년에 신규 지원된 자금은 무엇인가요?\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 5/98 [00:27<07:52,  5.08s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  혁신창업사업화자금 중 2020년에 신규 지원된 자금은 2,007,800 백만원입니다.\n","\n","Question:  재창업자금이 재도약지원자금으로 이관된 연도는 언제인가요?\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▌         | 6/98 [00:30<06:58,  4.55s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  재창업자금이 재도약지원자금으로 이관된 연도는 2015년입니다.\n","\n","Question:  창업기반지원과 신청 대상이 중복인 자금이 어떤 것이며, 이 자금이 폐지된 연도는 언제인가요?\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.77\n","  warnings.warn(\n","  7%|▋         | 7/98 [00:35<06:41,  4.42s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  창업기반지원과 신청 대상이 중복인 자금은 '창업보육자금'입니다. 이 자금은 2020년부터 폐지되었습니다.\n","\n","Question:  혁신창업사업화자금(융자) 사업을 시행하는 주체는 누구인가요?\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 8/98 [00:39<06:42,  4.47s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  혁신창업사업화자금(융자) 사업을 시행하는 주체는 중소벤처기업진흥공단입니다.\n","\n","Question:  혁신창업사업화자금(융자) 사업 집행절차는 어떻게 되나요?\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 9/98 [00:54<11:25,  7.71s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  혁신창업사업화자금(융자)의 사업 집행절차는 다음과 같습니다.\n","\n","1. 사업 계획 수립 및 공고를 거쳐 사전 상담 및 신청, 서류 및 현장 실사를 통해 지원 여부를 결정합니다.\n","2. 지원된 기업에 대한 평가와 승인을 거친 후 융자를 제공합니다.\n","\n","즉, 혁신창업사업화자금(융자의) 사업 집행 절차는 중소기업이 사업 계획을 제출하고, 중소기업진흥공단에서 사전 상담 및 신청, 서류 및 현장 실사를 통해 지원 여부를 결정하고, 이후 평가와 승인 과정을 거쳐 융자를 제공하는 것입니다.\n","\n","Question:  부모급여 지원 사업의 목적은 무엇인가요?\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 10/98 [01:01<10:49,  7.38s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"name":"stdout","output_type":"stream","text":["Answer:  부모급여 지원 사업의 목적은 출산 및 양육으로 손실되는 소득을 보전하고 주 양육자의 직접 돌봄이 중요한 아동 발달의 특성에 따라 영아기 돌봄을 두텁게 지원하기 위해서입니다.\n","\n","Question:  부모급여(영아수당)의 2024년 확정된 예산은 몇백만원인가요?\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 11/98 [01:05<09:08,  6.31s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  '24년 부모급여(영아수당)의 예산은 2888억원입니다.\n","\n","Question:  부모급여 지원 사업은 어떤 법령상 근거를 갖고 추진되고 있나요?\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.77\n","  warnings.warn(\n"," 12%|█▏        | 12/98 [01:07<07:27,  5.20s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  부모급여 지원 사업은 '가정의 요양에 관한 법률'에 근거하고 있습니다.\n","\n","Question:  영아수당 도입에 대한 추진경위는 어떻게 되나요?\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 13/98 [01:22<11:40,  8.24s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  영아수당 도입에 대한 추진 경위는 다음과 같습니다.\n","\n","'제4차 저출산‧고령사회 기본계획'의 5대 핵심 과제 중 하나로 영아수당 도입이 추진되었습니다. 예비 타당성 조사 결과 통과 후 근거 법률인 아동 수당법이 2021년 12월에 제정되었으며, 2022년 1월부터 영아 수당 지원 사업이 시작되었습니다. 또한, '부모 급여 도입'이 대통령 공약 사항이자 국정 과제로 포함되어 2024년부터 0∼11개월 아동에게 월 100만원 부모 급여를 지급하기로 결정되었습니다.\n","\n","Question:  부모급여 지원사업은 언제부터 시행되었나요?\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.77\n","  warnings.warn(\n"," 14%|█▍        | 14/98 [01:25<08:56,  6.38s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  부모급여 지원사업은 2008년부터 시행되었습니다.\n","\n","Question:  보건복지부의 부모급여(영아수당) 지원 사업시행방법은 무엇이며, 사업 수혜자는 누구인가?\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 15/98 [01:33<09:52,  7.14s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  부모급여(영아수당) 지원 사업은 출산 및 양육으로 손실되는 소득을 보전하고 주 양육자의 직접 돌봄이 중요한 아동 발달의 특성에 따라 영아기 돌봄을 두텁게 지원하기 위해 부모급여를 지급하는 것입니다. 사업 수혜자는 아동을 양육 중인 부모입니다.\n","\n","Question:  노인장기요양보험 사업 운영에 대한 목적은 무엇인가요?\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▋        | 16/98 [01:41<09:53,  7.24s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  노인장기요양보험 사업 운영의 목적은 어려운 노인에게 신체 또는 가사 활동 등을 제공하여 효율적인 정책 추진으로 노후의 건강 증진과 생활 안정을 도모하고 가족의 부담을 완화하여 국민 삶의 질을 향상시키는 것입니다.\n","\n","Question:  노인장기요양보험 운영지원에 대한 사업 내용을 설명해줘.\n"]},{"name":"stderr","output_type":"stream","text":[" 17%|█▋        | 17/98 [01:51<10:54,  8.08s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  노인장기요양보험 운영지원은 고령이나 노인성 질병으로 일상생활을 혼자서 수행하기 어려운 노인에게 신체 또는 가사 활동 등을 제공하는 노인장기요양보험에 국고를 지원하여 효율적인 정책 추진으로 노후의 건강증진과 생활 안정을 도모하고 가족의 부담을 완화하여 국민 삶의 질을 향상시키는 목적으로 진행됩니다.\n","\n","Question:  국고지원을 받는 기타 의료급여수급권자는 누구인가요?\n"]},{"name":"stderr","output_type":"stream","text":[" 18%|█▊        | 18/98 [01:56<09:37,  7.22s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  의료급여수급권자는 이재민, 의사상자, 국가유공자, 입양아동, 국가무형문화재보유자, 북한이탈주민 등입니다.\n","\n","Question:  장기요양보험가입자 및 피부양자의 자격취득과 관련하여 어떤 법률을 준용해야 하는가?\n"]},{"name":"stderr","output_type":"stream","text":[" 19%|█▉        | 19/98 [02:02<08:49,  6.70s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  장기요양보험가입자 및 피부양자의 자격취득과 관련하여 준용해야 하는 법률은 노인장기요양보험법 제11조입니다.\n","\n","Question:  노인장기요양보험법이 언제 제정되고 공포되었나?\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.77\n","  warnings.warn(\n"," 20%|██        | 20/98 [02:05<07:33,  5.82s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  노인장기요양보험법은 2008년 2월 29일 제정되어 같은 해 6월 30일에 공포되었습니다.\n","\n","Question:  장기요양인정점수 완화가 언제 이루어졌으며, 어떤 변화가 있었나?\n"]},{"name":"stderr","output_type":"stream","text":[" 21%|██▏       | 21/98 [02:16<09:09,  7.13s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  장기요양인정점수 완화는 '12년 7월에 이루어졌습니다. 그 때, 3등급의 인정점수가 완화되었습니다. 기존 55∼75점에서 53점∼75점으로 완화되었으며, 또 다른 완화가 '13년 7월에 이루어졌을 때 51점∼75점으로 다시 완화되었습니다.\n","\n","Question:  장기요양기관 지정갱신제의 법적 근거가 언제 마련되었는가?\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:796: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.77\n","  warnings.warn(\n"," 22%|██▏       | 22/98 [02:20<07:48,  6.17s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  장기요양기관 지정갱신제의 법적 근거는 2018년 장기요양법이 개정되어 2019년에 시행된 것입니다.\n","\n","Question:  22.10월에 요양보호사 1명당 시설수급자 인력배치기준이 개선된 내용은 무엇인가?\n"]},{"name":"stderr","output_type":"stream","text":[" 23%|██▎       | 23/98 [02:24<07:13,  5.78s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  2022년 10월에 개정된 요양보호사 1명당 시설 수급자 인력 배치 기준은 2.3명으로 변경되었습니다.\n","\n","Question:  에너지 바우처 제도의 주요 내용은 무엇인가요?\n"]},{"name":"stderr","output_type":"stream","text":[" 24%|██▍       | 24/98 [02:32<07:38,  6.20s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  에너지 바우처 제도의 주요 내용은 경제적 부담 등으로 에너지 이용에 어려움을 겪는 에너지 소외 계층에게 전기, 가스, 지역난방 등 에너지 이용에 필요한 비용을 지원하는 것입니다.\n","\n","Question:  에너지바우처 사업의 주요 수혜자는 누구인가요?\n"]},{"name":"stderr","output_type":"stream","text":[" 26%|██▌       | 25/98 [02:37<07:13,  5.94s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  에너지바우처 사업의 주요 수혜자는 경제적 부담 등으로 에너지 이용에 어려움을 겪는 에너지 소외 계층입니다.\n","\n","Question:  2024년 에너지바우처 사업의 사업시행주체는 무엇인가요?\n"]},{"name":"stderr","output_type":"stream","text":[" 27%|██▋       | 26/98 [02:41<06:21,  5.30s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  에너지바우처 사업의 사업시행주체는 국무총리입니다.\n","\n","Question:  하절기바우처와 동절기바우처의 2024년 예산 규모는 각각 얼마인가요?\n"]},{"name":"stderr","output_type":"stream","text":[" 28%|██▊       | 27/98 [02:47<06:32,  5.53s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  하절기바우처의 2024년 예산은 60,950 백만원입니다. 동절기바우처의 2024년 예산은 600,521 백만원입니다.\n","\n","Question:  2023년 에너지바우처 사업 예산에서 사업운영비 중 에너지복지 홍보에 얼마가 할당되었나요?\n"]},{"name":"stderr","output_type":"stream","text":[" 29%|██▊       | 28/98 [02:51<06:01,  5.17s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  '23년 사업운영비 에서 에너지복지 홍보 비용은 448백만원입니다.\n","\n","Question:  2023년 에너지바우처 사업 예산에서 사업운영비 중 시스템 고도화에 얼마가 할당되었나요?\n"]},{"name":"stderr","output_type":"stream","text":[" 30%|██▉       | 29/98 [02:56<05:52,  5.10s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  제목: 2023년 에너지바우처 사업 예산에서 시스템 고도화 비용은 705 백만원입니다.\n","\n","Question:  2023년 에너지바우처 사업 예산에서 콜센터 운영에 얼마가 할당되었나요?\n"]},{"name":"stderr","output_type":"stream","text":[" 31%|███       | 30/98 [03:01<05:40,  5.00s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  '마. 사업운영비' 항목 중 '콜센터 운영'에 대한 예산은 502.7 백만원입니다.\n","\n","Question:  2023년 에너지바우처 사업 예산에서 패널조사에 얼마가 할당되었나요?\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 31/98 [03:06<05:35,  5.01s/it]"]},{"name":"stdout","output_type":"stream","text":["Answer:  패널조사는 '마' 항목 중에 포함되어 있습니다. 따라서 133.5 백만원이 할당되었습니다.\n","\n","Question:  2023년 에너지바우처 사업 예산에서 에너지바우처 전달체계 구축에 얼마가 할당되었나요?\n"]},{"name":"stderr","output_type":"stream","text":[" 32%|███▏      | 31/98 [03:08<06:47,  6.09s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m llm \u001b[38;5;241m=\u001b[39m setup_llm_pipeline(model_id)\n\u001b[1;32m---> 92\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfewshot_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfewshot_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m save(results)\n","Cell \u001b[1;32mIn[2], line 57\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(train_db, test_db, fewshot_db, dataset, llm, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;124m\"\u001b[39m,dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 57\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQuestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m answer \u001b[38;5;241m=\u001b[39m extract_answer(answer)\n\u001b[0;32m     59\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer\u001b[39m\u001b[38;5;124m\"\u001b[39m: answer,\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m\"\u001b[39m: dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     63\u001b[0m     })\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2878\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2876\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2877\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2878\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2879\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:344\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    342\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 344\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    355\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    356\u001b[0m     )\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:701\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    693\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    694\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    695\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    699\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    700\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 701\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:880\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    866\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    867\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    868\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m         )\n\u001b[0;32m    879\u001b[0m     ]\n\u001b[1;32m--> 880\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    883\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:738\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    737\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 738\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    739\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    740\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:725\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    717\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    722\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    723\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    724\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 725\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    731\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    732\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    733\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    734\u001b[0m         )\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    736\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_pipeline.py:269\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m batch_prompts \u001b[38;5;241m=\u001b[39m prompts[i : i \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size]\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[1;32m--> 269\u001b[0m responses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:262\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[1;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\pipelines\\base.py:1238\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m   1235\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   1236\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[0;32m   1237\u001b[0m     )\n\u001b[1;32m-> 1238\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\pipelines\\base.py:1164\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[1;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[0;32m   1162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[0;32m   1163\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m-> 1164\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1165\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:351\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[1;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m         generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m prefix_length\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[1;32m--> 351\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    352\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\generation\\utils.py:2024\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2016\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2017\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2018\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2019\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2020\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2021\u001b[0m     )\n\u001b[0;32m   2023\u001b[0m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2025\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2027\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2030\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2031\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2032\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2033\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2035\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2036\u001b[0m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m     prepared_logits_warper \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_logits_warper(generation_config, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2039\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mdo_sample\n\u001b[0;32m   2040\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2041\u001b[0m     )\n","File \u001b[1;32mc:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\generation\\utils.py:3020\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[0m\n\u001b[0;32m   3018\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   3019\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[1;32m-> 3020\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3022\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tqdm import tqdm\n","from langchain.prompts import PromptTemplate\n","import pandas as pd\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","from faiss_module_bw import  make_db, make_fewshot_db\n","from model import setup_llm_pipeline\n","from fewshot_module import fewshot_ex\n","from save_module import save\n","from seed_module import seed_everything\n","from utils_module import make_dict, extract_answer, format_docs\n","seed_everything(52)\n","\n","def run(train_db,test_db,fewshot_db, dataset ,llm, verbose = False):\n","    # reordering = LongContextReorder()\n","    results =[]\n","    for i in tqdm(range(len(dataset))):\n","        test_retriver = test_db.as_retriever(search_type=\"similarity_score_threshold\",\n","                search_kwargs={'score_threshold': 0.77,'k':2})\n","        # train_retriever가 있으면 context를 포함한 fewshot prompt 생성\n","        # 없으면 fewshot prompt만 생성\n","        # fewshot_str = fewshot_ex(fewshot_db, dataset[i],train_db= train_db, fewshot_num = 3)\n","        #print(fewshot_str)\n","        full_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","You are the financial expert who helps me with my financial information Q&As.\n","You earn 10 points when you answer me and follow the rules and lose 7 points when you don't.\n","\n","12,500 백만원 = 125 억원 = 12,500,000,000 원\n","5,400 백만원 = 54 억원 = 5,400,000,000 원\n","\n","Here are some rules you should follow.\n","- Please use contexts to answer the question.\n","- Please your answers should be concise.\n","- Please answers must be written in Korean.\n","- Please answer the question in 1-3 sentences.\n","\n","- Use the three examples below to learn how to follow the rules and reference information in context.<|eot_id|>\n","\"\"\" +\"\"\"\n","<|start_header_id|>user<|end_header_id|>\n","Question: {input}\\n\\nContext\\n{context}<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\\n\n","\"\"\"\n","        prompt = PromptTemplate.from_template(full_template)\n","        qa_chain = (\n","        {\n","            \"context\": test_retriver | format_docs,\n","            \"input\": RunnablePassthrough(),\n","        }\n","        | prompt\n","        | llm\n","        | StrOutputParser()\n","        )\n","        # print(\"================================================\")\n","        if verbose:\n","            print(\"\\nQuestion: \",dataset[i]['Question'])\n","        answer = qa_chain.invoke(dataset[i]['Question'])\n","        answer = extract_answer(answer)\n","        results.append({\n","            \"Question\": dataset[i]['Question'],\n","            \"Answer\": answer,\n","            \"Source\": dataset[i]['Source']\n","            })\n","        if verbose:\n","            print(\"Answer: \",results[-1]['Answer'])\n","        #print(results[-1]['Source'])\n","    return results\n","    \n","    \n","if __name__ == \"__main__\":\n","    # EleutherAI/polyglot-ko-1.3b\n","    #\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","    # maywell/TinyWand-kiqu\n","    # yanolja/EEVE-Korean-Instruct-2.8B-v1.0\n","    # MLP-KTLim/llama-3-Korean-Bllossom-8B\n","    \n","    # train에도 RAG를 쓸 때 사용\n","    train_df = pd.read_csv('train.csv')\n","    train_db = make_db(train_df, './train_faiss_db')\n","\n","    # train_dict = make_dict('train.csv')\n","\n","    test_df = pd.read_csv('test.csv')\n","    test_db = make_db(test_df, './test_faiss_db')\n","\n","    test_dict = make_dict('test.csv')\n","    \n","    fewshot_db = make_fewshot_db(train_df, './fewshot_faiss_db')\n","    model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","    llm = setup_llm_pipeline(model_id)\n","    \n","    results = run(train_db= train_db,\n","        test_db= test_db,\n","        fewshot_db=fewshot_db, \n","        dataset= test_df.to_dict(orient='records') ,\n","        llm=llm,\n","        verbose=True)\n","    save(results)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","Loading checkpoint shards: 100%|██████████| 4/4 [00:59<00:00, 14.84s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./train_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]},{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./test_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]},{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./fewshot_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","  0%|          | 0/98 [00:00<?, ?it/s]\n"]},{"ename":"NameError","evalue":"name 'defaultdict' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 204\u001b[0m\n\u001b[0;32m    200\u001b[0m test_dict \u001b[38;5;241m=\u001b[39m make_dict(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    202\u001b[0m fewshot_db \u001b[38;5;241m=\u001b[39m make_fewshot_db(train_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./fewshot_faiss_db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 204\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_db\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfewshot_db\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfewshot_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43morient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    211\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m save(results)\n","Cell \u001b[1;32mIn[1], line 154\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(train_db, test_db, fewshot_db, dataset, llm, verbose)\u001b[0m\n\u001b[0;32m    151\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(full_template)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Use ensemble retriever\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m common_results \u001b[38;5;241m=\u001b[39m \u001b[43mperform_search_and_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mQuestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_retrivers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# Flatten the list of results to make searching for documents easier\u001b[39;00m\n\u001b[0;32m    157\u001b[0m all_results \u001b[38;5;241m=\u001b[39m [doc \u001b[38;5;28;01mfor\u001b[39;00m retriever \u001b[38;5;129;01min\u001b[39;00m test_retrivers \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m retriever\u001b[38;5;241m.\u001b[39minvoke(dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m])]\n","Cell \u001b[1;32mIn[1], line 42\u001b[0m, in \u001b[0;36mperform_search_and_filter\u001b[1;34m(query, retrievers)\u001b[0m\n\u001b[0;32m     39\u001b[0m     result_metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mfrozenset\u001b[39m(extract_doc_metadata(doc)\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m results}\n\u001b[0;32m     40\u001b[0m     search_results\u001b[38;5;241m.\u001b[39mappend(result_metadata)\n\u001b[1;32m---> 42\u001b[0m common_results \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_common_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m common_results\n","Cell \u001b[1;32mIn[1], line 56\u001b[0m, in \u001b[0;36mfilter_common_results\u001b[1;34m(results_list)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_common_results\u001b[39m(results_list):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m    Filter results to get common documents that appear in at least 2 search methods.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m        set: Set of documents that appear in at least 2 search methods.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     result_count \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Count occurrences of each document metadata\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m result_set \u001b[38;5;129;01min\u001b[39;00m results_list:\n","\u001b[1;31mNameError\u001b[0m: name 'defaultdict' is not defined"]}],"source":["from tqdm import tqdm\n","from langchain.prompts import PromptTemplate\n","import pandas as pd\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","\n","from faiss_module_bw import  make_db, make_fewshot_db, normalize_string\n","from model import setup_llm_pipeline\n","from fewshot_module import fewshot_ex\n","from save_module import save\n","from seed_module import seed_everything\n","from utils_module import make_dict, extract_answer, format_docs\n","seed_everything(52)\n","\n","def run(train_db,test_db,fewshot_db, dataset ,llm, verbose = False):\n","    # reordering = LongContextReorder()\n","    results =[]\n","    for i in tqdm(range(len(dataset))):\n","        test_retriver = test_db.as_retriever(search_type=\"similarity_score_threshold\",\n","                search_kwargs={'score_threshold': 0.77,'k':2})\n","        # train_retriever가 있으면 context를 포함한 fewshot prompt 생성\n","        # 없으면 fewshot prompt만 생성\n","        # fewshot_str = fewshot_ex(fewshot_db, dataset[i],train_db= train_db, fewshot_num = 3)\n","        #print(fewshot_str)\n","        full_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","You are the financial expert who helps me with my financial information Q&As.\n","You earn 10 points when you answer me and follow the rules and lose 7 points when you don't.\n","\n","12,500 백만원 = 125 억원 = 12,500,000,000 원\n","5,400 백만원 = 54 억원 = 5,400,000,000 원\n","\n","Here are some rules you should follow.\n","- Please use contexts to answer the question.\n","- Please your answers should be concise.\n","- Please answers must be written in Korean.\n","- Please answer the question in 1-3 sentences.\n","\n","- Use the three examples below to learn how to follow the rules and reference information in context.<|eot_id|>\n","\"\"\" +\"\"\"\n","<|start_header_id|>user<|end_header_id|>\n","Question: {input}\\n\\nContext\\n{context}<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\\n\n","\"\"\"\n","        prompt = PromptTemplate.from_template(full_template)\n","        qa_chain = (\n","        {\n","            \"context\": test_retriver | format_docs,\n","            \"input\": RunnablePassthrough(),\n","        }\n","        | prompt\n","        | llm\n","        | StrOutputParser()\n","        )\n","        # print(\"================================================\")\n","        if verbose:\n","            print(\"\\nQuestion: \",dataset[i]['Question'])\n","        answer = qa_chain.invoke(normalize_string(dataset[i]['Question']))\n","        answer = extract_answer(answer)\n","        results.append({\n","            \"Question\": dataset[i]['Question'],\n","            \"Answer\": answer,\n","            \"Source\": dataset[i]['Source']\n","            })\n","        if verbose:\n","            print(\"Answer: \",results[-1]['Answer'])\n","        #print(results[-1]['Source'])\n","    return results\n","    \n","    \n","if __name__ == \"__main__\":\n","    # EleutherAI/polyglot-ko-1.3b\n","    #\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","    # maywell/TinyWand-kiqu\n","    # yanolja/EEVE-Korean-Instruct-2.8B-v1.0\n","    # MLP-KTLim/llama-3-Korean-Bllossom-8B\n","    \n","    # train에도 RAG를 쓸 때 사용\n","    train_df = pd.read_csv('train.csv')\n","    train_db = make_db(train_df, './train_faiss_db')\n","\n","    # train_dict = make_dict('train.csv')\n","\n","    test_df = pd.read_csv('test.csv')\n","    test_db = make_db(test_df, './test_faiss_db')\n","\n","    test_dict = make_dict('test.csv')\n","    \n","    fewshot_db = make_fewshot_db(train_df, './fewshot_faiss_db')\n","    model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","    llm = setup_llm_pipeline(model_id)\n","    \n","    results = run(train_db= train_db,\n","        test_db= test_db,\n","        fewshot_db=fewshot_db, \n","        dataset= test_df.to_dict(orient='records') ,\n","        llm=llm,\n","        verbose=True)\n","    save(results)"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1228,"status":"ok","timestamp":1723202143989,"user":{"displayName":"임병완","userId":"16912505565738020902"},"user_tz":-540},"id":"1T0zfgMk2JGf","outputId":"6174a66b-d1c4-4495-8798-e5301f57fe62"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./train_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]},{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./test_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n"]},{"name":"stdout","output_type":"stream","text":["Loading FAISS DB from: ./fewshot_faiss_db\n"]},{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  return torch.load(io.BytesIO(b))\n","Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]}],"source":["\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","\n","pred_file = './submission_bw/sub_8_9_1111.csv'\n","df = pd.read_csv(pred_file)\n","pred = df['Answer']\n","\n","# gt_file = './submission/baseline_submission_8_4_18.csv'\n","gt_file = './submission/sub_8_9_056.csv'\n","df = pd.read_csv(gt_file)\n","gt = df['Answer']\n","\n","def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n","\n","    #공백 제거\n","    true_sentence = ''.join(true_sentence.split())\n","    predicted_sentence = ''.join(predicted_sentence.split())\n","\n","    true_counter = Counter(true_sentence)\n","    predicted_counter = Counter(predicted_sentence)\n","\n","    #문자가 등장한 개수도 고려\n","    if sum_mode:\n","        true_positive = sum((true_counter & predicted_counter).values())\n","        predicted_positive = sum(predicted_counter.values())\n","        actual_positive = sum(true_counter.values())\n","\n","    #문자 자체가 있는 것에 focus를 맞춤\n","    else:\n","        true_positive = len((true_counter & predicted_counter).values())\n","        predicted_positive = len(predicted_counter.values())\n","        actual_positive = len(true_counter.values())\n","\n","    #f1 score 계산\n","    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n","    recall = true_positive / actual_positive if actual_positive > 0 else 0\n","    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    return precision, recall, f1_score\n","\n","def calculate_average_f1_score(true_sentences, predicted_sentences):\n","\n","    total_precision = 0\n","    total_recall = 0\n","    total_f1_score = 0\n","\n","    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n","        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1_score += f1_score\n","\n","    avg_precision = total_precision / len(true_sentences)\n","    avg_recall = total_recall / len(true_sentences)\n","    avg_f1_score = total_f1_score / len(true_sentences)\n","\n","    return {\n","        'average_precision': avg_precision,\n","        'average_recall': avg_recall,\n","        'average_f1_score': avg_f1_score\n","    }\n","\n","result = calculate_average_f1_score(gt, pred)\n","print(result)\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"11b9df3156614598ad494595decc9451":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e3105e8aeab4c6aa1f557c8ab33bc5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"356c69478da84eb19ba6d9f1b95b9cd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db285f1ffca147bca599525ec4b84426","placeholder":"​","style":"IPY_MODEL_1e3105e8aeab4c6aa1f557c8ab33bc5a","value":" 4/4 [01:21&lt;00:00, 17.41s/it]"}},"41f1c2e21f324fbb80915dd31c44df5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_562a4a50e06745c28be71afbbe0e5581","placeholder":"​","style":"IPY_MODEL_e75a25f017ec4f3881b8fa1a766efb3d","value":"Loading checkpoint shards: 100%"}},"44114d90f75d4ae2ac8394570b0c10f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41f1c2e21f324fbb80915dd31c44df5e","IPY_MODEL_8c2564a89439454592d8880899e66cf9","IPY_MODEL_356c69478da84eb19ba6d9f1b95b9cd1"],"layout":"IPY_MODEL_11b9df3156614598ad494595decc9451"}},"562a4a50e06745c28be71afbbe0e5581":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8c2564a89439454592d8880899e66cf9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a30b22dff03c4366a6e196f5cf7361e6","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cdd9f38ffe0c459cb461606db6754365","value":4}},"a30b22dff03c4366a6e196f5cf7361e6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cdd9f38ffe0c459cb461606db6754365":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"db285f1ffca147bca599525ec4b84426":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e75a25f017ec4f3881b8fa1a766efb3d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
