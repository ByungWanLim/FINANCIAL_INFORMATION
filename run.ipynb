{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Vector stores\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "# etc\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='intfloat/multilingual-e5-small',\n",
    "        model_kwargs={'device': 'mps'},\n",
    "        encode_kwargs={'normalize_embeddings': True})\n",
    "    return embeddings\n",
    "\n",
    "def normalize_string(s):\n",
    "    return unicodedata.normalize('NFC', s)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        #context += f\"Document {i+1}\\n\"\n",
    "        context += doc.page_content\n",
    "        context += '\\n\\n'\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fewshot_db(df):\n",
    "    df = df.drop('SAMPLE_ID', axis=1)\n",
    "    df = df.drop('Source_path', axis=1)\n",
    "    df = df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", df[:1])\n",
    "    to_vectorize = [\"\\n\\n\".join(normalize_string(value) for value in example.values()) for example in df]\n",
    "    \n",
    "    faiss = FAISS.from_texts(to_vectorize, embedding=get_embedding())\n",
    "    bm = BM25Retriever.from_texts(to_vectorize)\n",
    "    knn = KNNRetriever.from_texts(to_vectorize, embeddings=get_embedding())\n",
    "    return faiss, bm, knn\n",
    "    \n",
    "def make_db(df):\n",
    "    # Create a new FAISS database\n",
    "    # pdf reader\n",
    "    documents = []\n",
    "    pdf_files = df['Source_path'].unique()\n",
    "    for pdf_file in pdf_files:\n",
    "        pdf_loader = PyPDFLoader(pdf_file)\n",
    "        pdf_documents = pdf_loader.load()\n",
    "        for pdf_document in pdf_documents:\n",
    "            pdf_document.page_content = pdf_document.page_content.replace(\"\\x07\",\"\")\n",
    "        documents.extend(pdf_documents)\n",
    "    # 정규화\n",
    "    # for doc in documents:\n",
    "    #     doc.page_content = normalize_string(doc.page_content)\n",
    "    chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=64)\n",
    "    chunks = chunk_splitter.split_documents(documents)\n",
    "    print(f\"Total number of chunks: {len(chunks)}\")\n",
    "    # FAISS DB 만들기\n",
    "    faiss = FAISS.from_documents(chunks, embedding=get_embedding())\n",
    "    bm =  BM25Retriever.from_documents(chunks)\n",
    "    knn = KNNRetriever.from_documents(chunks, embeddings=get_embedding())\n",
    "    return faiss, bm, knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_faiss_db, train_bm_retrievier, knn_retriever = make_db(train_df) \n",
    "test_faiss_db, test_bm_retrievier, test_knn_retriever = make_db(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Fewshot Set: [{'Source': '중소벤처기업부_혁신창업사업화자금(융자)', 'Question': '2022년 혁신창업사업화자금(융자)의 예산은 얼마인가요?'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fewshot_faiss_db, fewshot_bm_retrievier, fewshot_knn_retriever = fewshot_db(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_k = 1\n",
    "train_bm_retrievier.k = train_k\n",
    "knn_retriever.k = train_k\n",
    "faiss_retriever = train_faiss_db.as_retriever(search_kwargs={'k':train_k} )\n",
    "train_ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[train_bm_retrievier,knn_retriever, faiss_retriever], weights=[0.25,0.25, 0.5]\n",
    ")\n",
    "\n",
    "test_k = 3\n",
    "test_bm_retrievier.k = test_k\n",
    "test_knn_retriever.k = test_k\n",
    "test_faiss_retriever = test_faiss_db.as_retriever(search_kwargs={'k':test_k} )\n",
    "test_ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[test_bm_retrievier,test_knn_retriever, test_faiss_retriever], weights=[0.25,0.25, 0.5]\n",
    ")\n",
    "\n",
    "fewshot_k = 3\n",
    "fewshot_bm_retrievier.k = fewshot_k\n",
    "fewshot_knn_retriever.k = fewshot_k\n",
    "fewshot_faiss_retriever = fewshot_faiss_db.as_retriever(search_kwargs={'k':fewshot_k} )\n",
    "fewshot_ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[fewshot_bm_retrievier,fewshot_knn_retriever, fewshot_faiss_retriever], weights=[0.25,0.25, 0.5]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    # 토크나이저 로드 및 설정\n",
    "        # 양자화 설정 적용\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,low_cpu_mem_usage=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.4,\n",
    "        do_sample=True,\n",
    "        top_p = 0.6,\n",
    "        repetition_penalty=1.1,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=512,\n",
    "        eos_token_id = terminators,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return llm\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_results = fewshot_ensemble_retriever.invoke(test_df.iloc[1]['Question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response):\n",
    "    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.replace('**', '')\n",
    "        if line.startswith('Answer:'):\n",
    "            return line.replace('Answer:', '').strip()\n",
    "        if line.startswith('assistant:'):\n",
    "            return line.replace('assistant:', '').strip()\n",
    "    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "def fewshot_ex(fewshot_retriever, train_retriever, query):\n",
    "    fewshot_results = fewshot_retriever.invoke(query)\n",
    "    fewshot_str = \"\"\n",
    "    for result in fewshot_results:\n",
    "        buff_str = \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        question = result['Question']\n",
    "        buff_str += f\"Question\\n{question}\\n\\n\"\n",
    "        if train_retriever is not None:\n",
    "            buff_str += f\"Context\\n\"\n",
    "            docs = train_retriever.invoke(question)\n",
    "            buff_str += format_docs(docs)\n",
    "            buff_str += \"<eot_id>\"\n",
    "        buff_str += f\"<|start_header_id|>assistant<|end_header_id>\\n{result['Answer']}<|eot_id|>\"\n",
    "        fewshot_str += buff_str\n",
    "    return fewshot_str\n",
    "\n",
    "def run (train,test,fewshot,dataset,llm,verbose=False):\n",
    "    results = []\n",
    "    for i, row in tqdm(dataset.iterrows()):\n",
    "        full_template = \"<|begin_of_text|>\"\n",
    "        full_template += \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are the financial expert who helps me with my financial information Q&As.\n",
    "You earn 10 points when you answer me and follow the rules and lose 7 points when you don't.\n",
    "Here are some rules you should follow.\n",
    "- Please use contexts to answer the question.\n",
    "- Please your answers should be concise.\n",
    "- Please answers must be written in Korean.\n",
    "- Please answer the question in 1-3 sentences.\n",
    "\n",
    "- Use the three examples below to learn how to follow the rules and reference information in context.<|eot_id|>\n",
    "\"\"\"\n",
    "        question = row['Question']\n",
    "        if verbose:\n",
    "            print(f\"Question\\n{question}\")\n",
    "        fewshot_str = fewshot_ex(fewshot, train, question)\n",
    "        full_template += fewshot_str\n",
    "        full_template += \"\\n\\n\"\n",
    "        full_template += \"<\"\n",
    "        contexts = test.invoke(question)\n",
    "        contexts = format_docs(contexts)\n",
    "        full_template += \"\"\"<|start_header_id|>user<|end_header_id|>\\nQuestion\\n{input}\\n\\n\"\"\"\n",
    "        full_template += f\"\"\"Context\\n{contexts}<|eot_id|>\"\"\"\n",
    "        full_template += \"\"\"<|start_header_id|>assistant<|end_header_id>\\n\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate.from_template(full_template)\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\"\\nQuestion: \", question)\n",
    "        answer = qa_chain.invoke(input=question)\n",
    "        answer = extract_answer(answer)\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Source\": row['Source']\n",
    "        })\n",
    "        if verbose:\n",
    "            print(\"Answer: \", results[-1]['Answer'])\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
