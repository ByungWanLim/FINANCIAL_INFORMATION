{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo97\\anaconda3\\envs\\llm\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Vector stores\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader\n",
    "#from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "#from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader ,UnstructuredPDFLoader\n",
    "#from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "#from langchain_teddynote.retrievers import KiwiBM25Retriever, OktBM25Retriever\n",
    "from langchain_teddynote.retrievers import OktBM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "#from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import pymupdf4llm\n",
    "#import time\n",
    "#import re\n",
    "#from konlpy.tag import Okt\n",
    "#from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from konlpy.tag import Kkma\n",
    "# etc\n",
    "#import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import logging\n",
    "#from PyPDF2 import PdfReader\n",
    "#import json\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intfloat/multilingual-e5-small\n",
    "# jhgan/ko-sroberta-multitask\n",
    "\n",
    "def get_embedding():\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='jhgan/ko-sroberta-multitask',\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'normalize_embeddings': True})\n",
    "    return embeddings\n",
    "def normalize_string(s):\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFC', s)\n",
    "        return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"�\", \" \").replace(\"\u0003\", \" \")  # 잘못된 인코딩 문자 제거\n",
    "    return text\n",
    "\n",
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    i = 1\n",
    "    for doc in docs:\n",
    "        context += f\"Document: {i}\" +\"Source:\"+ doc.metadata['source'] +'\\n'\n",
    "        context += doc.page_content\n",
    "        context += '\\n---\\n'\n",
    "        i += 1\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(pdf_path):\n",
    "    documents = []\n",
    "    try:\n",
    "        # 페이지 단위로 pymupdf4llm 사용\n",
    "        md_read = pymupdf4llm.to_markdown(pdf_path,page_chunks=True)\n",
    "        total_pages = len(md_read)    \n",
    "        \n",
    "        for page_data in md_read:\n",
    "            page_number = page_data.get('metadata', {}).get('page', None)\n",
    "            text = clean_text(normalize_string(page_data['text']))\n",
    "            metadata = {\n",
    "                \"file_path\": pdf_path,\n",
    "                \"page_number\": page_number,\n",
    "                \"total_pages\": total_pages,\n",
    "                \"tables\": page_data.get('tables', None),\n",
    "            }\n",
    "            \n",
    "            documents.append(Document(page_content=text, metadata=metadata))\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to process PDF with pymupdf4llm: {e}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    markdown_headers = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "    \n",
    "    # 마크다운 헤더 스플리터 설정\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=markdown_headers,\n",
    "        strip_headers=False  # 헤더를 유지\n",
    "    )\n",
    "    chunks = []\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        metadata = doc.metadata\n",
    "        md_header_splits = markdown_splitter.split_text(text)\n",
    "        if metadata.get('tables'):\n",
    "            updated_metadata = {**metadata, **md_header_splits[0].metadata}\n",
    "            # 테이블이 있는 페이지는 청크로 분할하지 않음\n",
    "            chunks.append(Document(page_content=doc.page_content, metadata=updated_metadata))\n",
    "            continue\n",
    "        for md_split in md_header_splits:\n",
    "            # 마크다운 스플리터로 생성된 청크에 대해 추가 분할 적용\n",
    "            # 헤더 정보를 메타데이터에 유지\n",
    "            updated_metadata = {**metadata, **md_split.metadata}\n",
    "            chunks.append(Document(page_content=md_split.page_content, metadata=updated_metadata))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def make_db(df):\n",
    "    documents = []\n",
    "    chunks = []\n",
    "    pdf_files = df['Source_path'].unique()\n",
    "    for pdf_file in tqdm(pdf_files):\n",
    "        # 문서 로드\n",
    "        documents.extend(get_docs(pdf_file))\n",
    "        # 청크 생성\n",
    "        chunks.extend(chunk_documents(documents))\n",
    "    print(f\"Total number of documents: {len(documents)}\")\n",
    "    print(f\"Total number of chunks: {len(chunks)}\")\n",
    "\n",
    "    faiss = FAISS.from_documents(chunks, embedding=get_embedding())\n",
    "    bm = OktBM25Retriever.from_documents(chunks)\n",
    "\n",
    "    return faiss, bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_db(df):\n",
    "    df = df.drop('SAMPLE_ID', axis=1)\n",
    "    df = df.drop('Source_path', axis=1)\n",
    "    df = df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", len(df))\n",
    "    to_vectorize = [\"\\n\\n\".join(normalize_string(value) for value in example.values()) for example in df]\n",
    "    faiss = FAISS.from_texts(to_vectorize, embedding=get_embedding())\n",
    "    # bm = BM25Retriever.from_texts(to_vectorize)\n",
    "    # knn = KNNRetriever.from_texts(to_vectorize, embeddings=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        #context += f\"Document {i+1}\\n\"\n",
    "        doc.page_content = doc.page_content.replace(\"{\", \"(\")\n",
    "        doc.page_content = doc.page_content.replace(\"}\", \")\")\n",
    "        \n",
    "        context += doc.page_content\n",
    "        context += '\\n\\n'\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_llm_pipeline(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "#     # 토크나이저 로드 및 설정\n",
    "#         # 양자화 설정 적용\n",
    "#     bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True, \n",
    "#     bnb_4bit_use_double_quant=True, \n",
    "#     bnb_4bit_quant_type=\"nf4\", \n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#     )\n",
    "#     model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,low_cpu_mem_usage=True)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "#     terminators = [\n",
    "#     tokenizer.eos_token_id,\n",
    "#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "#     ]\n",
    "\n",
    "#     text_generation_pipeline = pipeline(\n",
    "#         model=model,\n",
    "#         tokenizer=tokenizer,\n",
    "#         task=\"text-generation\",\n",
    "#         return_full_text=False,\n",
    "#         max_new_tokens=1024,\n",
    "#         eos_token_id = terminators,\n",
    "#         pad_token_id = tokenizer.eos_token_id\n",
    "#     )\n",
    "\n",
    "#     llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "#     return llm\n",
    "# # ghost-x/ghost-8b-beta-1608\n",
    "# # OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k\n",
    "# llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bitsandbytes.config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BitsAndBytesConfig\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_llm_pipeline\u001b[39m(model_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 양자화 설정 적용\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[0;32m      8\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 기본적으로 4비트로 로드\u001b[39;00m\n\u001b[0;32m      9\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# 두 번 양자화 적용\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# 4비트 양자화 유형 선택\u001b[39;00m\n\u001b[0;32m     11\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16  \u001b[38;5;66;03m# 연산은 bf16으로 수행\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     )\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bitsandbytes.config'"
     ]
    }
   ],
   "source": [
    "def setup_llm_pipeline(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    # 양자화 설정 적용\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # 기본적으로 4비트로 로드\n",
    "        bnb_4bit_use_double_quant=True,  # 두 번 양자화 적용\n",
    "        bnb_4bit_quant_type=\"nf4\",  # 4비트 양자화 유형 선택\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16  # 연산은 bf16으로 수행\n",
    "    )\n",
    "\n",
    "    # 모델 로드\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        quantization_config=bnb_config,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    # 일부 중요한 레이어는 FP16으로 유지\n",
    "    for name, module in model.named_modules():\n",
    "        if \"attention\" in name or \"ffn\" in name:  # 중요한 레이어 식별 (예: attention 및 ffn)\n",
    "            module.to(torch.float16)  # 이 부분은 16비트로 유지\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id = terminators,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return llm\n",
    "# ghost-x/ghost-8b-beta-1608\n",
    "# OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n",
    "\n",
    "    #공백 제거\n",
    "    true_sentence = ''.join(true_sentence.split())\n",
    "    predicted_sentence = ''.join(predicted_sentence.split())\n",
    "    \n",
    "    true_counter = Counter(true_sentence)\n",
    "    predicted_counter = Counter(predicted_sentence)\n",
    "\n",
    "    #문자가 등장한 개수도 고려\n",
    "    if sum_mode:\n",
    "        true_positive = sum((true_counter & predicted_counter).values())\n",
    "        predicted_positive = sum(predicted_counter.values())\n",
    "        actual_positive = sum(true_counter.values())\n",
    "\n",
    "    #문자 자체가 있는 것에 focus를 맞춤\n",
    "    else:\n",
    "        true_positive = len((true_counter & predicted_counter).values())\n",
    "        predicted_positive = len(predicted_counter.values())\n",
    "        actual_positive = len(true_counter.values())\n",
    "\n",
    "    #f1 score 계산\n",
    "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_average_f1_score(true_sentences, predicted_sentences):\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n",
    "        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1_score += f1_score\n",
    "    \n",
    "    avg_precision = total_precision / len(true_sentences)\n",
    "    avg_recall = total_recall / len(true_sentences)\n",
    "    avg_f1_score = total_f1_score / len(true_sentences)\n",
    "    \n",
    "    return {\n",
    "        'average_precision': avg_precision,\n",
    "        'average_recall': avg_recall,\n",
    "        'average_f1_score': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response):\n",
    "    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.replace('**', '')\n",
    "        if line.startswith('Answer:'):\n",
    "            return line.replace('Answer:', '').strip()\n",
    "        if line.startswith('assistant:'):\n",
    "            return line.replace('assistant:', '').strip()\n",
    "    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "def rerun(question,context,answer,llm,num_repeat):\n",
    "    full_template = \"<|begin_of_text|>\"\n",
    "    full_template += \"\"\"<|start_header_id|>system<|end_header_id|>당신은 이전 답변을 검증하는 챗봇입니다. 질문과 문맥, 이전 답변을 참고해서 지시사항을 따르세요.<|eot_id|>\"\"\"\n",
    "    full_template += f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {question} \\n\\nContexts: {context} \\n\\nPrevious Answer: {answer} \\n\\n\"\"\"\n",
    "    full_template += \"\"\"{input}<|eot_id|>\"\"\"\n",
    "    full_template += \"\"\"<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=full_template)\n",
    "    chain = (\n",
    "    {\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(\"핵심 단어들을 바탕으로, 한 문장으로 요약하세요. 만약 한 문장이라면 그대로 출력하세요.\")\n",
    "    \n",
    "\n",
    "def run (test,dataset,llm,verbose=False):\n",
    "    results = []\n",
    "    for i, row in (dataset.iterrows()):\n",
    "        full_template = \"<|begin_of_text|>\"\n",
    "        full_template += \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "당신은 유용한 금융 정보 QnA 챗봇입니다.\n",
    "질문에 모두 답해야 하며, 답변은 반드시 문맥 정보를 활용해야합니다. \n",
    "객관적이고 공식적인 문체를 사용하세요.\n",
    "어떤 서론이나 배경 설명 포함 금지. 다음과 같습니다 금지.\n",
    "핵심 내용만을 한 문장으로 작성해주세요. \n",
    "<|eot_id|>\n",
    "\"\"\"\n",
    "        question = row['Question']          \n",
    "        # full_template += \"\"\" \"\"\"\n",
    "        contexts = test.invoke(normalize_string(question))\n",
    "        contexts = format_docs(contexts)\n",
    "        full_template += \"\"\"<|start_header_id|>user<|end_header_id|>Question: {input}\\n\\n\"\"\"\n",
    "        full_template += f\"\"\"Contexts: {contexts}<|eot_id|>\"\"\"\n",
    "        full_template += \"\"\"<|start_header_id|>assistant<|end_header_id>\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=full_template, input_variables=[\"input\"])\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        answer = qa_chain.invoke(input=question)\n",
    "        answer = extract_answer(answer)\n",
    "        lines = answer.split('\\n')\n",
    "        if  len(lines) > 1:\n",
    "            previous = answer\n",
    "            try:\n",
    "                before = calculate_f1_score(row['Answer'],answer)[2]\n",
    "            except:\n",
    "                before = None\n",
    "            answer = rerun(question=question,\n",
    "                           context=contexts,\n",
    "                           answer=answer,\n",
    "                           llm=llm,\n",
    "                           num_repeat=1)\n",
    "        answer = extract_answer(answer)\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Source\": row['Source']\n",
    "        })\n",
    "        if verbose:\n",
    "            print(f\"{i}/{len(dataset)}\")\n",
    "            print(\"Question: \", question, end=\" | \")\n",
    "            print(\"Context Number |\",len(contexts))\n",
    "            try:\n",
    "                print(calculate_f1_score(row['Answer'],answer)[2],end=\" | \")\n",
    "            except:\n",
    "                pass\n",
    "            print(\"Answer: \", results[-1]['Answer'])\n",
    "            try:\n",
    "                print(\"Before: \",before,\" | \",previous)  \n",
    "                \n",
    "                previous = None\n",
    "                before = None\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                print(\"REAL Answer: \",row['Answer'])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import copy\n",
    "\n",
    "# weight = [0.5,0.5]\n",
    "# train_faiss_db, train_bm_retrievier = make_db(train_df) \n",
    "\n",
    "# train_k = 3\n",
    "# train_bm_retrievier.k = train_k\n",
    "# # knn_retriever.k = train_k\n",
    "# train_faiss_retriever = train_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':train_k} )\n",
    "# train_ensemble_retriever = EnsembleRetriever(\n",
    "#     retrievers=[train_bm_retrievier, train_faiss_retriever], weights=weight\n",
    "# )\n",
    "\n",
    "\n",
    "# fewshot_k = 3\n",
    "\n",
    "# k_folds = 4\n",
    "# fold_results = []\n",
    "# kf = KFold(n_splits=k_folds, shuffle=True, random_state=52)\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "#     fold_result = []\n",
    "#     train_set = train_df.iloc[train_index]\n",
    "#     val_set = train_df.iloc[val_index]\n",
    "\n",
    "#     pred = run(train_ensemble_retriever, val_set, llm, verbose=True)\n",
    "#     result = pd.DataFrame()\n",
    "#     result['pred'] = [result['Answer'] for result in pred]\n",
    "#     val_set.index = range(len(val_set))\n",
    "#     result['gt'] = val_set['Answer']\n",
    "        \n",
    "#     result = calculate_average_f1_score(result['gt'], result['pred'])\n",
    "#     print(result)\n",
    "#     fold_results.append(result)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:15<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 69\n",
      "Total number of chunks: 338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leo97\\anaconda3\\envs\\llm\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from save_module import save\n",
    "\n",
    "\n",
    "weight = [0.5,0.5]\n",
    "test_faiss_db, test_bm_retrievier = make_db(test_df)\n",
    "\n",
    "test_k = 3\n",
    "test_bm_retrievier.k = test_k\n",
    "#test_knn_retriever.k = test_k\n",
    "test_faiss_retriever = test_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':test_k} )\n",
    "test_ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[test_bm_retrievier, test_faiss_retriever], weights=weight\n",
    ")\n",
    "\n",
    "\n",
    "results = run(test_ensemble_retriever, test_df, llm, verbose=True)\n",
    "save(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
