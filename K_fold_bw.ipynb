{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import torch\n",
    "from faiss_module_bw import make_db, make_fewshot_db\n",
    "from model import setup_llm_pipeline\n",
    "\n",
    "from save_module import save\n",
    "from seed_module import seed_everything\n",
    "from utils_module import make_dict, extract_answer, format_docs\n",
    "\n",
    "seed_everything(52)\n",
    "from sklearn.model_selection import KFold\n",
    "def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n",
    "\n",
    "    #공백 제거\n",
    "    true_sentence = ''.join(true_sentence.split())\n",
    "    predicted_sentence = ''.join(predicted_sentence.split())\n",
    "    \n",
    "    true_counter = Counter(true_sentence)\n",
    "    predicted_counter = Counter(predicted_sentence)\n",
    "\n",
    "    #문자가 등장한 개수도 고려\n",
    "    if sum_mode:\n",
    "        true_positive = sum((true_counter & predicted_counter).values())\n",
    "        predicted_positive = sum(predicted_counter.values())\n",
    "        actual_positive = sum(true_counter.values())\n",
    "\n",
    "    #문자 자체가 있는 것에 focus를 맞춤\n",
    "    else:\n",
    "        true_positive = len((true_counter & predicted_counter).values())\n",
    "        predicted_positive = len(predicted_counter.values())\n",
    "        actual_positive = len(true_counter.values())\n",
    "\n",
    "    #f1 score 계산\n",
    "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_average_f1_score(true_sentences, predicted_sentences):\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n",
    "        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1_score += f1_score\n",
    "    \n",
    "    avg_precision = total_precision / len(true_sentences)\n",
    "    avg_recall = total_recall / len(true_sentences)\n",
    "    avg_f1_score = total_f1_score / len(true_sentences)\n",
    "    \n",
    "    return {\n",
    "        'average_precision': avg_precision,\n",
    "        'average_recall': avg_recall,\n",
    "        'average_f1_score': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:22<00:00,  5.60s/it]\n"
     ]
    }
   ],
   "source": [
    "k_folds = 4\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=52)\n",
    "train_df = pd.read_csv('train.csv')\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "llm = setup_llm_pipeline(model_id)\n",
    "fold_result = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from vectordb_module import FAISSDatabaseManager\n",
    "# from run_blendrag import run\n",
    "#     # EleutherAI/polyglot-ko-1.3b\n",
    "#     #\"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "#     # maywell/TinyWand-kiqu\n",
    "#     # yanolja/EEVE-Korean-Instruct-2.8B-v1.0\n",
    "#     # MLP-KTLim/llama-3-Korean-Bllossom-8B\n",
    "#     # train에도 RAG를 쓸 때 사용\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "#     fold_results = []\n",
    "#     print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "#     # 수정된 부분: .iloc[] 사용\n",
    "#     train_set = train_df.iloc[train_index]\n",
    "#     val_set = train_df.iloc[val_index]\n",
    "    \n",
    "#     train_db = FAISSDatabaseManager(db_path=f'./serm_best_field_train_db', chunk_strategy=\"serm\", search_strategy=\"serm_best_field\")\n",
    "#     train_db.make_db(train_set)\n",
    "    \n",
    "#     val_db = FAISSDatabaseManager(db_path=f'./serm_best_field_train_db', chunk_strategy=\"serm\", search_strategy=\"serm_best_field\")\n",
    "#     val_db.make_db(train_set)\n",
    "    \n",
    "#     fewshot_db = FAISSDatabaseManager(db_path=f'./fewshot_db_{fold}',search_strategy=\"knn_best_field\")\n",
    "#     fewshot_db.make_db(val_set,fewshot=True)\n",
    "    \n",
    "#     pred = run(train_db= train_db,\n",
    "#         test_db= val_db,\n",
    "#         fewshot_db=fewshot_db, \n",
    "#         dataset= val_set.to_dict(orient='records') ,\n",
    "#         llm=llm,\n",
    "#         verbose=False)\n",
    "#     result = pd.DataFrame()\n",
    "#     result['pred'] = [result['Answer'] for result in pred]\n",
    "#     val_set.index = range(len(val_set))\n",
    "#     result['gt'] = val_set['Answer']\n",
    "        \n",
    "#     result = calculate_average_f1_score(result['gt'], result['pred'])\n",
    "#     print(result)\n",
    "#     fold_result.append(result)\n",
    "#     break\n",
    "#     print(f\"Fold {fold + 1} ended\")\n",
    "# # 모든 fold의 결과를 집계하고 메트릭 계산\n",
    "# all_results = [result for fold_result in fold_results for result in fold_result]\n",
    "# print(f\"Average F1 Score: {np.mean([result['average_f1_score'] for result in all_results])}\")\n",
    "# print(f\"Average Precision: {np.mean([result['average_precision'] for result in all_results])}\")\n",
    "# print(f\"Average Recall: {np.mean([result['average_recall'] for result in all_results])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5691677502 | 0.6880094825892175\n",
    "\n",
    "|  0.703940744417009 temp 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/4\n",
      "Loading FAISS DB from: ./train_faiss_db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\torch\\storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Fewshot Set: [{'Source': '1-1 2024 주요 재정통계 1권', 'Source_path': './train_source/1-1 2024 주요 재정통계 1권.pdf', 'Question': '2024년 중앙정부 재정체계는 어떻게 구성되어 있나요?', 'Answer': '2024년 중앙정부 재정체계는 예산(일반·특별회계)과 기금으로 구분되며, 2024년 기준으로 일반회계 1개, 특별회계 21개, 기금 68개로 구성되어 있습니다.'}]\n",
      "Creating FAISS DB\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/124 [00:00<?, ?it/s]c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  8%|▊         | 10/124 [01:12<14:49,  7.80s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 124/124 [18:20<00:00,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'average_precision': 0.5956608636739278, 'average_recall': 0.7539088542231868, 'average_f1_score': 0.6230294326970625}\n",
      "Average F1 Score: nan\n",
      "Average Precision: nan\n",
      "Average Recall: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\numpy\\core\\_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from run_bw2 import run\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "    fold_results = []\n",
    "    print(f\"\\nFold {fold + 1}/{k_folds}\")\n",
    "    # 수정된 부분: .iloc[] 사용\n",
    "    train_set = train_df.iloc[train_index]\n",
    "    val_set = train_df.iloc[val_index]\n",
    "    \n",
    "    train_db = make_db(train_set,'./train_faiss_db')\n",
    "\n",
    "    fewshot_db = make_fewshot_db(train_set,None)\n",
    "    \n",
    "    pred = results = run(train_db= train_db,\n",
    "        test_db= train_db,\n",
    "        fewshot_db=fewshot_db, \n",
    "        dataset= val_set.to_dict(orient='records') ,\n",
    "        llm=llm,\n",
    "        verbose=False)\n",
    "    result = pd.DataFrame()\n",
    "    result['pred'] = [result['Answer'] for result in pred]\n",
    "    val_set.index = range(len(val_set))\n",
    "    result['gt'] = val_set['Answer']\n",
    "        \n",
    "    result = calculate_average_f1_score(result['gt'], result['pred'])\n",
    "    print(result)\n",
    "    fold_result.append(result)\n",
    "    break\n",
    "    fewshot_db = None\n",
    "    train_db = None\n",
    "    val_db = None\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "# 모든 fold의 결과를 집계하고 메트릭 계산\n",
    "all_results = [result for fold_result in fold_results for result in fold_result]\n",
    "print(f\"Average F1 Score: {np.mean([result['average_f1_score'] for result in all_results])}\")\n",
    "print(f\"Average Precision: {np.mean([result['average_precision'] for result in all_results])}\")\n",
    "print(f\"Average Recall: {np.mean([result['average_recall'] for result in all_results])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화 전부 제거\n",
    "0.5845866120343769\n",
    "\n",
    "앙상블 리트리버\n",
    "k=3 0.5/0.5\n",
    "0.6033479893221689\n",
    "\n",
    "k=3 0.3/0.7\n",
    "0.6230294326970625\n",
    "\n",
    "k=3 0.15/0.85\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context위치변경\n",
    "\n",
    "1트 \n",
    "\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "Now Do it for real.<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Question: {input}\\n\\nContext: {context}<|eot_id|>\n",
    "\n",
    "k-fold 0.63\n",
    "\n",
    "2트\n",
    "\n",
    "0.6395752909263238\n",
    "\n",
    "3트\n",
    "\n",
    "0.6523232543460189"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## new prompt 예제에서 context에서 answer를 도출했다고 말함 그리고 그런식으로 작성하라고 함\n",
    "\n",
    "temp = 0.4, topp = 0.6 repetition_penalty = 1.1\n",
    "\n",
    "fewshot num = 3, fewshot context = 1 , context = 2 청크 512, 오버랩 64, \n",
    "\n",
    "LB: \n",
    "\n",
    "fold 1:0.6356956690740023 (원래)\n",
    "\n",
    "\n",
    "(NeW)\n",
    "LB: 0.5846913577\n",
    "\n",
    "fold 1 0.6375135913067492\n",
    " \n",
    " \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Today Date: 8 Aug 2024\n",
    "\n",
    "You are the financial expert who helps me with my financial information Q&As.\n",
    "You earn 10 points when you answer me and follow the rules and lose 7 points when you don't.\n",
    "\n",
    "12,500 백만원 = 125 억원 = 12,500,000,000 원\n",
    "5,400 백만원 = 54 억원 = 5,400,000,000 원\n",
    "\n",
    "Here are some rules you should follow.\n",
    "- Please use contexts to answer the question.\n",
    "- Please your answers should be concise.\n",
    "- Please answers must be written in Korean.\n",
    "- Please answer the question in 1-3 sentences.\n",
    "\n",
    "Please learn the answering like examples below.<|eot_id|>\n",
    "\"\"\" +f\"{fewshot_str}\" + \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "Now Do it for real.\n",
    "Given the following contexts about Question:\n",
    "{context}<|start_header_id|>user<|end_header_id|>\n",
    "{input}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## search_type=\"similarity_score_threshold\"\n",
    "temp = 0.4, topp = 0.6 repetition_penalty = 1.1\n",
    "\n",
    "fewshot num = 3, fewshot context = 1 , context = 2 청크 512, 오버랩 64, \n",
    "\n",
    "LB: 0.6014109705\t\n",
    "\n",
    "fold 1: 0.6478771022993277\n",
    "\n",
    "~~\n",
    "\n",
    "fewshot num = 3, fewshot context = 1 , context = 5 청크 512, 오버랩 64, \n",
    "\n",
    "LB: 0.5791770154\n",
    "\n",
    "fold 1: 0.661329586886551\n",
    "\n",
    "~~\n",
    "\n",
    "fewshot num = 3, fewshot context = 0 , context = 5 청크 512, 오버랩 64, \n",
    "\n",
    "LB: 0.5722539904\n",
    "\n",
    "fold 1: 0.6533704753399003\n",
    "\n",
    "\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "Today Date: 8 Aug 2024\n",
    "1,000,000 원= 100 만원\n",
    "10 백만원 = 10,000,000 원\n",
    "100 백만원 = 100,000,000 원\n",
    "100 백만원 = 1 억원\n",
    "1,000 백만원 = 10 억원\n",
    "You are the financial expert who helps me with my financial information Q&As.\n",
    "You earn 10 points when you answer me and follow the rules and lose 7 points when you don't.\n",
    "\n",
    "Here are some rules you should follow.\n",
    "- Please use contexts to answer the question.\n",
    "- Please your answers should be concise.\n",
    "- Please answers must be written in Korean.\n",
    "- Please answer the question in 1-3 sentences.\n",
    "\n",
    "Please answer like the example below.<|eot_id|>\n",
    "\"\"\" +f\"{fewshot_str}\" + \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "Now Do it for me.\n",
    "Given the following contexts about Question:\n",
    "{context}<|start_header_id|>user<|end_header_id|>\n",
    "{input}<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n\\n\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = 0.4, topp = 0.6 repetition_penalty = 1.1\n",
    "\n",
    "fewshot num = 3, fewshot context = 1 , context = 3 청크 512, 오버랩 64, \n",
    "\n",
    "LB: \n",
    "\n",
    "fold 1: 0.7253077903895752\n",
    "\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = 0.2, topp = 0.6 repetition_penalty = 1.1\n",
    "\n",
    "fewshot num = 3, fewshot context = 1 , context = 3\n",
    "\n",
    "LB: \n",
    "\n",
    "fold 1:  0.6544007302469212\n",
    "\n",
    "temp = 0.1, topp = 0.7 repetition_penalty = 1.1\n",
    "\n",
    "fewshot num = 3, fewshot context = 1 , context = 3\n",
    "\n",
    "LB: \n",
    "\n",
    "fold 1:  0.6554498759729692"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = 0.4, topp = 0.6 repetition_penalty = 1.1\n",
    "\n",
    "fewshot num = 3, fewshot context = 3 , context = 3\n",
    "\n",
    "LB: 0.5859080344\n",
    "\n",
    "fold 1: 0.6586463841048915"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = 0.4, topp = 0.6 repetition_penalty = 1.1\n",
    "\n",
    "fewshot num = 3, fewshot context = 0 , context = 3\n",
    "\n",
    "LB: \n",
    "\n",
    "fold 1: 0.6381098591323223"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp = 0.4, topp = 0.6 repetition_penalty = 1.1\n",
    "\n",
    "fewshot num = 10, fewshot context = 0 , context = 3\n",
    "\n",
    "LB: \n",
    "\n",
    "fold 1: 0.6327671925994144"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
