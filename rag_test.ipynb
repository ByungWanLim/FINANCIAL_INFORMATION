{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "from konlpy.tag import Okt\n",
    "import logging\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel, AutoTokenizer\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import MarkdownTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever, OktBM25Retriever\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "# Vector stores\n",
    "import fitz  # PyMuPDF\n",
    "import pymupdf4llm\n",
    "from pymupdf4llm import to_markdown\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else device  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "def get_embedding():\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='jhgan/ko-sroberta-multitask',\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'normalize_embeddings': True})\n",
    "    return embeddings\n",
    "def normalize_string(s):\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFC', s)\n",
    "        return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"�\", \" \").replace(\"\u0003\", \" \")  # 잘못된 인코딩 문자 제거\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### markdown, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path): \n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"#####\", \"Header 5\"),\n",
    "        (\"######\", \"Header 6\")\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=True)\n",
    "    md_chunks = md_header_splitter.split_text(md_text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512, chunk_overlap=64\n",
    "    )\n",
    "\n",
    "    splits = text_splitter.split_documents(md_chunks)\n",
    "\n",
    "    for i in splits:\n",
    "        metadata = {'Source_path': pdf_path}\n",
    "        i.metadata = {**i.metadata, **metadata}\n",
    "    return splits\n",
    "\n",
    "\n",
    "def make_db(df):\n",
    "    documents = []\n",
    "    \n",
    "    pdf_files = df['Source_path'].unique()\n",
    "    for pdf_file in tqdm(pdf_files):\n",
    "        # 문서 로드\n",
    "        documents.extend(process_pdf(pdf_file))\n",
    "        \n",
    "    print(f\"Total number of documents: {len(documents)}\")\n",
    "\n",
    "    faiss = FAISS.from_documents(documents, embedding=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# md_text = pymupdf4llm.to_markdown(test_df.iloc[0]['Source_path'])\n",
    "\n",
    "\n",
    "# headers_to_split_on = [\n",
    "#     (\"#\", \"Header 1\"),\n",
    "#     (\"##\", \"Header 2\"),\n",
    "#     (\"###\", \"Header 3\"),\n",
    "# ]\n",
    "\n",
    "# md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "# md_chunks = md_header_splitter.split_text(md_text)\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(\n",
    "#     chunk_size=512, chunk_overlap=64\n",
    "# )\n",
    "\n",
    "# splits = text_splitter.split_documents(md_chunks)\n",
    "\n",
    "# for i in splits:\n",
    "#     metadata = {'Source_path': test_df.iloc[0]['Source_path']}\n",
    "#     i.metadata = {**i.metadata, **metadata}\n",
    "# faiss = FAISS.from_documents(splits, embedding=get_embedding())\n",
    "\n",
    "# docs = faiss.similarity_search(\n",
    "#     query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "#     filter={\"Source_path\": test_df.iloc[0]['Source_path']},\n",
    "#     k = 99,\n",
    "#     fetch_k = 20000\n",
    "# )\n",
    "# buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "# faiss_retriever = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k,'fetch_k': 50})\n",
    "# knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "# knn_retriever.k = k\n",
    "# bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "# bm_retriever.k = k\n",
    "\n",
    "# ensenble_retriever = EnsembleRetriever(retrievers=[faiss_retriever, knn_retriever,bm_retriever], weight=[0.4, 0.3, 0.3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### konlptextsplitter, pytessearct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ocr_extract_text_from_pdf(pdf_path):\n",
    "#     documents = []\n",
    "#     try:\n",
    "#         # PDF를 이미지로 변환\n",
    "#         images = convert_from_path(pdf_path, dpi=500)\n",
    "        \n",
    "#         # 각 페이지에서 OCR로 텍스트 추출\n",
    "#         for i, image in enumerate(images):\n",
    "#             text = pytesseract.image_to_string(image, lang='kor+eng')\n",
    "#             text = clean_text(normalize_string(text))\n",
    "#             # 각 페이지마다 별도의 Document 생성\n",
    "#             documents.append(Document(page_content=text, metadata={\"source\": pdf_path, \"page\": i + 1}))\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    \n",
    "#     return documents\n",
    "\n",
    "# def load_pdf_file(pdf_file):\n",
    "#     try:\n",
    "#         # OCR을 이용한 텍스트 추출\n",
    "#         documents = ocr_extract_text_from_pdf(pdf_file)\n",
    "#         return documents\n",
    "#     except Exception as e:\n",
    "#         print(f\"Skipping corrupted file {pdf_file}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# def split_documents_with_konlpy(documents):\n",
    "#     adjusted_documents = []\n",
    "#     konlpy_splitter = KonlpyTextSplitter(separator='\\n\\n')\n",
    "\n",
    "#     for doc in documents:\n",
    "#         # KonlpyTextSplitter로 문장 단위 분리\n",
    "#         sentences = konlpy_splitter.split_text(doc.page_content)\n",
    "#         for sentence in sentences:\n",
    "#             adjusted_documents.append(Document(page_content=sentence, metadata=doc.metadata))\n",
    "    \n",
    "#     return adjusted_documents\n",
    "\n",
    "# def make_db(df, chunk_name=\"train\", chunk_size=800, chunk_overlap=60):\n",
    "#     documents = []\n",
    "#     chunk_file_path = f\"{chunk_name}_chunks.json\"\n",
    "\n",
    "#     if os.path.exists(chunk_file_path):\n",
    "#         print(f\"Loading chunks from {chunk_file_path}...\")\n",
    "#         documents = load_chunks(chunk_file_path)\n",
    "#     else:\n",
    "#         pdf_files = df['Source_path'].unique()\n",
    "#         with ThreadPoolExecutor() as executor:\n",
    "#             results = list(tqdm(executor.map(load_pdf_file, pdf_files), total=len(pdf_files), desc=\"Processing PDFs\"))\n",
    "        \n",
    "#         for result in results:\n",
    "#             documents.extend(result)\n",
    "        \n",
    "\n",
    "\n",
    "#         # 청크 데이터를 저장\n",
    "#         save_chunks(documents, chunk_file_path)\n",
    "\n",
    "#     # KonlpyTextSplitter로 문장 단위로 먼저 분할\n",
    "#     documents = split_documents_with_konlpy(documents)\n",
    "    \n",
    "#     # make_db에서 RecursiveCharacterTextSplitter로 청크를 분할\n",
    "#     recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=chunk_size,\n",
    "#         chunk_overlap=chunk_overlap,\n",
    "#         separators=[\"\\n\\n\", \".\"]\n",
    "#     )\n",
    "#     documents = recursive_splitter.split_documents(documents)\n",
    "    \n",
    "#     faiss = FAISS.from_documents(documents, embedding=get_embedding())\n",
    "#     bm = KiwiBM25Retriever.from_documents(documents)\n",
    "    \n",
    "#     return faiss, bm\n",
    "\n",
    "# def save_chunks(documents, path):\n",
    "#     chunk_data = []\n",
    "#     for doc in documents:\n",
    "#         chunk_data.append({\n",
    "#             \"page_content\": doc.page_content,\n",
    "#             \"metadata\": doc.metadata\n",
    "#         })\n",
    "    \n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(chunk_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "#     print(f\"Chunks saved to {path}\")\n",
    "\n",
    "# def load_chunks(path):\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         chunk_data = json.load(f)\n",
    "    \n",
    "#     documents = []\n",
    "#     for chunk in chunk_data:\n",
    "#         documents.append(Document(page_content=chunk[\"page_content\"], metadata=chunk[\"metadata\"]))\n",
    "    \n",
    "#     print(f\"Chunks loaded from {path}\")\n",
    "#     return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언스트럭처"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalize_string(s):\n",
    "#     try:\n",
    "#         normalized = unicodedata.normalize('NFC', s)\n",
    "#         return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "#     except Exception:\n",
    "#         return s\n",
    "# def clean_text(text):\n",
    "#     text = text.replace(\"�\", \"\").replace(\"\u0003\", \"\")  # 잘못된 인코딩 문자 제거\n",
    "#     text = ' '.join(text.split())  # 여러 공백을 하나로 줄임\n",
    "#     return text\n",
    "\n",
    "# def load_pdf_file(pdf_file):\n",
    "#     try:\n",
    "#         pdf_loader = UnstructuredLoader(pdf_file,\n",
    "#                                         chunking_strategy = 'by_title',\n",
    "#                                         max_characters = 1000,\n",
    "#                                         overlap = 200)\n",
    "#         return pdf_loader.load()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Skipping corrupted file {pdf_file}: {e}\")\n",
    "#         return []\n",
    "\n",
    "# def make_db(df, chunk_name=\"train\"):\n",
    "#     documents = []\n",
    "#     chunk_file_path = f\"{chunk_name}_chunks.json\"\n",
    "\n",
    "#     if os.path.exists(chunk_file_path):\n",
    "#         # 이미 저장된 청크 데이터가 있으면 불러오기\n",
    "#         print(f\"Loading chunks from {chunk_file_path}...\")\n",
    "#         documents = load_chunks(chunk_file_path)\n",
    "#     else:\n",
    "#         # 청크 데이터가 없으면 PDF 파일을 처리하여 새로 생성\n",
    "#         pdf_files = df['Source_path'].unique()\n",
    "\n",
    "#         # tqdm으로 파일 처리 진행 상황 표시\n",
    "#         with ThreadPoolExecutor() as executor:\n",
    "#             results = list(tqdm(executor.map(load_pdf_file, pdf_files), total=len(pdf_files), desc=\"Processing PDFs\"))\n",
    "        \n",
    "#         for result in results:\n",
    "#             documents.extend(result)\n",
    "        \n",
    "#         # 정규화\n",
    "#         for doc in documents:\n",
    "#             doc.page_content = normalize_string(doc.page_content)\n",
    "        \n",
    "#         # 청크 데이터를 저장\n",
    "#         save_chunks(documents, chunk_file_path)\n",
    "    \n",
    "#     # FAISS DB 만들기 (메타데이터 포함)\n",
    "#     faiss = FAISS.from_documents(documents, embedding=get_embedding())\n",
    "#     bm = KiwiBM25Retriever.from_documents(documents)\n",
    "    \n",
    "#     return faiss, bm\n",
    "\n",
    "# import json\n",
    "\n",
    "# def save_chunks(documents, path):\n",
    "#     chunk_data = []\n",
    "#     for doc in documents:\n",
    "#         chunk_data.append({\n",
    "#             \"page_content\": doc.page_content,\n",
    "#             \"metadata\": doc.metadata\n",
    "#         })\n",
    "    \n",
    "#     with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "#         json.dump(chunk_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "#     print(f\"Chunks saved to {path}\")\n",
    "\n",
    "# from langchain.docstore.document import Document\n",
    "\n",
    "# def load_chunks(path):\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         chunk_data = json.load(f)\n",
    "    \n",
    "#     documents = []\n",
    "#     for chunk in chunk_data:\n",
    "#         documents.append(Document(page_content=chunk[\"page_content\"], metadata=chunk[\"metadata\"]))\n",
    "    \n",
    "#     print(f\"Chunks loaded from {path}\")\n",
    "#     return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pdf 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding():\n",
    "#     device = 'cuda' if torch.cuda.is_available() and 'mps' in torch.cuda.get_device_capability() else 'cpu'  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n",
    "#     embeddings = HuggingFaceEmbeddings(\n",
    "#         model_name='jhgan/ko-sroberta-multitask',\n",
    "#         model_kwargs={'device': device},\n",
    "#         encode_kwargs={'normalize_embeddings': True})\n",
    "#     return embeddings\n",
    "\n",
    "# def normalize_string(s):\n",
    "#     try:\n",
    "#         normalized = unicodedata.normalize('NFC', s)\n",
    "#         return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "#     except Exception:\n",
    "#         return s\n",
    "\n",
    "# def clean_text(text):\n",
    "#     text = text.replace(\"�\", \"\").replace(\"\u0003\", \"\")  # 잘못된 인코딩 문자 제거\n",
    "#     text = ' '.join(text.split())  # 여러 공백을 하나로 줄임\n",
    "#     return text\n",
    "\n",
    "# def extract_text_and_tables(pdf_path):\n",
    "#     text_content = \"\"\n",
    "#     try:\n",
    "#         # fitz를 사용한 텍스트 추출\n",
    "#         doc = fitz.open(pdf_path)\n",
    "#         for page in doc:\n",
    "#             text = page.get_text(\"text\")\n",
    "#             if text:\n",
    "#                 text = clean_text(normalize_string(text))\n",
    "#                 text_content += text + \"\\n\"\n",
    "\n",
    "#         # pdfplumber를 사용한 표 추출\n",
    "#         with pdfplumber.open(pdf_path) as pdf:\n",
    "#             for page in pdf.pages:\n",
    "#                 tables = page.extract_tables()\n",
    "#                 if tables:\n",
    "#                     text_content += \"\\n__table__\\n\"\n",
    "#                     for table in tables:\n",
    "#                         if table:\n",
    "#                             for row in table:\n",
    "#                                 processed_row = []\n",
    "#                                 for cell in row:\n",
    "#                                     if cell is None:\n",
    "#                                         processed_row.append(\" \")  # 병합된 셀 처리\n",
    "#                                     else:\n",
    "#                                         cell_text = normalize_string(clean_text(str(cell).strip()))\n",
    "#                                         if \"\\n\" in cell_text:\n",
    "#                                             cell_text = \" \".join(cell_text.split())\n",
    "#                                         processed_row.append(cell_text)\n",
    "#                                 row_text = \" | \".join(processed_row)\n",
    "#                                 text_content += row_text + \"\\n\"\n",
    "#                     text_content += \"__end_table__\\n\"\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error extracting text and tables from {pdf_path}: {e}\")\n",
    "#     return text_content\n",
    "\n",
    "# class CustomPDFLoader:\n",
    "#     def __init__(self, file_path):\n",
    "#         self.file_path = file_path\n",
    "\n",
    "#     def load(self):\n",
    "#         documents = []\n",
    "#         try:\n",
    "#             # 텍스트와 표를 모두 추출\n",
    "#             text_content = extract_text_and_tables(self.file_path)\n",
    "\n",
    "#             # 메타데이터 생성 및 Document 객체로 변환\n",
    "#             metadata = {\n",
    "#                 \"source\": self.file_path,\n",
    "#             }\n",
    "#             documents.append(Document(page_content=text_content, metadata=metadata))\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading PDF file {self.file_path}: {e}\")\n",
    "#         return documents\n",
    "\n",
    "# def chunk_documents(documents):\n",
    "#     adjusted_documents = []\n",
    "#     for doc in documents:\n",
    "#         content = doc.page_content\n",
    "#         chunks = []\n",
    "        \n",
    "#         # 표 구분자를 기준으로 분할\n",
    "#         table_sections = content.split(\"__table__\")\n",
    "        \n",
    "#         for section in table_sections:\n",
    "#             if \"__end_table__\" in section:\n",
    "#                 table_content = section.split(\"__end_table__\")[0]\n",
    "#                 chunks.append(table_content)\n",
    "#             else:\n",
    "#                 chunk_splitter = RecursiveCharacterTextSplitter(\n",
    "#                     chunk_size=1000,  # 텍스트 청크 크기를 키움\n",
    "#                     chunk_overlap=200,\n",
    "#                     separators=[\"\\n\\n\", \".\", \"?\", \"!\", \"\\n\"]\n",
    "#                 )\n",
    "#                 normal_chunks = chunk_splitter.split_text(section)\n",
    "#                 chunks.extend(normal_chunks)\n",
    "\n",
    "#         for chunk in chunks:\n",
    "#             adjusted_documents.append(Document(page_content=chunk, metadata=doc.metadata))\n",
    "    \n",
    "#     return adjusted_documents\n",
    "\n",
    "# def format_docs(docs):\n",
    "#     context = \"\"\n",
    "#     for doc in docs:\n",
    "#         context += doc.page_content\n",
    "#         context += '\\n\\n'\n",
    "#     return context\n",
    "\n",
    "# def make_db(df):\n",
    "#     documents = []\n",
    "#     pdf_files = df['Source_path'].unique()\n",
    "\n",
    "#     # tqdm으로 파일 처리 진행 상황 표시\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         results = list(tqdm(executor.map(load_pdf_file, pdf_files), total=len(pdf_files), desc=\"Processing PDFs\"))\n",
    "    \n",
    "#     for result in results:\n",
    "#         documents.extend(result)\n",
    "    \n",
    "#     # 정규화\n",
    "#     for doc in documents:\n",
    "#         doc.page_content = normalize_string(doc.page_content)\n",
    "    \n",
    "#     # 표가 망가지지 않도록 청크 분할 처리\n",
    "#     chunks = chunk_documents(documents)\n",
    "#     print(f\"Total number of chunks: {len(chunks)}\")\n",
    "    \n",
    "#     # FAISS DB 만들기 (메타데이터 포함)\n",
    "#     faiss = FAISS.from_documents(chunks, embedding=get_embedding())\n",
    "#     bm = KiwiBM25Retriever.from_documents(chunks)\n",
    "    \n",
    "#     return faiss, bm\n",
    "\n",
    "# def load_pdf_file(pdf_file):\n",
    "#     try:\n",
    "#         pdf_loader = CustomPDFLoader(pdf_file)\n",
    "#         return pdf_loader.load()\n",
    "#     except Exception as e:\n",
    "#         print(f\"Skipping corrupted file {pdf_file}: {e}\")\n",
    "#         return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 310\n"
     ]
    }
   ],
   "source": [
    "faiss = make_db(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "핵심재정사업 성과관리제도를 안착시키기 위해 필요한 노력과 성과 정보를 학습의 도구로 활용하는 방안은 무엇인가?\n",
      "./test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf\n",
      "{'Header 1': 'ISSUE & FOCUS', 'Header 2': '05', 'Source_path': './test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf'}\n",
      "**정사업 성과관리제도를 안착시켜 나갈 수 있기를 기대**  \n",
      "작업반 간 모범사례 등의 공유를 위한 교훈 저장소 생성 등으로 성과정보가 사업추진기관에 의해 학습의  \n",
      "도구로서 유용하게 쓰이게끔 하고 성과중심의 조직문화를 형성해 가는 것이 필요  \n",
      "-  성공사례와 목표 결과를 공유할 수 있는 다양하고 효과적인 커뮤니케이션 전략을 개발해 범부처 차원의  \n",
      "우선순위 목표에 대한 국민적 관심과 열린 대화를 형성  \n",
      "-----  \n",
      "**FIS** **ISSUE & FOCUS**\n",
      "-------------------\n",
      "{'Header 1': 'ISSUE & FOCUS', 'Header 3': '핵심재정사업 성과관리', 'Source_path': './test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf'}\n",
      "을 일목요연하게 정리해 보고자 한다. 또한 향후 제도를 성공적으로 안착시키기 위해 핵심재정사업 성과관리체계를 어떠한 방향  \n",
      "으로 운영해 나가는 것이 필요할지 살펴보았다.\n",
      "-------------------\n",
      "{'Header 1': 'ISSUE & FOCUS', 'Header 2': '04', 'Source_path': './test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf'}\n",
      "영역에 대한 구현 계획 및 조정을 강화 및 수행하기 위한 팁과 도구를 개발  \n",
      "-  목표 팀 간 모범사례 등의 공유를 위한 교훈 저장소 생성(예를 들어, 목표 팀이 목표를 설정, 계획 및 실행  \n",
      "하는 데 도움이 되는 일련의 전략이 포함된 목표 플레이북을 발행)  \n",
      "성과정보를 사용해 목표 진행 상황을 정기적으로 평가하고, 지속적 개선에 대한 약속 공유  \n",
      "-  데이터 품질 및 사용 개선에 집중하여 조직 프로세스를 지속적으로 개선하고 성과 격차를 식별하며 개선  \n",
      "목표를 설정(성과지표와 목표치 정보는 기초적인 정보로서의 역할을 할 수 있음)  \n",
      "**25) 미국 감사원(Government Accountability Office, GAO)이 제시한 범부처 우선순위(CAP) 목표를 보다 효과적으로 구현하는 데 도움이 되는 5가지 항목의 주요**\n",
      "고려사항 내용을 참고하여 작성  \n",
      "-  성과정보의 유용성을 개선함으로써 더 많은 성과정보의 사용을 촉진할 수 있으며, 의사 결정에서 성과정보를\n",
      "-------------------\n",
      "{'Header 1': 'ISSUE & FOCUS', 'Header 2': '02', 'Source_path': './test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf'}\n",
      "**재정사업 성과관리란 예산이 효과적으로 쓰일 수 있도록 재정사업[1)]의 목표와 성과지표를 사전에 설정하고, 평가를**  \n",
      "**통해 예산에 환류하는 과정을 의미**  \n",
      "OECD, World Bank 등은 2000년대부터 프로그램(program)을 기준으로 예산을 나누어 성과정보를 생산  \n",
      "할 것, 편성과 집행, 환류에 이르는 재정의 전 과정을 성과정보를 바탕으로 관리할 것을 강조하기 시작  \n",
      "-  1990년대 이후 체계적인 성과관리제도 도입에 대한 필요성이 대두되면서 미국, 캐나다, 영국 등 주요국  \n",
      "위주로 재정사업 성과관리의 실효성 제고를 위한 제도 도입 등 노력이 시작  \n",
      "-  이후 2000년대 후반 금융위기로 성과중심 재정관리 강화 움직임이 국제적으로 확산됨에 따라 2007년 기준,  \n",
      "70% 이상의 OECD 국가에서 예결산 문서에 비재무적 성과정보를 포함  \n",
      "우리나라도 2006년 4대 재정개혁(국가재정운용계획, 총액배분자율편성예산, 재정성과관리, 디지털예산회계시스템)을\n",
      "-------------------\n",
      "{'Header 1': 'ISSUE & FOCUS', 'Header 2': '04', 'Source_path': './test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf'}\n",
      "-  성과정보의 유용성을 개선함으로써 더 많은 성과정보의 사용을 촉진할 수 있으며, 의사 결정에서 성과정보를  \n",
      "사용하는 것이 결과를 개선하는 데 필수적  \n",
      "-  징벌적 행위로 만드는 것보다 전략적 검토에서 조직 학습 기회를 통합하도록 권장  \n",
      "현재 및 미래 CAP목표 달성을 위한 부처별 진행 노력들을 국회와 대중에게 명확하고 일관된 정보로 제공  \n",
      "-  장기적인 관리 문제를 해결하기 위해 CAP 목표 진행 상황에 대한 정기적인 보고와 성과측정 및 분기별 목표  \n",
      "개발을 위해 CAP 목표 팀이 취하고 있거나 취할 예정인 조치 보고  \n",
      "-  중기 목표 기간 종료 시점에 CAP 목표를 구현하기 위해 달성한 최종 진행 상황에 대한 공개 보고는 향후  \n",
      "CAP 목표 구현을 알리는 데 도움  \n",
      "-  성공 사례와 목표 결과를 소셜 미디어 플랫폼을 통해 공유하는 등 다양하고 효과적인 커뮤니케이션 전략을  \n",
      "개발해 CAP 목표에 대한 관심과 열린 대화를 형성\n",
      "-------------------\n",
      "{'Header 1': 'ISSUE & FOCUS', 'Header 2': '01', 'Source_path': './test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf'}\n",
      "범부처 차원에서 우선순위 목표를 관리하는 ‘핵심재정사업 성과관리’는 부처의 핵심 업무와 재정운용  \n",
      "성과를 관리하기 위해 도입한 부처별 ‘대표 성과지표 관리’와 구분되어 이해되고 있는가?  \n",
      "현행 핵심재정사업 성과관리체계는 중기 결과목표에 대한 평가를 병행하기 위해 2018년에 도입한 ‘핵심  \n",
      "사업평가’와 어떤 점에서 구분되며, 어떠한 차별화된 기능을 할 것으로 기대되는가?  \n",
      "제도가 성공적으로 안착하고 실제 국정운영을 뒷받침하는 동력으로서 잘 기능하려면, 앞으로 핵심재정사업  \n",
      "성과관리체계는 어떤 방향으로 운영되어 나가야 하는가?\n",
      "-------------------\n",
      "{'Header 1': 'ISSUE & FOCUS', 'Header 2': '05', 'Source_path': './test_source/「FIS 이슈 & 포커스」 23-2호 《핵심재정사업 성과관리》.pdf'}\n",
      "**핵심재정사업 성과관리체계를 통해 정부 국정비전이 반영된 재정사업(군)의 조기 성과창출을 지원해 국정운영**  \n",
      "**동력을 강화할 수 있도록 범부처 우선순위(CAP) 목표 관리에 집중**  \n",
      "윤석열 정부는 향후 5년간(’23~’27년) 집중적으로 관리할 핵심재정사업(군)으로 ‘서민·약자 복지’, ‘미래 대비’,  \n",
      "‘국가의 본질적 기능 강화’의 3대 분야, 12대 재정사업(군)을 선정하고 성과지표·목표를 설정  \n",
      "향후 5년 동안 국민이 체감할 수 있는 가시적인 성과창출을 이룩해 내기 위해서는 선택과 집중 원칙이  \n",
      "필요하며, 목표 및 성과측정의 초기 시점에 주요 이해 관계자 및 고위 관리자들을 참여시킴으로써 그들의  \n",
      "노력과 자원이 최우선 순위에 집중되게 유도하는 것이 중요  \n",
      "**성과정보를 사용해 목표 진행 상황을 정기적으로 평가·공개하고, 지속적 개선에 대한 약속 공유함으로써 핵심재**  \n",
      "**정사업 성과관리제도를 안착시켜 나갈 수 있기를 기대**\n",
      "-------------------\n",
      "2917\n"
     ]
    }
   ],
   "source": [
    "i = 48\n",
    "k = 4\n",
    "\n",
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        # Header 정보를 순서대로 추가\n",
    "        for header_level in range(1, 6):\n",
    "            header_key = f'Header {header_level}'\n",
    "            if header_key in doc.metadata:\n",
    "                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n",
    "        # 문서 내용 추가\n",
    "        context += doc.page_content\n",
    "        context += '\\n---\\n'\n",
    "    return context.strip()\n",
    "\n",
    "\n",
    "docs = faiss.similarity_search(\n",
    "    query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "    filter={\"Source_path\": test_df.iloc[i]['Source_path']},\n",
    "    k = 99,\n",
    "    fetch_k = 20000\n",
    ")\n",
    "buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n",
    "faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n",
    "knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "knn_retriever.k = k\n",
    "bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "bm_retriever.k = k\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr,faiss_retriever_sim,knn_retriever,bm_retriever], weight=[0.25 ,0.25,0.25 ,0.25])\n",
    "\n",
    "print(test_df.iloc[i]['Question'])\n",
    "print(test_df.iloc[i]['Source_path'])\n",
    "\n",
    "# results_bm = bm_retriever.invoke(test_df.iloc[i]['Question'])\n",
    "# results_faiss = faiss_retriever.invoke(test_df.iloc[i]['Question'])\n",
    "# for i in results_bm:\n",
    "#     print(i.metadata)\n",
    "#     print('-------------------')\n",
    "# print('============================================')\n",
    "# for i in results_faiss:\n",
    "#     print(i.metadata)\n",
    "#     print('-------------------')\n",
    "# print('============================================')\n",
    "results = ensemble_retriever.invoke(normalize_string(test_df.iloc[i]['Question']))\n",
    "for i in results:\n",
    "    print(i.metadata)\n",
    "    print(i.page_content)\n",
    "    print('-------------------')\n",
    "print(len(format_docs(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Header 1: 사  업  명\\n(74) 노인장기요양보험 사업운영 (2231-303)\\n---\\nHeader 1: 5. 사업근거 및 추진경위\\n노인장기요양보험법 제35조의2(장기요양기관 재무·회계기준) ① 장기요양기관의 장은 보건복지부령으로 정  \\n하는 재무·회계에 관한 기준(이하 “장기요양기관 재무·회계기준”이라 한다)에 따라 장기요양기관을 투명  \\n하게 운영하여야 한다. 다만, 장기요양기관 중 「사회복지사업법」제34조에 따라 설치한 사회복지시설은  \\n같은 조 제3항에 따른 재무ㆍ회계에 관한 기준에 따른다.  \\n노인장기요양보험법 제58조(국가의 부담) ① 국가는 매년 예산의 범위 안에서 해당 연도 장기요양  \\n보험료 예상수입액의 100분의 20에 상당하는 금액을 공단에 지원한다.\\n② 국가와 지방자치단체는 대통령령으로 정하는 바에 따라 의료급여수급권자의 장기요양급여\\n비용, 의사소견서 발급비용, 방문간호지시서 발급비용 중 공단이 부담하여야 할 비용(제40조제\\n1항 단서 및 제3항제1호에 따라 면제 및 감경됨으로 인하여공단이 부담하게 되는 비용을 포\\n함한다) 및 관리운영비의 전액을 부담한다.\\n---\\nHeader 1: 7. 사업 집행절차\\n○ 노인장기요양보험 운영지원  \\n○ 공무원·사립학교교원 등 장기요양보험료 국가부담금  \\n○ 기타의료급여 수급권자 급여비용 국가부담금  \\n○ 장기요양기관 재무회계 프로그램 운영  \\n부처 피출연·피보조 기관  \\n보건복지부 ⇒ 한국사회보장정보원  \\n(972백만원) (972백만원) (972백만원)  \\n-----\\n---'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_docs(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_ensemble_retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_df)):\n\u001b[1;32m      3\u001b[0m     query \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuestion\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mtest_ensemble_retriever\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(query)\n\u001b[1;32m      5\u001b[0m     chunk_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(i\u001b[38;5;241m.\u001b[39mpage_content) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m      6\u001b[0m     total\u001b[38;5;241m.\u001b[39mextend(chunk_sizes)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_ensemble_retriever' is not defined"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "for i in range(len(test_df)):\n",
    "    query = test_df.iloc[i]['Question']\n",
    "    results = test_ensemble_retriever.invoke(query)\n",
    "    chunk_sizes = [len(i.page_content) for i in results]\n",
    "    total.extend(chunk_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1875.1284403669724"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21년 국정감사에서 에너지 바우처 사업에 대한 주요 지적사항은 무엇이었나요?\n",
      "./test_source/산업통상자원부_에너지바우처.pdf\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m             adjusted_docs\u001b[38;5;241m.\u001b[39mappend(doc)\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m adjusted_docs\n\u001b[1;32m---> 31\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mequal_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSource_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m  results:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mmetadata)\n",
      "Cell \u001b[1;32mIn[18], line 28\u001b[0m, in \u001b[0;36mequal_path\u001b[1;34m(contexts, source_path)\u001b[0m\n\u001b[0;32m     26\u001b[0m adjusted_docs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m contexts:\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfile_path\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m source_path:\n\u001b[0;32m     29\u001b[0m         adjusted_docs\u001b[38;5;241m.\u001b[39mappend(doc)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adjusted_docs\n",
      "\u001b[1;31mKeyError\u001b[0m: 'file_path'"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n",
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    i = 1\n",
    "    for doc in docs:\n",
    "        context += f\"Document: {i}\" +\"Source:\"+ doc.metadata['file_path'] +'\\n'\n",
    "        context += doc.page_content\n",
    "        context += '\\n---\\n'\n",
    "        i += 1\n",
    "    return context\n",
    "\n",
    "\n",
    "i= 35\n",
    "\n",
    "query = test_df.iloc[i]['Question']\n",
    "print(query)\n",
    "print(test_df.iloc[i]['Source_path'])\n",
    "results = test_ensemble_retriever.invoke(normalize_string(query))\n",
    "def equal_path(contexts, source_path):\n",
    "    adjusted_docs = []\n",
    "    for doc in contexts:\n",
    "        if doc.metadata['file_path'] == source_path:\n",
    "            adjusted_docs.append(doc)\n",
    "    return adjusted_docs\n",
    "results = equal_path(results, test_df.iloc[i]['Source_path'])\n",
    "for result in  results:\n",
    "    print(result.metadata)\n",
    "    print(result.page_content)\n",
    "    print('-------------------')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
