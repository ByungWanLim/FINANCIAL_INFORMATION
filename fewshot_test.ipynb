{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import os\n",
    "from glob import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFacePipeline ,HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import (\n",
    "    FewShotPromptTemplate\n",
    ")\n",
    "import pickle\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='intfloat/multilingual-e5-small',\n",
    "        model_kwargs={'device': 'cuda'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "    return embeddings\n",
    "\n",
    "def load_and_vectorize(csv_path, db_path):\n",
    "    if os.path.exists(db_path) and os.path.exists(db_path + '_metadata.pkl'):\n",
    "        print(\"Loading FAISS DB from:\", db_path)\n",
    "        db, metadata = load_faiss_db(db_path)\n",
    "        return db\n",
    "\n",
    "    # CSV 파일을 불러와 데이터프레임 생성\n",
    "    train_df = pd.read_csv(csv_path)\n",
    "    train_df.drop('SAMPLE_ID', axis=1, inplace=True)\n",
    "    trainset = train_df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", trainset[:3])\n",
    "    \n",
    "    # 벡터화할 텍스트 생성\n",
    "    to_vectorize = [\"\\n\\n\".join(example.values()) for example in trainset]\n",
    "    \n",
    "    # 벡터화 및 FAISS DB 생성\n",
    "    fewshow_vectordb = FAISS.from_texts(to_vectorize, embedding=get_embedding(), metadatas=trainset)\n",
    "    \n",
    "    # FAISS DB 저장\n",
    "    save_faiss_db(fewshow_vectordb, db_path)\n",
    "    \n",
    "    return fewshow_vectordb\n",
    "\n",
    "def save_faiss_db(db, db_path):\n",
    "    db.save_local(db_path)\n",
    "    with open(db_path + '_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(db, f)\n",
    "\n",
    "def load_faiss_db(db_path):\n",
    "    db = FAISS.load_local(db_path, embeddings=get_embedding(), allow_dangerous_deserialization=True)\n",
    "    with open(db_path + '_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return db, metadata\n",
    "\n",
    "def load_chunks_make_docdb(pdf_directory, db_path):\n",
    "    if os.path.exists(db_path) and os.path.exists(db_path + '_metadata.pkl'):\n",
    "        print(\"Loading FAISS DB from:\", db_path)\n",
    "        db, metadata = load_faiss_db(db_path)\n",
    "        return db\n",
    "\n",
    "    print(\"Loading PDF files from:\", pdf_directory)\n",
    "    documents = []\n",
    "\n",
    "    # PDF 파일들을 로드하여 분할\n",
    "    pdf_files = glob(os.path.join(pdf_directory, '*.pdf').replace('\\\\', '/'))\n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        pdf_documents = loader.load()\n",
    "        documents.extend(pdf_documents)\n",
    "    \n",
    "    # 분할된 텍스트를 벡터로 변환하여 FAISS DB에 저장\n",
    "    chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = chunk_splitter.split_documents(documents)\n",
    "    print(\"Done.\", len(chunks), \"chunks\")\n",
    "    \n",
    "    print(\"Creating FAISS DB\")\n",
    "    # FAISS DB 생성 및 저장\n",
    "    db = FAISS.from_documents(chunks, embedding=get_embedding())\n",
    "    save_faiss_db(db, db_path)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return db\n",
    "\n",
    "def make_dict(dir='train.csv'):\n",
    "    df = pd.read_csv(dir)\n",
    "    df.drop('SAMPLE_ID', axis=1, inplace=True)\n",
    "    \n",
    "    return df.to_dict(orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS DB from: ./fewshot_faiss_db\n",
      "Loading FAISS DB from: ./train_faiss_db\n",
      "Loading FAISS DB from: ./test_faiss_db\n"
     ]
    }
   ],
   "source": [
    "fewshot_db = load_and_vectorize('train.csv', './fewshot_faiss_db')\n",
    "\n",
    "\n",
    "train_db = load_chunks_make_docdb('./train_source', './train_faiss_db')\n",
    "train_retriever = train_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})\n",
    "\n",
    "test_db = load_chunks_make_docdb('./test_source', './test_faiss_db')\n",
    "test_retriever = test_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})\n",
    "\n",
    "train_dict = make_dict('train.csv')\n",
    "test_dict = make_dict('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에너지바우처 사업의 주요 수혜자는 누구인가요?\n",
      "산업통상자원부_에너지바우처\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "def make_fewshot_prompt(fewshot_vectordb, k = 3):\n",
    "    # Semantic Similarity Example Selector 설정\n",
    "    example_prompt = PromptTemplate.from_template(\"Question: {Question}\\nAnswer: {Answer}\\nSource: {Source}\")\n",
    "\n",
    "    example_selector = SemanticSimilarityExampleSelector(\n",
    "        vectorstore=fewshot_vectordb,\n",
    "        k=k,\n",
    "    )\n",
    "\n",
    "    # FewShotPromptTemplate 생성\n",
    "    fewshot_prompt = FewShotPromptTemplate(\n",
    "        example_selector=example_selector,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"Question: {input}\",\n",
    "        input_variables=[\"input\"],\n",
    "    )\n",
    "    return fewshot_prompt\n",
    "\n",
    "def make_fewshot_string(fewshot_prompt, train_retriever, buff):\n",
    "    ex_qa = fewshot_prompt.invoke({\"input\": buff['Question']}).to_string()\n",
    "    fewshot_list = ex_qa.split('\\n\\n')[:-1]\n",
    "    for i, entry in enumerate(fewshot_list):\n",
    "        question = entry.split('\\n')[0]\n",
    "        question = question.replace('Question: ', '')\n",
    "        retrieved_docs = train_retriever.invoke(question)\n",
    "        entry += retrieved_docs[0].page_content\n",
    "        num = \"Example {}\\n\".format(i+1)\n",
    "        fewshot_list[i] = num + entry + '\\n\\n'\n",
    "    return str(fewshot_list)\n",
    "\n",
    "fewshot_prompt = make_fewshot_prompt(fewshot_db)\n",
    "i = 24\n",
    "print(test_dict[i]['Question'])\n",
    "print(test_dict[i]['Source'])\n",
    "print('==============================')\n",
    "\n",
    "\n",
    "ex_qa = fewshot_prompt.invoke({\"input\": test_dict[i]['Question']}).to_string()\n",
    "fewshot_list = ex_qa.split('\\n\\n')[:-1]\n",
    "for i, entry in enumerate(fewshot_list):\n",
    "    question = entry.split('\\n')[0]\n",
    "    question = question.replace('Question: ', '')\n",
    "    retrieved_docs = train_retriever.invoke(question)\n",
    "    entry += retrieved_docs[0].page_content\n",
    "    num = \"Example {}\\n\".format(i+1)\n",
    "    fewshot_list[i] = num + entry + '\\n\\n'\n",
    "print(str(fewshot_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, entry in enumerate(fewshot_list):\n",
    "#     question = entry.split('\\n')[0]\n",
    "#     question = question.replace('Question: ', '')\n",
    "#     retrieved_docs = train_retriever.invoke(question)\n",
    "#     entry += retrieved_docs[0].page_content\n",
    "#     num = \"Example {}\\n\".format(i+1)\n",
    "#     fewshot_list[i] = num + entry + '\\n\\n'\n",
    "\n",
    "    \n",
    "# for i in fewshot_list:\n",
    "#     print(i)\n",
    "#     print('==============================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fewshot_str = str(fewshot_list)\n",
    "# fewshot_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(test_dict)):\n",
    "#     print(test_dict[i]['Question'])\n",
    "#     print(test_dict[i]['Source'])\n",
    "    \n",
    "#     fewshot_str = make_fewshot_string(fewshot_prompt, train_retriever, test_dict[i])\n",
    "#     print(fewshot_str)\n",
    "#     print('==============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def setup_llm_pipeline(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128)\n",
    "    hf = HuggingFacePipeline(pipeline=pipe)\n",
    "    return hf\n",
    "\n",
    "def run():\n",
    "    fewshot_db = load_and_vectorize('train.csv', './fewshot_faiss_db')\n",
    "    fewshot_prompt = make_fewshot_prompt(fewshot_db)\n",
    "    \n",
    "    train_db = load_chunks_make_docdb('./train_source', './train_faiss_db')\n",
    "    train_retriever = train_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})\n",
    "    \n",
    "    test_db = load_chunks_make_docdb('./test_source', './test_faiss_db')\n",
    "    test_retriver = test_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})\n",
    "    \n",
    "    train_dict = make_dict('train.csv')\n",
    "    test_dict = make_dict('test.csv')\n",
    "    \n",
    "    llm = setup_llm_pipeline(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "    results =[]\n",
    "    for i in tqdm(range(len(test_dict))):\n",
    "        print(test_dict[i]['Question'])\n",
    "        fewshot_str = make_fewshot_string(fewshot_prompt, train_retriever, test_dict[i])\n",
    "        # print(fewshot_str)\n",
    "        full_template = \"\"\"\n",
    "        너는 사람들에게 방대한 재정 데이터를 일반 국민과 전문가들이 이해할 수 있는 방식으로 전달하고 싶어한다.\n",
    "        너는 이를 위한 몇가지 예시 참고해서  재정 보고서, 예산 설명자료, 기획재정부 보도자료 등 다양한 재정 관련 텍스트 데이터를 활용해 주어진 질문에 대해 정확도가 높은 응답을 제시해줘.\n",
    "        반드시 context를 참고하여 답변을 작성해야해.\n",
    "        예시를 참고하여 아래 질문에 대한 답변을 작성해주세요.\n",
    "        \"\"\"+f\"\"\"\n",
    "        이건 질문과 관련된 예시야.\n",
    "        {fewshot_str}\n",
    "        \"\"\"+\"\"\"\n",
    "        \n",
    "        이 context를 참고하여 아래 질문에 대한 답변을 작성해주세요.\n",
    "        {context}\n",
    "\n",
    "        질문은 이것이다.\n",
    "        {input}\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(full_template)\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"context\": test_retriver | format_docs,\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"Question\": test_dict[i]['Question'],\n",
    "            \"Answer\": qa_chain.invoke(test_dict[i]['Question']),\n",
    "            \"Source\": test_dict[i]['Source']\n",
    "        \n",
    "            })\n",
    "        print(\"================================================\")\n",
    "        print(\"Questions: \",results[-1]['Question'])\n",
    "        print(\"Answer: \",results[-1]['Answer'])\n",
    "        #print(results[-1]['Source'])\n",
    "        \n",
    "    # 제출용 샘플 파일 로드\n",
    "    submit_df = pd.read_csv(\"./sample_submission.csv\")\n",
    "    # 생성된 답변을 제출 DataFrame에 추가\n",
    "    submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "    submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "    # 결과를 CSV 파일로 저장\n",
    "    submit_df.to_csv(\"./baseline_submission.csv\", encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS DB from: ./fewshot_faiss_db\n",
      "Loading FAISS DB from: ./train_faiss_db\n",
      "Loading FAISS DB from: ./test_faiss_db\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4639d29c542422e9f5b96cfb9f94619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2bee5f36a4411eaa867f434e8f3088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317e5bbf6ab3466c9d5cf1f678baca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/3.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run():\n",
    "    fewshot_db = load_and_vectorize('train.csv', './fewshot_faiss_db')\n",
    "    fewshot_prompt = make_fewshot_prompt(fewshot_db)\n",
    "    \n",
    "    train_db = load_chunks_make_docdb('./train_source', './train_faiss_db')\n",
    "    train_retriever = train_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})\n",
    "    \n",
    "    test_db = load_chunks_make_docdb('./test_source', './test_faiss_db')\n",
    "    test_retriver = test_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})\n",
    "    \n",
    "    train_dict = make_dict('train.csv')\n",
    "    test_dict = make_dict('test.csv')\n",
    "    \n",
    "    llm = setup_llm_pipeline(\"beomi/Llama-3-Open-Ko-8B\")\n",
    "    results =[]\n",
    "    for i in tqdm(range(len(test_dict))):\n",
    "        print(test_dict[i]['Question'])\n",
    "        fewshot_str = make_fewshot_string(fewshot_prompt, train_retriever, test_dict[i])\n",
    "        # print(fewshot_str)\n",
    "        full_template = \"\"\"\n",
    "        너는 사람들에게 방대한 재정 데이터를 일반 국민과 전문가들이 이해할 수 있는 방식으로 전달하고 싶어한다.\n",
    "        너는 이를 위한 몇가지 예시 참고해서  재정 보고서, 예산 설명자료, 기획재정부 보도자료 등 다양한 재정 관련 텍스트 데이터를 활용해 주어진 질문에 대해 정확도가 높은 응답을 제시해줘.\n",
    "        반드시 context를 참고하여 답변을 작성해야해.\n",
    "        예시를 참고하여 아래 질문에 대한 답변을 작성해주세요.\n",
    "        \"\"\"+f\"\"\"\n",
    "        이건 질문과 관련된 예시야.\n",
    "        {fewshot_str}\n",
    "        \"\"\"+\"\"\"\n",
    "        \n",
    "        이 context를 참고하여 아래 질문에 대한 답변을 작성해주세요.\n",
    "        {context}\n",
    "\n",
    "        질문은 이것이다.\n",
    "        {input}\n",
    "        \"\"\"\n",
    "        prompt = PromptTemplate.from_template(full_template)\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"context\": test_retriver | format_docs,\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            \"Question\": test_dict[i]['Question'],\n",
    "            \"Answer\": qa_chain.invoke(test_dict[i]['Question']),\n",
    "            \"Source\": test_dict[i]['Source']\n",
    "        \n",
    "            })\n",
    "        print(\"================================================\")\n",
    "        print(\"Questions: \",results[-1]['Question'])\n",
    "        print(\"Answer: \",results[-1]['Answer'])\n",
    "        #print(results[-1]['Source'])\n",
    "        \n",
    "    # 제출용 샘플 파일 로드\n",
    "    submit_df = pd.read_csv(\"./sample_submission.csv\")\n",
    "    # 생성된 답변을 제출 DataFrame에 추가\n",
    "    submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "    submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "    # 결과를 CSV 파일로 저장\n",
    "    submit_df.to_csv(\"./baseline_submission.csv\", encoding='UTF-8-sig', index=False)\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
