{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import os\n",
    "from glob import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import (\n",
    "    FewShotPromptTemplate\n",
    ")\n",
    "import pickle\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'\n",
    "#model_id ='meta-llama/Meta-Llama-3.1-8B-Instruct'\n",
    "\n",
    "def get_embedding():\n",
    "    model_kwargs = {'device': 'cuda'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='intfloat/multilingual-e5-small',\n",
    "        model_kwargs={'device': 'cuda'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_vectorize(csv_path, db_path):\n",
    "    if os.path.exists(db_path) and os.path.exists(db_path + '_metadata.pkl'):\n",
    "        print(\"Loading FAISS DB from:\", db_path)\n",
    "        db, metadata = load_faiss_db(db_path)\n",
    "        return db\n",
    "\n",
    "    # CSV 파일을 불러와 데이터프레임 생성\n",
    "    train_df = pd.read_csv(csv_path)\n",
    "    train_df.drop('SAMPLE_ID', axis=1, inplace=True)\n",
    "    trainset = train_df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", trainset[:3])\n",
    "    \n",
    "    # 벡터화할 텍스트 생성\n",
    "    to_vectorize = [\"\\n\\n\".join(example.values()) for example in trainset]\n",
    "    \n",
    "    # 벡터화 및 FAISS DB 생성\n",
    "    fewshow_vectordb = FAISS.from_texts(to_vectorize, embedding=get_embedding(), metadatas=trainset)\n",
    "    \n",
    "    # FAISS DB 저장\n",
    "    save_faiss_db(fewshow_vectordb, db_path)\n",
    "    \n",
    "    return fewshow_vectordb\n",
    "\n",
    "def save_faiss_db(db, db_path):\n",
    "    db.save_local(db_path)\n",
    "    with open(db_path + '_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(db, f)\n",
    "\n",
    "def load_faiss_db(db_path):\n",
    "    db = FAISS.load_local(db_path, embeddings=get_embedding(), allow_dangerous_deserialization=True)\n",
    "    with open(db_path + '_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return db, metadata\n",
    "\n",
    "def load_chunks_make_docdb(pdf_directory, db_path):\n",
    "    if os.path.exists(db_path) and os.path.exists(db_path + '_metadata.pkl'):\n",
    "        print(\"Loading FAISS DB from:\", db_path)\n",
    "        db, metadata = load_faiss_db(db_path)\n",
    "        return db\n",
    "\n",
    "    print(\"Loading PDF files from:\", pdf_directory)\n",
    "    documents = []\n",
    "\n",
    "    # PDF 파일들을 로드하여 분할\n",
    "    pdf_files = glob(os.path.join(pdf_directory, '*.pdf').replace('\\\\', '/'))\n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        pdf_documents = loader.load()\n",
    "        documents.extend(pdf_documents)\n",
    "    \n",
    "    # 분할된 텍스트를 벡터로 변환하여 FAISS DB에 저장\n",
    "    chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = chunk_splitter.split_documents(documents)\n",
    "    print(\"Done.\", len(chunks), \"chunks\")\n",
    "    \n",
    "    print(\"Creating FAISS DB\")\n",
    "    # FAISS DB 생성 및 저장\n",
    "    db = FAISS.from_documents(chunks, embedding=get_embedding())\n",
    "    save_faiss_db(db, db_path)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS DB from: ./fewshot_faiss_db\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "FAISS.__init__() missing 4 required positional arguments: 'embedding_function', 'index', 'docstore', and 'index_to_docstore_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fewshot_vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_vectorize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./fewshot_faiss_db\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# save_faiss_db(fewshot_vectordb, 'train_faiss_db')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load and index documents from both PDF and text files\u001b[39;00m\n\u001b[0;32m      5\u001b[0m train_vectordb \u001b[38;5;241m=\u001b[39m load_chunks_make_docdb(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./train_source\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./train_faiss_db\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mload_and_vectorize\u001b[1;34m(csv_path, db_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(db_path) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(db_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_metadata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading FAISS DB from:\u001b[39m\u001b[38;5;124m\"\u001b[39m, db_path)\n\u001b[1;32m---> 17\u001b[0m     db, metadata \u001b[38;5;241m=\u001b[39m \u001b[43mload_faiss_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m db\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# CSV 파일을 불러와 데이터프레임 생성\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36mload_faiss_db\u001b[1;34m(db_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_faiss_db\u001b[39m(db_path):\n\u001b[1;32m----> 2\u001b[0m     faiss \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     db \u001b[38;5;241m=\u001b[39m faiss\u001b[38;5;241m.\u001b[39mload_local(db_path, embeddings\u001b[38;5;241m=\u001b[39mget_embedding(), allow_dangerous_deserialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(db_path \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_metadata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mTypeError\u001b[0m: FAISS.__init__() missing 4 required positional arguments: 'embedding_function', 'index', 'docstore', and 'index_to_docstore_id'"
     ]
    }
   ],
   "source": [
    "fewshot_vectordb = load_and_vectorize('train.csv', './fewshot_faiss_db')\n",
    "# save_faiss_db(fewshot_vectordb, 'train_faiss_db')\n",
    "\n",
    "# Load and index documents from both PDF and text files\n",
    "train_vectordb = load_chunks_make_docdb('./train_source', './train_faiss_db')\n",
    "# save_faiss_db(test_vectordb, './train_faiss_db')\n",
    "\n",
    "# Load and index documents from both PDF and text files\n",
    "# save_faiss_db(train_vectordb, './train_faiss_db')\n",
    "test_vectordb = load_chunks_make_docdb('./test_source','./test_faiss_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': '1-1 2024 주요 재정통계 1권',\n",
       " 'Source_path': './train_source/1-1 2024 주요 재정통계 1권.pdf',\n",
       " 'Question': '2024년 중앙정부의 예산 지출은 어떻게 구성되어 있나요?',\n",
       " 'Answer': '2024년 중앙정부의 예산 지출은 일반회계 356.5조원, 21개 특별회계 81.7조원으로 구성되어 있습니다.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "train_df.drop('SAMPLE_ID', axis=1, inplace=True)\n",
    "trainset = train_df.to_dict(orient='records')\n",
    "trainset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Source': '중소벤처기업부_혁신창업사업화자금(융자)',\n",
       " 'Source_path': './test_source/중소벤처기업부_혁신창업사업화자금(융자).pdf',\n",
       " 'Question': '중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 무엇인가요?'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "test_df.drop('SAMPLE_ID', axis=1, inplace=True)\n",
    "testset = test_df.to_dict(orient='records')\n",
    "testset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: 창업사업화지원의 사업목적은 무엇인가?\n",
      "Answer: 창업사업화지원의 사업목적은 창업기업의 성장단계별, 초격차 분야별, 글로벌화 지원체계를 구축‧운영하여 혁신 기술창업을 활성화하고 창업기업 성장 및 생존율 제고하는 것이다.\n",
      "Source: 중소벤처기업부_창업사업화지원\n",
      "\n",
      "Question: 언제 청년 일자리 대책의 일환으로 기술혁신 기반 예비창업자의 사업화를 지원하는 '기술혁신형 창업기업 지원사업'이 신설되었나요?\n",
      "Answer: 기술혁신형 창업기업 지원사업(현, 예비창업패키지)'은 '18년에 신설되었습니다.\n",
      "Source: 중소벤처기업부_창업사업화지원\n",
      "\n",
      "Question: 2024년 중소벤처기업부의 창업사업화지원 사업에서 어떤 기관들이 사업시행주체로 참여하는가?\n",
      "Answer: 중소벤처기업부, 창업진흥원이 사업시행주체이다.\n",
      "Source: 중소벤처기업부_창업사업화지원\n",
      "\n",
      "Question: 중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 무엇인가요?\n"
     ]
    }
   ],
   "source": [
    "fewshot_vectordb,_ = load_faiss_db('./fewshot_faiss_db')\n",
    "train_vectordb,_ = load_faiss_db('./train_faiss_db')\n",
    "test_vectordb,_ = load_faiss_db('./test_faiss_db')\n",
    "\n",
    "# Semantic Similarity Example Selector 설정\n",
    "example_prompt = PromptTemplate.from_template(\"Question: {Question}\\nAnswer: {Answer}\\nSource: {Source}\")\n",
    "# print(example_prompt.invoke(trainset[1]).to_string())\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=fewshot_vectordb,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "# FewShotPromptTemplate 생성\n",
    "fewshot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Question: {input}\",\n",
    "    input_variables=[\"input\"],\n",
    ")\n",
    "\n",
    "# 프롬프트 사용 예제\n",
    "test_input = testset[1]['Question']\n",
    "ex_qa = fewshot_prompt.invoke({\"input\": test_input}).to_string()\n",
    "print(ex_qa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainset[1])\n",
    "train_retriever = train_vectordb.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})\n",
    "retrieved_docs = train_retriever.invoke(testset[1]['Question'])\n",
    "\n",
    "print(len(retrieved_docs))\n",
    "retrieved_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fewshot 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_qa = fewshot_prompt.invoke({\"input\": testset[1]['Question']}).to_string()\n",
    "fewshot_list = ex_qa.split('\\n\\n')[:-1]\n",
    "\n",
    "for i, entry in enumerate(fewshot_list):\n",
    "    entry_dict = dict([entry.split(': ') for entry in entry.split('\\n')])\n",
    "    retrieved_docs = train_retriever.invoke(entry_dict['Question'])\n",
    "    entry_dict['context'] = retrieved_docs[0].page_content\n",
    "    fewshot_list[i] = entry_dict\n",
    "    \n",
    "examples = []\n",
    "for entry in fewshot_list:\n",
    "    question = entry['Question']\n",
    "    answer = entry['Answer']\n",
    "    source = entry['Source']\n",
    "    context_str = entry['context']\n",
    "    example_str = f\"Question: {question}\\nAnswer: {answer}\\nSource: {source}\\nContext: {context_str}\"\n",
    "    examples.append(example_str)\n",
    "    \n",
    "# examples 리스트를 하나의 문자열로 결합\n",
    "examples_str = \"\\n\\n\".join(examples)\n",
    "example_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PipelinePromptTemplate, PromptTemplate\n",
    "\n",
    "full_template = \"\"\"\n",
    "너는 사람들에게 방대한 재정 데이터를 일반 국민과 전문가들이 이해할 수 있는 방식으로 전달하고 싶어한다.\n",
    "너는 이를 위한 몇가지 예시 참고해서  재정 보고서, 예산 설명자료, 기획재정부 보도자료 등 다양한 재정 관련 텍스트 데이터를 활용해 주어진 질문에 대해 정확도가 높은 응답을 제시해줘.\n",
    "반드시 context를 참고하여 답변을 작성해야해.\n",
    "예시를 참고하여 아래 질문에 대한 답변을 작성해주세요.\n",
    "\n",
    "이건 질문과 관련된 예시야.\n",
    "{example}\n",
    "\n",
    "이 context를 참고하여 아래 질문에 대한 답변을 작성해주세요.\n",
    "{context}\n",
    "\n",
    "질문은 이것이다.\n",
    "{input}\"\"\"\n",
    "prompt = PromptTemplate.from_template(full_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline(model_id):\n",
    "\n",
    "    model_id = model_id\n",
    "\n",
    "    # 토크나이저 로드 및 설정\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "\n",
    "    # 모델 로드 및 양자화 설정 적용\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True )\n",
    "\n",
    "    # HuggingFacePipeline 객체 생성\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.2,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "\n",
    "    hf = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return hf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [01:21<00:00, 13.57s/it]\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_pipeline(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    )\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128)\n",
    "    hf = HuggingFacePipeline(pipeline=pipe)\n",
    "    return hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "train_retriever = train_vectordb.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})\n",
    "test_retriver = test_vectordb.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1, 'fetch_k': 50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98 [00:00<?, ?it/s]c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    for i in tqdm(range(len(testset))):\n",
    "        ex_qa = fewshot_prompt.invoke({\"input\": testset[i]['Question']}).to_string()\n",
    "        fewshot_list = ex_qa.split('\\n\\n')[:-1]\n",
    "        for i, entry in enumerate(fewshot_list):\n",
    "            entry_dict = dict([entry.split(': ') for entry in entry.split('\\n')])\n",
    "            retrieved_docs = train_retriever.invoke(entry_dict['Question'])\n",
    "            entry_dict['context'] = retrieved_docs[0].page_content\n",
    "            fewshot_list[i] = entry_dict\n",
    "            \n",
    "        examples = []\n",
    "        for entry in fewshot_list:\n",
    "            question = entry['Question']\n",
    "            answer = entry['Answer']\n",
    "            source = entry['Source']\n",
    "            context_str = entry['context']\n",
    "            example_str = f\"Question: {question}\\nAnswer: {answer}\\nSource: {source}\\nContext: {context_str}\"\n",
    "            examples.append(example_str)\n",
    "            \n",
    "        # examples 리스트를 하나의 문자열로 결합\n",
    "        examples_str = \"\\n\\n\".join(examples)\n",
    "        full_template = \"\"\"\n",
    "        너는 사람들에게 방대한 재정 데이터를 일반 국민과 전문가들이 이해할 수 있는 방식으로 전달하고 싶어한다.\n",
    "        너는 이를 위한 몇가지 예시 참고해서  재정 보고서, 예산 설명자료, 기획재정부 보도자료 등 다양한 재정 관련 텍스트 데이터를 활용해 주어진 질문에 대해 정확도가 높은 응답을 제시해줘.\n",
    "        반드시 context를 참고하여 답변을 작성해야해.\n",
    "        예시를 참고하여 아래 질문에 대한 답변을 작성해주세요.\n",
    "        \"\"\" +f\"\"\"\n",
    "        이건 질문과 관련된 예시야.\n",
    "        {examples_str}\n",
    "        \"\"\"+\"\"\"\n",
    "        이 context를 참고하여 아래 질문에 대한 답변을 작성해주세요.\n",
    "        {context}\n",
    "\n",
    "        질문은 이것이다.\n",
    "        {input}\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate.from_template(full_template)\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"context\": test_retriver | format_docs,\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(\"./sample_submission.csv\")\n",
    "\n",
    "# 생성된 답변을 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'] for item in results]\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(\"./baseline_submission.csv\", encoding='UTF-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
