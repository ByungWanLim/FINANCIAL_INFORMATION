{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import os\n",
    "from glob import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline ,HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import (\n",
    "    FewShotPromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate\n",
    ")\n",
    "import bitsandbytes as bnb\n",
    "import pickle\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "\n",
    "from faiss_module import load_and_vectorize,load_chunks_make_docdb\n",
    "from model import setup_llm_pipeline\n",
    "from save import save\n",
    "from seed import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS DB from: ./fewshot_faiss_db\n",
      "Loading FAISS DB from: ./train_faiss_db\n",
      "Loading FAISS DB from: ./test_faiss_db\n"
     ]
    }
   ],
   "source": [
    "def make_dict(dir='train.csv'):\n",
    "    df = pd.read_csv(dir)\n",
    "    df.drop('SAMPLE_ID', axis=1, inplace=True)\n",
    "    \n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "def make_fewshot_prompt(fewshot_vectordb, k = 10):\n",
    "    # Semantic Similarity Example Selector 설정\n",
    "    example_prompt = PromptTemplate.from_template(\"<|start_header_id|>user<|end_header_id|>: <|begin_of_text|>{Question}<|end_of_text|>\\n<|start_header_id|>assistant<|end_header_id|>: <|begin_of_text|>{Answer}<|end_of_text|>\")\n",
    "\n",
    "    example_selector = SemanticSimilarityExampleSelector(\n",
    "        vectorstore=fewshot_vectordb,\n",
    "        k=k,\n",
    "    )\n",
    "\n",
    "    # FewShotPromptTemplate 생성\n",
    "    fewshot_prompt = FewShotPromptTemplate(\n",
    "        example_selector=example_selector,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"Question: {input}\",\n",
    "        input_variables=[\"input\"],\n",
    "    )\n",
    "    return fewshot_prompt\n",
    "\n",
    "def make_fewshot_string(fewshot_prompt, train_retriever, buff):\n",
    "    ex_qa = fewshot_prompt.invoke({\"input\": buff['Question']}).to_string()\n",
    "    fewshot_list = ex_qa.split('\\n\\n')[:-1]\n",
    "    for i, entry in enumerate(fewshot_list):\n",
    "        question = entry.split('\\n')[0]\n",
    "        question = question.replace('Question: ', '')\n",
    "        retrieved_docs = train_retriever.invoke(question)\n",
    "        num = \"Example {}\\n\".format(i+1)\n",
    "        fewshot_list[i] = num + \"<|start_header_id|>context<|end_header_id|>: <|begin_of_text|>\" + entry + '<|end_of_text|>\\n\\n##################################################'\n",
    "    return str(fewshot_list)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        context += doc.page_content\n",
    "        context += '\\n\\n'\n",
    "    return context\n",
    "\n",
    "def extract_answer(response):\n",
    "    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        if line.startswith('Answer:'):\n",
    "            return line.replace('Answer:', '').strip()\n",
    "        if line.startswith('assistant:'):\n",
    "            return line.replace('assistant:', '').strip()\n",
    "    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "fewshot_db = load_and_vectorize('train.csv', './fewshot_faiss_db')\n",
    "fewshot_prompt = make_fewshot_prompt(fewshot_db)\n",
    "\n",
    "train_db = load_chunks_make_docdb('./train_source', './train_faiss_db')\n",
    "train_retriever = train_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1})\n",
    "\n",
    "test_db = load_chunks_make_docdb('./test_source', './test_faiss_db')\n",
    "test_retriver = test_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 3})\n",
    "\n",
    "train_dict = make_dict('train.csv')\n",
    "test_dict = make_dict('test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:18<00:00,  4.69s/it]\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q33================================================\n",
      "Questions:  에너지바우처 사업의 향후 기대효과는 무엇인가요?\n",
      "Answer:  에너지바우처 사업의 향후 기대 효과로는 재정 관리 강화가 있습니다. '국가재정법' 개정이 이루어졌으며, 동 법에 따른 재정 사업 평가와 개별 법령에 따르며 수행되는 평가 대상 중복을 최소화하기 위해 규정을 신선했습니다.\n",
      "Q35================================================\n",
      "Questions:  21년 국정감사에서 에너지 바우처 사업에 대한 주요 지적사항은 무엇이었나요?\n",
      "Answer:  저소득층 폭염피해 최소화를 위해 동절기 에너지바우처 일부를 하절기에 사용할 수 있도록 제도개선을 해야한다는 것이었습니다.\n",
      "Q37================================================\n",
      "Questions:  에너지 바우처 사업의 향후 추진방향 중 '취약계층의 에너지 비용 부담 완화'를 위한 계획은 무엇이었나요?\n",
      "Answer:  에너지 바우처 사업의 취약 계층 에너지 비용 부담 완화를 위해 다음과 같은 계획들이 있었던 것 같습니다.\n",
      "\n",
      "1. **중증 및 희귀난치성 질환자 세대의 추가**: 2018년부터 중증 및 희귀난치성 질환자를 위한 에너지 바우처 서비스를 제공하기 시작했습니다.\n",
      "2. **한부모 및 소년소녀 가정 세대의 추가**: 2019년부터 한부모 또는 소년소녀 가정의 아동들을 위한 에너지 바우처 서비스를 제공하기 시작했습니다.\n",
      "\n",
      "이러한 노력들은 취약 계층에 대한 에너지 비용 부담을 줄이고 그들의 삶을 더 편안하게 만드는 것을 목표로 하고 있습니다.\n",
      "Q48================================================\n",
      "Questions:  핵심재정사업 성과관리제도를 안착시키기 위해 필요한 노력과 성과 정보를 학습의 도구로 활용하는 방안은 무엇인가?\n",
      "Answer:  핵심재정사업 성과관리제도의 안착을 위하여 필요한 노력으로는 다음과 같은 것들이 있다.\n",
      "\n",
      "1.  다양한 커뮤니케이션 전략 개발 : 성공 사례와 목표 결과를 공유할 수 있는 다원적이고 효과적인 커뮤니케이션 전략을 개발하여 범 부처 차원의 우선 순위 목표에 대한 국민적 관심과 열린 대화를 형성해야 한다.\n",
      "\n",
      "2.  성과 정보 저장소 생성 : 교훈을 담아두는 장소로 성과 정보가 사업 추진 기관에 의해 학습의 도구로서 유용하게 쓰일 수 있도록 하여 성과 중심의 조직 문화를 형성해야 한다.\n",
      "\n",
      "\n",
      "\n",
      "따라서 핵심재정사업 성과 관리 체계 운영 방향으로는 재정 성과 관리 결과를 재정 운용에 반영하고, 또 국민에게 알기 쉬운 홍보를 할 수 있도록 공개하기 위한 지속 적 인 조치를 필요로 합니다.\n",
      "Q57================================================\n",
      "Questions:  재정성과관리제도는 어떤 측면에서 국정운영과 연결되는가?\n",
      "Answer:  재정성과관리제도의 목적은 국가 재정을 효율적으로 운용하고 투명하게 관리하는 것이다. 따라서, 재정성과관리제도가 국정운영과 깊은 연관성을 가지고 있다.\n",
      "Q62================================================\n",
      "Questions:  어떤 국제기구들이 사업을 기준으로 예산을 나누어 성과 정보를 생산하고 있는가?\n",
      "Answer:  OECD(Organization for Economic Co-operation and Development), 세계 은행(World Bank)이 있습니다.\n",
      "Q70================================================\n",
      "Questions:  재정성과관리제도의 중요성과 국정운영과의 연결성은 무엇인가?\n",
      "Answer:  재정성과관리제도가 중요한 이유는 국가 재정을 효율적으로 운용하고 투명하게 관리할 수 있게 하기 때문입니다. 또한, 재정성과관리제도로 인해 국정운영을 위한 정보가 제공되고, 이를 바탕으로 정책 결정이 이루어진다는 점에서 국정운영과의 연결성을 가질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def run(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "\n",
    "    \n",
    "    llm = setup_llm_pipeline(model_id)\n",
    "    # reordering = LongContextReorder()\n",
    "    results =[]\n",
    "    iii = [33,35,37,48,57,62,70]\n",
    "    for i in (iii):\n",
    "        \n",
    "        fewshot_str = make_fewshot_string(fewshot_prompt, train_retriever, test_dict[i])\n",
    "        # print(fewshot_str)\n",
    "        \n",
    "        full_template = \"\"\"\n",
    "##################################################\n",
    "You are the financial literacy expert who helps me with my financial literacy Q&As.\n",
    "You earn 10 points when you answer me and follow the rules and lose 12 points when you don't.\n",
    "##################################################\n",
    "\n",
    "\"\"\" +\"\"\"Here are some rules you should follow.\n",
    "\n",
    "Rule 1: Be sure to utilize retrieved contexts for your answers.\n",
    "Rule 2: The most important thing is to be concise and relevant in your answers. \n",
    "Rule 3: Answers must be written in Korean.\n",
    "Rule 4: Use fewer than 128 words.\n",
    "Rule 5: If you can't answer that, try summarizing the context and make it a 1-2 Sentence summary.\n",
    "\n",
    "##################################################\n",
    "context: \n",
    "{context}\n",
    "\n",
    "<|start_header_id|>user<|end_header_id|>: <|begin_of_text|>{input}<|end_of_text|>\n",
    "\n",
    "<|start_header_id|>assistant<|end_header_id|>: \n",
    "\"\"\"\n",
    "        prompt = PromptTemplate.from_template(full_template)\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"context\": test_retriver | format_docs,\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "        print(f\"Q{i}================================================\")\n",
    "        print(\"Questions: \",test_dict[i]['Question'])\n",
    "        answer = qa_chain.invoke(test_dict[i]['Question'])\n",
    "        #answer = extract_answer(answer)\n",
    "        results.append({\n",
    "            \"Question\": test_dict[i]['Question'],\n",
    "            \"Answer\": answer,\n",
    "            \"Source\": test_dict[i]['Source']\n",
    "            })\n",
    "        print(\"Answer: \",results[-1]['Answer'])\n",
    "        #print(results[-1]['Source'])\n",
    "run(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
