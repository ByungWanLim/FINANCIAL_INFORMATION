{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2614,"status":"ok","timestamp":1724343204767,"user":{"displayName":"임병완","userId":"16912505565738020902"},"user_tz":-540},"id":"c98hhB05X6ZF","outputId":"ad19e09b-9bce-4d06-81cc-ac7c182f6f1b"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1724343204767,"user":{"displayName":"임병완","userId":"16912505565738020902"},"user_tz":-540},"id":"DdMyghv3X6ZJ","outputId":"31166a35-f62b-474f-84ce-8fc0dd523001"},"outputs":[],"source":["# %cd /content/drive/Othercomputers/Laptop/FINANCIAL_INFORMATION"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"rpl9xn_aX6ZK"},"outputs":[],"source":["# !pip install -q -r requirements.txt\n","# !pip install --upgrade -q transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"6_0APDsumbJO"},"outputs":[],"source":["# !huggingface-cli login"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gAACYOpkX6ZK"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# Model\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, LlamaForCausalLM\n","from langchain_huggingface import HuggingFacePipeline\n","import torch\n","from langchain_core.output_parsers import StrOutputParser\n","from langchain_core.runnables import RunnablePassthrough\n","from langchain.prompts import PromptTemplate\n","# Vector stores\n","import fitz  # PyMuPDF\n","import pdfplumber\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","#from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter, MarkdownHeaderTextSplitter\n","from langchain.text_splitter import MarkdownHeaderTextSplitter\n","#from langchain_community.retrievers import BM25Retriever, KNNRetriever\n","from langchain.retrievers import EnsembleRetriever\n","from langchain_community.vectorstores import FAISS\n","from langchain_huggingface import HuggingFaceEmbeddings\n","#from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader ,UnstructuredPDFLoader\n","from langchain_community.retrievers import BM25Retriever, KNNRetriever\n","from langchain.retrievers import EnsembleRetriever\n","#from langchain_teddynote.retrievers import KiwiBM25Retriever, OktBM25Retriever\n","from langchain_teddynote.retrievers import OktBM25Retriever\n","from langchain.docstore.document import Document\n","#from concurrent.futures import ThreadPoolExecutor\n","import pandas as pd\n","import unicodedata\n","import pymupdf4llm\n","#import time\n","#import re\n","#from konlpy.tag import Okt\n","#from pdf2image import convert_from_path\n","import pytesseract\n","from konlpy.tag import Kkma\n","# etc\n","#import os\n","import pandas as pd\n","from tqdm import tqdm\n","import unicodedata\n","import logging\n","#from PyPDF2 import PdfReader\n","#import json\n","from seed_module import seed_everything\n","\n","seed_everything(42)\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n","print(device)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"wQ6fm_FtX6ZM"},"outputs":[],"source":["# intfloat/multilingual-e5-small\n","# jhgan/ko-sroberta-multitask\n","\n","def get_embedding():\n","\n","    embeddings = HuggingFaceEmbeddings(\n","        model_name='jhgan/ko-sroberta-multitask',\n","        model_kwargs={'device': device},\n","\n","        encode_kwargs={'normalize_embeddings': True})\n","    return embeddings\n","def normalize_string(s):\n","    try:\n","        normalized = unicodedata.normalize('NFC', s)\n","        return normalized.encode('utf-8', errors='replace').decode('utf-8')\n","    except Exception:\n","        return s\n","def clean_text(text):\n","    text = text.replace(\"�\", \" \").replace(\"\u0003\", \" \")  # 잘못된 인코딩 문자 제거\n","    return text\n","\n","def format_docs(docs):\n","    context = \"\"\n","\n","    for doc in docs:\n","        # Header 정보를 순서대로 추가\n","        for header_level in range(1, 6):\n","            header_key = f'Header{header_level}'\n","            if header_key in doc.metadata:\n","                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n","        # 문서 내용 추가\n","        context += doc.page_content\n","        context += '\\n---\\n'\n","    return context\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"CiJxnDx1X6ZN"},"outputs":[],"source":["def process_pdf(pdf_path):\n","    md_text = pymupdf4llm.to_markdown(pdf_path)\n","\n","    headers_to_split_on = [\n","        (\"#\", \"Header 1\"),\n","        (\"##\", \"Header 2\"),\n","        (\"###\", \"Header 3\"),\n","        (\"####\", \"Header 4\"),\n","        (\"#####\", \"Header 5\"),\n","        (\"######\", \"Header 6\")\n","    ]\n","\n","    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=True)\n","    md_chunks = md_header_splitter.split_text(md_text)\n","\n","    text_splitter = RecursiveCharacterTextSplitter(\n","        chunk_size=300, chunk_overlap=60\n","    )\n","\n","    splits = text_splitter.split_documents(md_chunks)\n","    \n","\n","    for i in splits:\n","        \n","        metadata = {'Source_path': pdf_path}\n","        i.metadata = {**i.metadata, **metadata}\n","    return splits\n","\n","\n","def make_db(df):\n","    documents = []\n","\n","    pdf_files = df['Source_path'].unique()\n","    for pdf_file in tqdm(pdf_files):\n","        # 문서 로드\n","        documents.extend(process_pdf(pdf_file))\n","\n","    print(f\"Total number of documents: {len(documents)}\")\n","\n","    faiss = FAISS.from_documents(documents, embedding=get_embedding())\n","    return faiss"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-kbbq7E5X6ZO"},"outputs":[],"source":["def fewshot_db(df):\n","    df = df.drop('SAMPLE_ID', axis=1)\n","    df = df.drop('Source_path', axis=1)\n","    df = df.to_dict(orient='records')\n","    print(\"Loaded Fewshot Set:\", len(df))\n","    to_vectorize = [\"\\n\\n\".join(normalize_string(value) for value in example.values()) for example in df]\n","    faiss = FAISS.from_texts(to_vectorize, embedding=get_embedding())\n","    # bm = BM25Retriever.from_texts(to_vectorize)\n","    # knn = KNNRetriever.from_texts(to_vectorize, embeddings=get_embedding())\n","    return faiss"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"eQGqgXNfX6ZP"},"outputs":[],"source":["train_df = pd.read_csv('train.csv', encoding='utf-8')\n","test_df = pd.read_csv('test.csv', encoding='utf-8')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"ATjsG5SMX6ZP"},"outputs":[],"source":["def format_docs(docs):\n","    context = \"\"\n","    for doc in docs:\n","        # Header 정보를 순서대로 추가\n","        for header_level in range(1, 6):\n","            header_key = f'Header {header_level}'\n","            if header_key in doc.metadata:\n","                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n","        # 문서 내용 추가\n","        context += doc.page_content\n","        context += '\\n---\\n'\n","    return context"]},{"cell_type":"markdown","metadata":{"id":"BpRPDeKIX6ZQ"},"source":["### Model"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.28s/it]\n"]}],"source":["def setup_llm_pipeline(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n","    # 토크나이저 로드 및 설정\n","        # 양자화 설정 적용\n","    bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=torch.bfloat16\n","    )\n","    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,low_cpu_mem_usage=True)\n","\n","    # # 일부 중요한 레이어는 FP16으로 유지\n","    # for name, module in model.named_modules():\n","    #     if \"attention\" in name or \"ffn\" in name:  # 중요한 레이어 식별 (예: attention 및 ffn)\n","    #         module.to(torch.float16)  # 이 부분은 16비트로 유지\n","\n","    tokenizer = AutoTokenizer.from_pretrained(model_id, clean_up_tokenization_spaces=True)\n","    terminators = [\n","    tokenizer.eos_token_id,\n","    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","    ]\n","\n","    text_generation_pipeline = pipeline(\n","        model=model,\n","        tokenizer=tokenizer,\n","        task=\"text-generation\",\n","        return_full_text=False,\n","        max_new_tokens=1024,\n","        eos_token_id = terminators,\n","        pad_token_id = tokenizer.eos_token_id\n","    )\n","\n","    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n","\n","    return llm\n","# ghost-x/ghost-8b-beta-1608\n","# OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k\n","llm = setup_llm_pipeline()"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"pFPoRjz5X6Zb"},"outputs":[],"source":["# from transformers import QuantoConfig\n","\n","# def setup_llm_pipeline(model_id=\"hugging-quants/Meta-Llama-3.1-8B-Instruct-GPTQ-INT4\"):\n","\n","#     # 모델 로드 및 양자화 적용\n","#     model = AutoModelForCausalLM.from_pretrained(\n","#     model_id,\n","#     torch_dtype=torch.float16,\n","#     low_cpu_mem_usage=True,\n","#     device_map=\"auto\",\n","#     )\n","\n","#     tokenizer = AutoTokenizer.from_pretrained(model_id)\n","#     terminators = [\n","#     tokenizer.eos_token_id,\n","#     tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n","#     ]\n","\n","#     text_generation_pipeline = pipeline(\n","#         model=model,\n","#         tokenizer=tokenizer,\n","#         task=\"text-generation\",\n","#         return_full_text=False,\n","#         max_new_tokens=1024,\n","#         eos_token_id = terminators,\n","#         pad_token_id = tokenizer.eos_token_id\n","#     )\n","\n","#     llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n","\n","#     return llm\n","# # ghost-x/ghost-8b-beta-1608\n","# # OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k\n","# llm = setup_llm_pipeline()"]},{"cell_type":"markdown","metadata":{"id":"3EknRGK9X6Zc"},"source":["### 점수"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"OcCU3ZRWX6Zd"},"outputs":[],"source":["from collections import Counter\n","def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n","\n","    #공백 제거\n","    true_sentence = ''.join(true_sentence.split())\n","    predicted_sentence = ''.join(predicted_sentence.split())\n","\n","    true_counter = Counter(true_sentence)\n","    predicted_counter = Counter(predicted_sentence)\n","\n","    #문자가 등장한 개수도 고려\n","    if sum_mode:\n","        true_positive = sum((true_counter & predicted_counter).values())\n","        predicted_positive = sum(predicted_counter.values())\n","        actual_positive = sum(true_counter.values())\n","\n","    #문자 자체가 있는 것에 focus를 맞춤\n","    else:\n","        true_positive = len((true_counter & predicted_counter).values())\n","        predicted_positive = len(predicted_counter.values())\n","        actual_positive = len(true_counter.values())\n","\n","    #f1 score 계산\n","    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n","    recall = true_positive / actual_positive if actual_positive > 0 else 0\n","    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","    return precision, recall, f1_score\n","\n","def calculate_average_f1_score(true_sentences, predicted_sentences):\n","\n","    total_precision = 0\n","    total_recall = 0\n","    total_f1_score = 0\n","\n","    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n","        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n","        total_precision += precision\n","        total_recall += recall\n","        total_f1_score += f1_score\n","\n","    avg_precision = total_precision / len(true_sentences)\n","    avg_recall = total_recall / len(true_sentences)\n","    avg_f1_score = total_f1_score / len(true_sentences)\n","\n","    return {\n","        'average_precision': avg_precision,\n","        'average_recall': avg_recall,\n","        'average_f1_score': avg_f1_score\n","    }"]},{"cell_type":"markdown","metadata":{"id":"WJryDd-GX6Zd"},"source":["### RUN"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"6-tmUuelX6Zd"},"outputs":[],"source":["\n","def extract_answer(response):\n","    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n","    lines = response.split('\\n')\n","    for line in lines:\n","        line = line.replace('**', '')\n","        if line.startswith('Answer:'):\n","            return line.replace('Answer:', '').strip()\n","        if line.startswith('assistant:'):\n","            return line.replace('assistant:', '').strip()\n","    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n","\n","def rerun(question,context,answer,llm,num_repeat):\n","    full_template = \"<|begin_of_text|>\"\n","    full_template += \"\"\"<|start_header_id|>system<|end_header_id|>당신은 이전 답변을 검증하는 챗봇입니다. 질문과 문맥, 이전 답변을 참고해서 지시사항을 따르세요. 지시사항을 따를 때 서론 없이 출력하세요.<|eot_id|>\"\"\"\n","    full_template += f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {question} \\n\\nContexts: {context} \\n\\nPrevious Answer: {answer} \\n\\n\"\"\"\n","    full_template += \"\"\"{input}<|eot_id|>\"\"\"\n","    full_template += \"\"\"<|start_header_id|>assistant<|end_header_id|>\"\"\"\n","\n","    prompt = PromptTemplate(template=full_template)\n","    chain = (\n","    {\n","        \"input\": RunnablePassthrough(),\n","    }\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n","    )\n","    torch.cuda.empty_cache()\n","    return chain.invoke(\"핵심 단어들을 바탕으로, 한 문장으로 요약하세요. 만약 한 문장이라면 그대로 출력하세요.\")\n","\n","def run(faiss,dataset,llm,k=3,verbose=False):\n","    results = []\n","    source_path = dataset.iloc[0]['Source_path']\n","    docs = faiss.similarity_search(\n","    query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n","    filter={\"Source_path\": source_path},\n","    k = 99,\n","    fetch_k = 20000\n","    )\n","    buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n","    faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n","    # faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n","    # knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n","    # knn_retriever.k = k\n","    bm_retriever = OktBM25Retriever.from_documents(docs)\n","    bm_retriever.k = k\n","\n","    ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr,bm_retriever], weight=[0.5,0.5])\n","\n","    \n","    for i, row in (dataset.iterrows()):\n","        if source_path != row['Source_path']:   \n","            source_path = row['Source_path']\n","            docs = faiss.similarity_search(\n","                query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n","                filter={\"Source_path\": source_path},\n","                k = 99,\n","                fetch_k = 20000\n","            )\n","            buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n","            faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n","            # faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n","            # knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n","            # knn_retriever.k = k\n","            bm_retriever = OktBM25Retriever.from_documents(docs)\n","            bm_retriever.k = k\n","\n","            ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr,bm_retriever], weight=[0.5,0.5])\n","\n","            \n","        full_template = \"<|begin_of_text|>\"\n","        full_template += \"\"\"<|start_header_id|>system<|end_header_id|>\n","당신은 유용한 금융 정보 QnA 챗봇입니다.\n","질문을 명확히 파악하세요. 모든 질문에 답변하세요.\n","정확한 답변을 반드시 요구합니다.\n","답변을 지어내지 마세요.\n","질문과 직접적인 관련이 없는 내용은 제외하세요.\n","간결하게 작성하세요.<|eot_id|>\n","\"\"\"\n","        question = row['Question']          \n","        # full_template += \"\"\" \"\"\"\n","        contexts = ensemble_retriever.invoke(normalize_string(question))\n","        contexts = format_docs(contexts)\n","        \n","        full_template += \"<|start_header_id|>user<|end_header_id|>\"\n","        full_template += \"Question: {input}\\n\\n<|eot_id|>\"\n","        full_template += \"Contexts: {contexts}\"\n","        full_template += \"\"\"<|eot_id|>\"\"\"\n","        full_template += \"\"\"<|start_header_id|>assistant<|end_header_id>\"\"\"\n","        \n","        prompt = PromptTemplate(template=full_template, input_variables=[\"contexts\",\"input\"])\n","        qa_chain = (\n","        {\n","            \"contexts\": RunnablePassthrough(),\n","            \"input\": RunnablePassthrough(),\n","        }\n","        | prompt\n","        | llm\n","        | StrOutputParser()\n","        )\n","\n","        answer = qa_chain.invoke({'input':question,'contexts':contexts})\n","        answer = extract_answer(answer)\n","        lines = answer.split('\\n')\n","        if  len(lines) > 1 or '|' in answer or ':' in answer or '다음과' in answer:\n","            previous = answer\n","            try:\n","                before = calculate_f1_score(row['Answer'],answer)[2]\n","            except:\n","                before = None\n","            answer = rerun(question=question,\n","                           context=contexts,\n","                           answer=answer,\n","                           llm=llm,\n","                           num_repeat=1)\n","        answer = extract_answer(answer)\n","        results.append({\n","            \"Question\": question,\n","            \"Answer\": answer,\n","            \"Source\": row['Source']\n","        })\n","        if verbose:\n","            print(f\"{i}/{len(dataset)}\")\n","            print(\"Question: \", question, end=\" | \")\n","            print(\"Context Number |\",len(contexts))\n","            try:\n","                print(calculate_f1_score(row['Answer'],answer)[2],end=\" | \")\n","            except:\n","                pass\n","            print(\"Answer: \", results[-1]['Answer'])\n","            try:\n","                print(\"Before: \",before,\" | \",previous)  \n","                \n","                previous = None\n","                before = None\n","            except:\n","                pass\n","            \n","            try:\n","                print(\"REAL Answer: \",row['Answer'])\n","            except:\n","                pass\n","            \n","            print()\n","        torch.cuda.empty_cache()\n","    return results"]},{"cell_type":"markdown","metadata":{"id":"wpEp-dhDX6Ze"},"source":["### 케이폴드"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"BsP2dAnHX6Zf"},"outputs":[],"source":["# from sklearn.model_selection import KFold\n","# import copy\n","\n","# # weight = [0.3,0.3,0.4]\n","# # train_faiss_db,knn_retriever ,train_bm_retrievier = make_db(train_df)\n","\n","# # train_k = 3\n","# # train_bm_retrievier.k = train_k\n","# # knn_retriever.k = train_k\n","# # train_faiss_retriever = train_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':train_k} )\n","# # train_ensemble_retriever = EnsembleRetriever(\n","# #     retrievers=[train_bm_retrievier, knn_retriever,train_faiss_retriever], weights=weight , search_kwargs={'k':train_k}\n","# # )\n","\n","\n","# fewshot_k = 3\n","\n","# k_folds = 4\n","# fold_results = []\n","# kf = KFold(n_splits=k_folds, shuffle=True, random_state=52)\n","# for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n","#     fold_result = []\n","#     # train_set = train_df.iloc[train_index]\n","#     val_set = train_df.iloc[val_index]\n","\n","#     faiss = make_db(val_set)\n","\n","#     pred = run(faiss, val_set, llm, verbose=True)\n","#     result = pd.DataFrame()\n","#     result['pred'] = [result['Answer'] for result in pred]\n","#     val_set.index = range(len(val_set))\n","#     result['gt'] = val_set['Answer']\n","\n","#     result = calculate_average_f1_score(result['gt'], result['pred'])\n","#     print(result)\n","#     fold_results.append(result)\n","#     break"]},{"cell_type":"markdown","metadata":{"id":"w6cRyhfnX6Zf"},"source":["### 실전"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"THntFWRyX6Zf"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 9/9 [00:20<00:00,  2.23s/it]\n"]},{"name":"stdout","output_type":"stream","text":["Total number of documents: 523\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pook0\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","C:\\Users\\pook0\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\llama\\modeling_llama.py:653: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n","  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"]},{"name":"stdout","output_type":"stream","text":["0/98\n","Question:  2022년 혁신창업사업화자금(융자)의 예산은 얼마인가요? | Context Number | 596\n","Answer:  2022년 혁신창업사업화자금(융자)의 예산은 2,300,000만원입니다.\n","\n","1/98\n","Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 무엇인가요? | Context Number | 864\n","Answer:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 기술력과 사업성이 우수하고 미래 성장가능성이 높은 중소벤처기업의 창업을 활성화하고 고용창출을 도모하고, 중소기업이 보유한 우수 기술의 상업화를 방지하고 개발기술의 제품화 및 사업화를 촉진하여 기술기반 중소기업을 육성하는 것입니다.\n","Before:  None  |  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 다음과 같습니다.\n","\n","1.  기술력과 사업성이 우수하고 미래 성장가능성이 높은 중소벤처기업의 창업을 활성화하고 고용창출을 도모합니다.\n","2.  중소기업이 보유한 우수 기술의 상업화를 방지하고 개발기술의 제품화 및 사업화를 촉진하여 기술기반 중소기업을 육성합니다.\n","\n","2/98\n","Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업근거는 어떤 법률에 근거하고 있나요? | Context Number | 1335\n","Answer:  중소벤처기업부의 혁신창업사업화자금(융자) 사업근거는 중소기업진흥에 관한 법률 제66조, 제67조, 제74조, 중소기업창업지원법 제35조에 근거하고 있습니다.\n","Before:  None  |  None\n","\n","3/98\n","Question:  2010년에 신규 지원된 혁신창업사업화자금은 무엇인가요? | Context Number | 714\n","Answer:  2010년에 신규 지원된 혁신창업사업화자금은 재창업자금(실패 경영인에 대한 재기지원)이란 것입니다.\n","Before:  None  |  None\n","\n","4/98\n","Question:  혁신창업사업화자금 중 2020년에 신규 지원된 자금은 무엇인가요? | Context Number | 714\n","Answer:  2020년에 신규 지원된 혁신창업사업화자금은 미래기술육성자금과 고성장촉진자금입니다.\n","Before:  None  |  None\n","\n","5/98\n","Question:  재창업자금이 재도약지원자금으로 이관된 연도는 언제인가요? | Context Number | 971\n","Answer:  재창업자금이 재도약지원자금으로 이관된 연도는 2015년입니다.\n","Before:  None  |  None\n","\n","6/98\n","Question:  창업기반지원과 신청 대상이 중복인 자금이 어떤 것이며, 이 자금이 폐지된 연도는 언제인가요? | Context Number | 1152\n","Answer:  창업기반지원과 신청 대상이 중복인 자금이 일자리창출촉진자금입니다. 이 자금은 2023년 1월에 폐지되었습니다.\n","Before:  None  |  None\n","\n","7/98\n","Question:  혁신창업사업화자금(융자) 사업을 시행하는 주체는 누구인가요? | Context Number | 832\n","Answer:  혁신창업사업화자금(융자) 사업을 시행하는 주체는 중소벤처기업진흥공단입니다.\n","Before:  None  |  None\n","\n"]},{"name":"stderr","output_type":"stream","text":["You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"name":"stdout","output_type":"stream","text":["8/98\n","Question:  혁신창업사업화자금(융자) 사업 집행절차는 어떻게 되나요? | Context Number | 975\n","Answer:  혁신창업사업화자금(융자) 사업 집행절차는 사전상담 및 신청, 사업계획수립/공고, 융자 실행 지원결정통보 평가 및 승인, 융자 실행 순입니다.\n","Before:  None  |  혁신창업사업화자금(융자) 사업 집행절차는 다음과 같습니다.\n","\n","1. 사전상담 및 신청\n","2. 사업계획수립/공고\n","3. 융자 실행 지원결정통보 평가 및 승인\n","4. 융자 실행\n","\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pook0\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["9/98\n","Question:  부모급여 지원 사업의 목적은 무엇인가요? | Context Number | 1091\n","Answer:  부모급여 지원 사업의 목적은 출산 및 양육으로 손실되는 소득을 보전하고, 주 양육자의 직접 돌봄이 중요한 아동 발달의 특성에 따라 영아기 돌봄을 두텁게 지원하기 위해 부모급여 지급입니다.\n","Before:  None  |  None\n","\n","10/98\n","Question:  부모급여(영아수당)의 2024년 확정된 예산은 몇백만원인가요? | Context Number | 993\n","Answer:  2024 년 부모급여(영아수당)의 확정된 예산은 2,888,694 백만원입니다.\n","Before:  None  |  None\n","\n","11/98\n","Question:  부모급여 지원 사업은 어떤 법령상 근거를 갖고 추진되고 있나요? | Context Number | 1349\n","Answer:  부모급여 지원 사업은 아동수당법 제4조 제5항에 근거하고 있습니다.\n","Before:  None  |  None\n","\n","12/98\n","Question:  영아수당 도입에 대한 추진경위는 어떻게 되나요? | Context Number | 1110\n","Answer:  영아수당 도입의 추진경위는 예비타당성조사 통과, 근거법 마련, 영아수당 지원사업 시행, 대통령 공약사항 및 국정과제 포함, 부모급여 지급으로 가정양육 지원 및 부모의 경제적 부담 완화 등으로 진행되었습니다.\n","Before:  None  |  영아수당 도입에 대한 추진경위는 다음과 같습니다.\n","\n","1.  예비타당성조사 통과(2021년 8월)\n","2.  근거법 마련(2021년 12월)\n","3.  영아수당 지원사업 시행(2022년 1월부터)\n","4.  '부모급여 도입' 대통령 공약사항 및 국정과제에 포함(2022년 5월)\n","5.  0~11개월 아동에 대한 부모급여 지급으로 가정양육 지원 및 부모의 경제적 부담을 획기적으로 완화(2023년 70만원, 2024년 100만원으로 단계적 확대)\n","\n","이러한 추진경위로 부모급여(영아수당) 지원 사업이 추진되고 있습니다.\n","\n","13/98\n","Question:  부모급여 지원사업은 언제부터 시행되었나요? | Context Number | 995\n","Answer:  2023년 1월부터 부모급여 지원사업이 시행되었습니다.\n","Before:  None  |  None\n","\n","14/98\n","Question:  보건복지부의 부모급여(영아수당) 지원 사업시행방법은 무엇이며, 사업 수혜자는 누구인가? | Context Number | 780\n","Answer:  보건복지부의 부모급여(영아수당) 지원 사업시행방법은 지자체 보조이며, 사업 수혜자는 만 0~1세 아동입니다.\n","Before:  None  |  None\n","\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\pook0\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["15/98\n","Question:  노인장기요양보험 사업 운영에 대한 목적은 무엇인가요? | Context Number | 1364\n","Answer:  노인장기요양보험 사업 운영의 목적은 노인이나 고령자들이 일상생활을 혼자서 수행하기 어려운 경우 신체 또는 가사 활동을 제공하는 노인장기요양보험에 대한 국고 지원을 통해 효율적인 정책 추진을 통해 노후의 건강 증진 및 생활 안정 도모와 가족의 부담 완화 및 국민 삶의 질 향상을 목적으로 합니다.\n","Before:  None  |  None\n","\n"]}],"source":["def save(results):\n","    # 제출용 샘플 파일 로드\n","    submit_df = pd.read_csv(\"./sample_submission.csv\")\n","    # 생성된 답변을 제출 DataFrame에 추가\n","    submit_df['Answer'] = [item['Answer'] for item in results]\n","    submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")     # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [ 주의 ]\n","    time = pd.Timestamp.now()\n","    # 결과를 CSV 파일로 저장\n","    submit_df.to_csv(f\"./submission_bw/sub_{time.month}_{time.day}_{time.hour}{time.minute}.csv\", encoding='UTF-8-sig', index=False)\n","    print(\"저장 완료\")\n","\n","faiss= make_db(test_df)\n","\n","results = run(faiss, test_df, llm, verbose=True)\n","save(results)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
