{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, LlamaForCausalLM\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Vector stores\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "#from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "#from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader ,UnstructuredPDFLoader\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "#from langchain_teddynote.retrievers import KiwiBM25Retriever, OktBM25Retriever\n",
    "from langchain_teddynote.retrievers import OktBM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "#from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import pymupdf4llm\n",
    "#import time\n",
    "#import re\n",
    "#from konlpy.tag import Okt\n",
    "#from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from konlpy.tag import Kkma\n",
    "# etc\n",
    "#import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import logging\n",
    "#from PyPDF2 import PdfReader\n",
    "#import json\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intfloat/multilingual-e5-small\n",
    "# jhgan/ko-sroberta-multitask\n",
    "\n",
    "def get_embedding():\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='jhgan/ko-sroberta-multitask',\n",
    "        model_kwargs={'device': device},\n",
    "        \n",
    "        encode_kwargs={'normalize_embeddings': True})\n",
    "    return embeddings\n",
    "def normalize_string(s):\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFC', s)\n",
    "        return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"�\", \" \").replace(\"\u0003\", \" \")  # 잘못된 인코딩 문자 제거\n",
    "    return text\n",
    "\n",
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Header 정보를 순서대로 추가\n",
    "        for header_level in range(1, 6):\n",
    "            header_key = f'Header{header_level}'\n",
    "            if header_key in doc.metadata:\n",
    "                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n",
    "        # 문서 내용 추가\n",
    "        context += doc.page_content\n",
    "        context += '\\n---\\n'\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path): \n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "        (\"#####\", \"Header 5\"),\n",
    "        (\"######\", \"Header 6\")\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=True)\n",
    "    md_chunks = md_header_splitter.split_text(md_text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=256, chunk_overlap=64\n",
    "    )\n",
    "\n",
    "    splits = text_splitter.split_documents(md_chunks)\n",
    "\n",
    "    for i in splits:\n",
    "        metadata = {'Source_path': pdf_path}\n",
    "        i.metadata = {**i.metadata, **metadata}\n",
    "    return splits\n",
    "\n",
    "\n",
    "def make_db(df):\n",
    "    documents = []\n",
    "    \n",
    "    pdf_files = df['Source_path'].unique()\n",
    "    for pdf_file in tqdm(pdf_files):\n",
    "        # 문서 로드\n",
    "        documents.extend(process_pdf(pdf_file))\n",
    "        \n",
    "    print(f\"Total number of documents: {len(documents)}\")\n",
    "\n",
    "    faiss = FAISS.from_documents(documents, embedding=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_db(df):\n",
    "    df = df.drop('SAMPLE_ID', axis=1)\n",
    "    df = df.drop('Source_path', axis=1)\n",
    "    df = df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", len(df))\n",
    "    to_vectorize = [\"\\n\\n\".join(normalize_string(value) for value in example.values()) for example in df]\n",
    "    faiss = FAISS.from_texts(to_vectorize, embedding=get_embedding())\n",
    "    # bm = BM25Retriever.from_texts(to_vectorize)\n",
    "    # knn = KNNRetriever.from_texts(to_vectorize, embeddings=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        # Header 정보를 순서대로 추가\n",
    "        for header_level in range(1, 6):\n",
    "            header_key = f'Header {header_level}'\n",
    "            if header_key in doc.metadata:\n",
    "                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n",
    "        # 문서 내용 추가\n",
    "        context += doc.page_content\n",
    "        context += '\\n---\\n'\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.43s/it]\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_pipeline(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    # 양자화 설정 적용\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,  # 기본적으로 4비트로 로드\n",
    "        bnb_4bit_use_double_quant=True,  # 두 번 양자화 적용\n",
    "        bnb_4bit_quant_type=\"nf4\",  # 4비트 양자화 유형 선택\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16  # 연산은 bf16으로 수행\n",
    "    )\n",
    "\n",
    "    # 모델 로드\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id, \n",
    "        quantization_config=bnb_config,\n",
    "        #low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    # # 일부 중요한 레이어는 FP16으로 유지\n",
    "    # for name, module in model.named_modules():\n",
    "    #     if \"attention\" in name or \"ffn\" in name:  # 중요한 레이어 식별 (예: attention 및 ffn)\n",
    "    #         module.to(torch.float16)  # 이 부분은 16비트로 유지\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id = terminators,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return llm\n",
    "# ghost-x/ghost-8b-beta-1608\n",
    "# OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n",
    "\n",
    "    #공백 제거\n",
    "    true_sentence = ''.join(true_sentence.split())\n",
    "    predicted_sentence = ''.join(predicted_sentence.split())\n",
    "    \n",
    "    true_counter = Counter(true_sentence)\n",
    "    predicted_counter = Counter(predicted_sentence)\n",
    "\n",
    "    #문자가 등장한 개수도 고려\n",
    "    if sum_mode:\n",
    "        true_positive = sum((true_counter & predicted_counter).values())\n",
    "        predicted_positive = sum(predicted_counter.values())\n",
    "        actual_positive = sum(true_counter.values())\n",
    "\n",
    "    #문자 자체가 있는 것에 focus를 맞춤\n",
    "    else:\n",
    "        true_positive = len((true_counter & predicted_counter).values())\n",
    "        predicted_positive = len(predicted_counter.values())\n",
    "        actual_positive = len(true_counter.values())\n",
    "\n",
    "    #f1 score 계산\n",
    "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_average_f1_score(true_sentences, predicted_sentences):\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n",
    "        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1_score += f1_score\n",
    "    \n",
    "    avg_precision = total_precision / len(true_sentences)\n",
    "    avg_recall = total_recall / len(true_sentences)\n",
    "    avg_f1_score = total_f1_score / len(true_sentences)\n",
    "    \n",
    "    return {\n",
    "        'average_precision': avg_precision,\n",
    "        'average_recall': avg_recall,\n",
    "        'average_f1_score': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def extract_answer(response):\n",
    "#     # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "#     lines = response.split('\\n')\n",
    "#     for line in lines:\n",
    "#         line = line.replace('**', '')\n",
    "#         if line.startswith('Answer:'):\n",
    "#             return line.replace('Answer:', '').strip()\n",
    "#         if line.startswith('assistant:'):\n",
    "#             return line.replace('assistant:', '').strip()\n",
    "#     return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "# def equal_path(contexts, source_path):\n",
    "#     adjusted_docs = []\n",
    "#     for doc in contexts:\n",
    "#         if doc.metadata['Source_path'] == source_path:\n",
    "#             adjusted_docs.append(doc)\n",
    "#     return adjusted_docs\n",
    "\n",
    "# def rerun(question,context,answer,llm,num_repeat):\n",
    "#     full_template = \"<|begin_of_text|>\"\n",
    "#     full_template += \"\"\"<|start_header_id|>system<|end_header_id|>당신은 이전 답변을 검증하는 챗봇입니다. 질문과 문맥, 이전 답변을 참고해서 지시사항을 따르세요. 지시사항을 따를 때 서론 없이 출력하세요.<|eot_id|>\"\"\"\n",
    "#     full_template += f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {question} \\n\\nContexts: {context} \\n\\nPrevious Answer: {answer} \\n\\n\"\"\"\n",
    "#     full_template += \"\"\"{input}<|eot_id|>\"\"\"\n",
    "#     full_template += \"\"\"<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "#     prompt = PromptTemplate(template=full_template)\n",
    "#     chain = (\n",
    "#     {\n",
    "#         \"input\": RunnablePassthrough(),\n",
    "#     }\n",
    "#     | prompt\n",
    "#     | llm\n",
    "#     | StrOutputParser()\n",
    "#     )\n",
    "#     return chain.invoke(\"핵심 단어들을 바탕으로, 한 문장으로 요약하세요. 만약 한 문장이라면 그대로 출력하세요.\")\n",
    "    \n",
    "# def run(faiss,dataset,llm,k=2,verbose=False):\n",
    "#     results = []\n",
    "#     source_path = dataset.iloc[0]['Source_path']\n",
    "#     docs = faiss.similarity_search(\n",
    "#         query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "#         filter={\"Source_path\": source_path},\n",
    "#         k = 99,\n",
    "#         fetch_k = 20000\n",
    "#         )\n",
    "#     buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "#     faiss_retriever = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n",
    "#     knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "#     knn_retriever.k = k\n",
    "#     bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "#     bm_retriever.k = k\n",
    "#     ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever, knn_retriever,bm_retriever], weight=[0.4, 0.3, 0.3])\n",
    "    \n",
    "#     for i, row in (dataset.iterrows()):\n",
    "#         if source_path != row['Source_path']:   \n",
    "#             source_path = row['Source_path']\n",
    "#             docs = faiss.similarity_search(\n",
    "#                 query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "#                 filter={\"Source_path\": source_path},\n",
    "#                 k = 99,\n",
    "#                 fetch_k = 20000\n",
    "#                 )\n",
    "#             buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "#             faiss_retriever = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n",
    "#             knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "#             knn_retriever.k = k\n",
    "#             bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "#             bm_retriever.k = k\n",
    "#             ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever, knn_retriever,bm_retriever], weight=[0.4, 0.3, 0.3])\n",
    "            \n",
    "#         full_template = \"<|begin_of_text|>\"\n",
    "#         full_template += \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "# 당신은 유용한 금융 정보 QnA 챗봇입니다.\n",
    "# 질문을 차근차근 생각하고, 답변 시 반드시 문맥 정보를 활용해야합니다. \n",
    "# 객관적이고 공식적인 문체를 사용하세요.\n",
    "# 서론 없이 핵심 내용을 한 문장으로 작성해주세요. \n",
    "# <|eot_id|>\n",
    "# \"\"\"\n",
    "#         question = row['Question']          \n",
    "#         # full_template += \"\"\" \"\"\"\n",
    "#         contexts = ensemble_retriever.invoke(normalize_string(question))\n",
    "#         # contexts = equal_path(contexts,row['Source_path'])\n",
    "#         contexts = format_docs(contexts)\n",
    "#         full_template += \"\"\"<|start_header_id|>user<|end_header_id|>Question: {input}\\n\\n\"\"\"\n",
    "#         full_template += f\"\"\"Contexts: {contexts}<|eot_id|>\"\"\"\n",
    "#         full_template += \"\"\"<|start_header_id|>assistant<|end_header_id>\"\"\"\n",
    "        \n",
    "#         prompt = PromptTemplate(template=full_template, input_variables=[\"input\"])\n",
    "#         qa_chain = (\n",
    "#         {\n",
    "#             \"input\": RunnablePassthrough(),\n",
    "#         }\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#         | StrOutputParser()\n",
    "#         )\n",
    "\n",
    "#         answer = qa_chain.invoke(input=question)\n",
    "#         answer = extract_answer(answer)\n",
    "#         lines = answer.split('\\n')\n",
    "#         if  len(lines) > 1:\n",
    "#             previous = answer\n",
    "#             try:\n",
    "#                 before = calculate_f1_score(row['Answer'],answer)[2]\n",
    "#             except:\n",
    "#                 before = None\n",
    "#             answer = rerun(question=question,\n",
    "#                            context=contexts,\n",
    "#                            answer=answer,\n",
    "#                            llm=llm,\n",
    "#                            num_repeat=1)\n",
    "#         answer = extract_answer(answer)\n",
    "#         results.append({\n",
    "#             \"Question\": question,\n",
    "#             \"Answer\": answer,\n",
    "#             \"Source\": row['Source']\n",
    "#         })\n",
    "#         if verbose:\n",
    "#             print(f\"{i}/{len(dataset)}\")\n",
    "#             print(\"Question: \", question, end=\" | \")\n",
    "#             print(\"Context Number |\",len(contexts))\n",
    "#             try:\n",
    "#                 print(calculate_f1_score(row['Answer'],answer)[2],end=\" | \")\n",
    "#             except:\n",
    "#                 pass\n",
    "#             print(\"Answer: \", results[-1]['Answer'])\n",
    "#             try:\n",
    "#                 print(\"Before: \",before,\" | \",previous)  \n",
    "                \n",
    "#                 previous = None\n",
    "#                 before = None\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#             try:\n",
    "#                 print(\"REAL Answer: \",row['Answer'])\n",
    "#             except:\n",
    "#                 pass\n",
    "            \n",
    "#             print()\n",
    "\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response):\n",
    "    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.replace('**', '')\n",
    "        if line.startswith('Answer:'):\n",
    "            return line.replace('Answer:', '').strip()\n",
    "        if line.startswith('assistant:'):\n",
    "            return line.replace('assistant:', '').strip()\n",
    "    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "def rerun(question,context,answer,llm,num_repeat):\n",
    "    full_template = \"<|begin_of_text|>\"\n",
    "    full_template += \"\"\"<|start_header_id|>system<|end_header_id|>당신은 이전 답변을 검증하는 챗봇입니다. 질문과 문맥, 이전 답변을 참고해서 지시사항을 따르세요. 지시사항을 따를 때 서론 없이 출력하세요.<|eot_id|>\"\"\"\n",
    "    full_template += f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {question} \\n\\nContexts: {context} \\n\\nPrevious Answer: {answer} \\n\\n\"\"\"\n",
    "    full_template += \"\"\"{input}<|eot_id|>\"\"\"\n",
    "    full_template += \"\"\"<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=full_template)\n",
    "    chain = (\n",
    "    {\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(\"핵심 단어들을 바탕으로, 한 문장으로 요약하세요. 만약 한 문장이라면 그대로 출력하세요.\")\n",
    "\n",
    "def run(faiss, dataset, llm, k=2, verbose=False):\n",
    "    results = []\n",
    "    source_path = dataset.iloc[0]['Source_path']\n",
    "    docs = faiss.similarity_search(\n",
    "        query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "        filter={\"Source_path\": source_path},\n",
    "        k=99,\n",
    "        fetch_k=20000\n",
    "    )\n",
    "    buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "    faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": k})\n",
    "    faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n",
    "    knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "    knn_retriever.k = k\n",
    "    bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "    bm_retriever.k = k\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr, faiss_retriever_sim, knn_retriever, bm_retriever], weight=[0.25, 0.25, 0.25, 0.25])\n",
    "\n",
    "    for i, row in dataset.iterrows():\n",
    "        # 조건이 바뀌면 새로운 리트리버 세팅\n",
    "        if source_path != row['Source_path']:\n",
    "            source_path = row['Source_path']\n",
    "            docs = faiss.similarity_search(\n",
    "                query=\"\",\n",
    "                filter={\"Source_path\": source_path},\n",
    "                k=99,\n",
    "                fetch_k=20000\n",
    "            )\n",
    "            buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "            faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": k})\n",
    "            faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n",
    "            knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "            knn_retriever.k = k\n",
    "            bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "            bm_retriever.k = k\n",
    "\n",
    "            ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr, faiss_retriever_sim, knn_retriever, bm_retriever], weight=[0.25, 0.25, 0.25, 0.25])\n",
    "        \n",
    "        question = row['Question']  \n",
    "        # 매 반복마다 contexts를 새로 할당\n",
    "        contexts = ensemble_retriever.invoke(normalize_string(question))\n",
    "\n",
    "        # full_template 초기화\n",
    "        full_template = f\"\"\"system\n",
    "        당신은 유용한 금융 정보 QnA 챗봇입니다.\n",
    "        질문을 차근차근 생각하고, 답변 시 반드시 문맥 정보를 활용해야합니다.\n",
    "        객관적이고 공식적인 문체를 사용해 간결하게 작성하세요.\n",
    "        서론 없이 핵심 내용을 한 문장으로 작성해주세요.\n",
    "\n",
    "        userQuestion: {question}\\n\\n\n",
    "        Contexts: {format_docs(contexts)}\n",
    "        assistant<|end_header_id>\n",
    "        \"\"\"\n",
    "\n",
    "        prompt = PromptTemplate(template=full_template, input_variables=[\"input\"])\n",
    "        qa_chain = (\n",
    "            {\n",
    "                \"input\": RunnablePassthrough(),\n",
    "            }\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        answer = qa_chain.invoke(input=question)\n",
    "        answer = extract_answer(answer)\n",
    "\n",
    "        lines = answer.split('\\n')\n",
    "        if len(lines) > 1:\n",
    "            try:\n",
    "                before = calculate_f1_score(row['Answer'], answer)[2]\n",
    "            except:\n",
    "                before = None\n",
    "            answer = rerun(question=question, context=contexts, answer=answer, llm=llm, num_repeat=1)\n",
    "        answer = extract_answer(answer)\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Source\": row['Source']\n",
    "        })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"{i}/{len(dataset)}\")\n",
    "            print(\"Question: \", question, end=\" | \")\n",
    "            print(\"Context Number |\", len(contexts))\n",
    "            try:\n",
    "                print(calculate_f1_score(row['Answer'], answer)[2], end=\" | \")\n",
    "            except:\n",
    "                pass\n",
    "            print(\"Answer: \", results[-1]['Answer'])\n",
    "\n",
    "            try:\n",
    "                print(\"Before: \", before)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try:\n",
    "                print(\"REAL Answer: \", row['Answer'])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            print()\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케이폴드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import copy\n",
    "\n",
    "# # weight = [0.3,0.3,0.4]\n",
    "# # train_faiss_db,knn_retriever ,train_bm_retrievier = make_db(train_df) \n",
    "\n",
    "# # train_k = 3\n",
    "# # train_bm_retrievier.k = train_k\n",
    "# # knn_retriever.k = train_k\n",
    "# # train_faiss_retriever = train_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':train_k} )\n",
    "# # train_ensemble_retriever = EnsembleRetriever(\n",
    "# #     retrievers=[train_bm_retrievier, knn_retriever,train_faiss_retriever], weights=weight , search_kwargs={'k':train_k}\n",
    "# # )\n",
    "\n",
    "\n",
    "# # fewshot_k = 3\n",
    "\n",
    "# k_folds = 4\n",
    "# fold_results = []\n",
    "# kf = KFold(n_splits=k_folds, shuffle=True, random_state=52)\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "#     fold_result = []\n",
    "#     train_set = train_df.iloc[train_index]\n",
    "#     val_set = train_df.iloc[val_index]\n",
    "\n",
    "#     faiss = make_db(val_set)\n",
    "\n",
    "#     pred = run(faiss,val_set, llm, verbose=True)\n",
    "#     result = pd.DataFrame()\n",
    "#     result['pred'] = [result['Answer'] for result in pred]\n",
    "#     val_set.index = range(len(val_set))\n",
    "#     result['gt'] = val_set['Answer']\n",
    "        \n",
    "#     result = calculate_average_f1_score(result['gt'], result['pred'])\n",
    "#     print(result)\n",
    "#     fold_results.append(result)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:21<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pook0\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\pook0\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\llama\\modeling_llama.py:653: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/98\n",
      "Question:  2022년 혁신창업사업화자금(융자)의 예산은 얼마인가요? | Context Number | 3\n",
      "Answer:  답변: 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다. | 2022년 혁신창업사업화자금(융자)의 예산은 2,300,000 백만원입니다.\n",
      "\n",
      "1/98\n",
      "Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 무엇인가요? | Context Number | 5\n",
      "Answer:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 기술력과 사업성이 우수하고 미래 성장가능성이 높은 중소벤처기업의 창업을 활성화하고 고용창출을 도모하고, 중소기업이 보유한 우수 기술의 상실을 방지하고 개발기술의 제품화·사업화를 촉진하여 기술기반 중소기업을 육성하는 것입니다.  |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>        . |endoftext|<|end_header_id>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from save_module import save\n",
    "\n",
    "faiss= make_db(test_df)\n",
    "\n",
    "results = run(faiss, test_df, llm, verbose=True)\n",
    "save(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
