{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Vector stores\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader ,UnstructuredPDFLoader\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever, OktBM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import pymupdf4llm\n",
    "import time\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from konlpy.tag import Kkma\n",
    "# etc\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import logging\n",
    "from PyPDF2 import PdfReader\n",
    "import json\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "# intfloat/multilingual-e5-small\n",
    "# jhgan/ko-sroberta-multitask\n",
    "def get_embedding():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='jhgan/ko-sroberta-multitask',\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'normalize_embeddings': True})\n",
    "    return embeddings\n",
    "\n",
    "def normalize_string(s):\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFC', s)\n",
    "        return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"�\", \" \").replace(\"\u0003\", \" \") # 잘못된 인코딩 문자 제거\n",
    "    text = ' '.join(text.split())  # 여러 공백을 하나로 줄임\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs(pdf_path):\n",
    "    documents = []\n",
    "    failed_pages = []  # 실패한 페이지를 추적하기 위한 리스트\n",
    "\n",
    "    try:\n",
    "        # 페이지 단위로 pymupdf4llm 사용\n",
    "        md_read = pymupdf4llm.to_markdown(pdf_path, page_chunks=True)\n",
    "        total_pages = len(md_read)\n",
    "\n",
    "        for page_data in md_read:\n",
    "            try:\n",
    "                page_number = page_data.get('metadata', {}).get('page', None)\n",
    "                if page_number is None:\n",
    "                    raise ValueError(\"Page number missing in metadata\")\n",
    "\n",
    "                text = clean_text(normalize_string(page_data.get('text', '')))\n",
    "                if not text:\n",
    "                    raise ValueError(\"Empty text\")  # 텍스트가 비어 있으면 예외 발생\n",
    "\n",
    "                metadata = {\n",
    "                    \"file_path\": pdf_path,\n",
    "                    \"page_number\": page_number,\n",
    "                    \"total_pages\": total_pages,\n",
    "                    \"tables\": page_data.get('tables', [])\n",
    "                }\n",
    "\n",
    "                document = Document(page_content=text, metadata=metadata)\n",
    "                documents.append(document)\n",
    "\n",
    "            except Exception as e:\n",
    "                logging.warning(f\"Failed to process page {page_number} with pymupdf4llm: {e}\")\n",
    "                failed_pages.append(page_number)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to read PDF with pymupdf4llm: {e}\")\n",
    "        # pymupdf4llm 전체 실패 시 모든 페이지를 PyMuPDF로 처리하도록 설정\n",
    "        failed_pages = list(range(1, len(fitz.open(pdf_path)) + 1))\n",
    "\n",
    "    # 실패한 페이지에 대해 PyMuPDF로 재처리\n",
    "    if failed_pages:\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            for page_num in failed_pages:\n",
    "                page = doc.load_page(page_num - 1)  # 페이지 번호는 0부터 시작하기 때문에 -1\n",
    "                text = page.get_text(\"text\")\n",
    "                if text:\n",
    "                    text = clean_text(normalize_string(text))\n",
    "                    metadata = {\n",
    "                        \"file_path\": pdf_path,\n",
    "                        \"page_number\": page_num,\n",
    "                        \"total_pages\": doc.page_count\n",
    "                    }\n",
    "                    document = Document(page_content=text, metadata=metadata)\n",
    "                    documents.append(document)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to read PDF with PyMuPDF: {e}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=60,\n",
    "        separators=[\"\\n\\n\", \".\", \"?\", \"!\", \"\\n\", \"\\t\"]\n",
    "    )\n",
    "    chunks = []\n",
    "\n",
    "    for doc in docs:\n",
    "        text = doc.page_content\n",
    "        metadata = doc.metadata\n",
    "\n",
    "        # 표가 있는 경우 페이지 전체를 하나의 청크로 사용\n",
    "        if metadata.get('tables'):\n",
    "            chunks.append(Document(page_content=text, metadata=metadata))\n",
    "        else:\n",
    "            text_chunks = text_splitter.split_text(text)\n",
    "            chunks.extend([Document(page_content=chunk, metadata=metadata) for chunk in text_chunks])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def make_db(df):\n",
    "    documents = []\n",
    "    pdf_files = df['Source_path'].unique()\n",
    "\n",
    "    # tqdm으로 파일 처리 진행 상황 표시\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(get_docs, pdf_files), total=len(pdf_files), desc=\"Processing PDFs\"))\n",
    "\n",
    "    for result in results:\n",
    "        documents.extend(result)\n",
    "\n",
    "    # 정규화\n",
    "    for doc in documents:\n",
    "        doc.page_content = normalize_string(doc.page_content)\n",
    "\n",
    "    # 표가 망가지지 않도록 청크 분할 처리\n",
    "    chunks = chunk_documents(documents)\n",
    "    print(f\"Total number of chunks: {len(chunks)}\")\n",
    "\n",
    "    faiss = FAISS.from_documents(chunks, embedding=get_embedding())\n",
    "    bm = OktBM25Retriever.from_documents(chunks)\n",
    "\n",
    "    return faiss, bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_db(df):\n",
    "    df = df.drop('SAMPLE_ID', axis=1)\n",
    "    df = df.drop('Source_path', axis=1)\n",
    "    df = df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", len(df))\n",
    "    to_vectorize = [\"\\n\\n\".join(normalize_string(value) for value in example.values()) for example in df]\n",
    "    faiss = FAISS.from_texts(to_vectorize, embedding=get_embedding())\n",
    "    # bm = BM25Retriever.from_texts(to_vectorize)\n",
    "    # knn = KNNRetriever.from_texts(to_vectorize, embeddings=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        #context += f\"Document {i+1}\\n\"\n",
    "        doc.page_content = doc.page_content.replace(\"{\", \"(\")\n",
    "        doc.page_content = doc.page_content.replace(\"}\", \")\")\n",
    "        \n",
    "        context += doc.page_content\n",
    "        context += '\\n\\n'\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_pipeline(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    # 토크나이저 로드 및 설정\n",
    "        # 양자화 설정 적용\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,low_cpu_mem_usage=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id = terminators,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return llm\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n",
    "\n",
    "    #공백 제거\n",
    "    true_sentence = ''.join(true_sentence.split())\n",
    "    predicted_sentence = ''.join(predicted_sentence.split())\n",
    "    \n",
    "    true_counter = Counter(true_sentence)\n",
    "    predicted_counter = Counter(predicted_sentence)\n",
    "\n",
    "    #문자가 등장한 개수도 고려\n",
    "    if sum_mode:\n",
    "        true_positive = sum((true_counter & predicted_counter).values())\n",
    "        predicted_positive = sum(predicted_counter.values())\n",
    "        actual_positive = sum(true_counter.values())\n",
    "\n",
    "    #문자 자체가 있는 것에 focus를 맞춤\n",
    "    else:\n",
    "        true_positive = len((true_counter & predicted_counter).values())\n",
    "        predicted_positive = len(predicted_counter.values())\n",
    "        actual_positive = len(true_counter.values())\n",
    "\n",
    "    #f1 score 계산\n",
    "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_average_f1_score(true_sentences, predicted_sentences):\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n",
    "        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1_score += f1_score\n",
    "    \n",
    "    avg_precision = total_precision / len(true_sentences)\n",
    "    avg_recall = total_recall / len(true_sentences)\n",
    "    avg_f1_score = total_f1_score / len(true_sentences)\n",
    "    \n",
    "    return {\n",
    "        'average_precision': avg_precision,\n",
    "        'average_recall': avg_recall,\n",
    "        'average_f1_score': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response):\n",
    "    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.replace('**', '')\n",
    "        if line.startswith('Answer:'):\n",
    "            return line.replace('Answer:', '').strip()\n",
    "        if line.startswith('assistant:'):\n",
    "            return line.replace('assistant:', '').strip()\n",
    "    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "\n",
    "\n",
    "def run (test,dataset,llm,verbose=False):\n",
    "    results = []\n",
    "    for i, row in (dataset.iterrows()):\n",
    "# \n",
    "        full_template = \"<|begin_of_text|>\"\n",
    "        full_template += \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "당신은 유용한 금융 정보 QnA 챗봇입니다.\n",
    "문맥 정보들을 바탕으로 질문에 답변하세요.\n",
    "객관적이고 공식적인 문체를 사용하세요.\n",
    "어떤 서론이나 배경 설명 없이 바로 핵심 내용을 전달해주세요.\n",
    "<|eot_id|>\n",
    "\"\"\"\n",
    "        question = row['Question']          \n",
    "        full_template += \"\"\" \"\"\"\n",
    "        contexts = test.invoke(normalize_string(question))\n",
    "        contexts = format_docs(contexts)\n",
    "        full_template += \"\"\"<|start_header_id|>user<|end_header_id|>Question: {input} \\n1 문장으로 작성하세요.\\n\\n\"\"\"\n",
    "        full_template += f\"\"\"Context: {contexts}<|eot_id|>\\n\\n\"\"\"\n",
    "        full_template += \"\"\"<|start_header_id|>assistant<|end_header_id>\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=full_template, input_variables=[\"input\"])\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        answer = qa_chain.invoke(input=question)\n",
    "        #answer = extract_answer(answer)\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Source\": row['Source']\n",
    "        })\n",
    "        if verbose:\n",
    "            print(f\"{i}/{len(dataset)}\")\n",
    "            print(\"Question: \", question, end=\" | \")\n",
    "            print(\"Context Number |\",len(contexts))\n",
    "            print(\"Answer: \", results[-1]['Answer'])\n",
    "            try:\n",
    "                print(calculate_f1_score(row['Answer'],answer)[2],\" | \",\"REAL Answer: \",row['Answer'])\n",
    "            except:\n",
    "                pass\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "Processing PDFs:   0%|          | 0/16 [00:00<?, ?it/s]ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "Processing PDFs:   6%|▋         | 1/16 [00:04<01:03,  4.21s/it]ERROR:root:Failed to read PDF with pymupdf4llm: not a textpage of this page\n",
      "Processing PDFs: 100%|██████████| 16/16 [00:43<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 1562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:660: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/124\n",
      "Question:  2024년 중앙정부의 예산 지출은 어떻게 구성되어 있나요? | Context Number | 1459\n",
      "Answer:  1. 2024년 중앙정부의 예산 지출은 656.6조원으로 구성되어 있으며, 일반회계가 356.5조원으로 중앙정부 총지출의 54.3%를 차지합니다.\n",
      "0.6890756302521008  |  REAL Answer:  2024년 중앙정부의 예산 지출은 일반회계 356.5조원, 21개 특별회계 81.7조원으로 구성되어 있습니다.\n",
      "4/124\n",
      "Question:  2024년 총수입은 얼마이며, 예산수입과 기금수입은 각각 몇 조원인가요? | Context Number | 1622\n",
      "Answer:  2024년도 총수입은 612.2조원이며, 예산수입은 395.5조원, 기금수입은 216.7조원입니다.\n",
      "0.9896907216494846  |  REAL Answer:  2024년 총수입은 612.2조원이며, 예산수입은 395.5조원, 기금수입은 216.7조원입니다.\n",
      "6/124\n",
      "Question:  2024년의 기금수입은 어떻게 구성되어 있나요? | Context Number | 1555\n",
      "Answer:  1. 총수입은 일반회계수입 + 특별회계수입 + 기금수입 - 내부거래 - 보전거래입니다.\n",
      "0.2947368421052632  |  REAL Answer:  2024년도 기금수입은 사회보장성기금 92.3조원, 경상이전수입 39.6조원, 기타 84.7조원으로 구성되어 있습니다.\n",
      "11/124\n",
      "Question:  2024년 총지출 기준 예산의 일반회계와 특별회계의 비중이 각각 얼마인가? | Context Number | 2362\n",
      "Answer:  2024년 총지출 기준 예산의 일반회계와 특별회계의 비중은 각각 71.6%와 28.4입니다.\n",
      "0.8  |  REAL Answer:  2024년 총지출 중 일반회계와 특별회계의 비중은 각각 54.3%와 12.4%이다.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "\n",
    "weight = [0.5,0.5]\n",
    "train_faiss_db, train_bm_retrievier = make_db(train_df) \n",
    "\n",
    "train_k = 2\n",
    "train_bm_retrievier.k = train_k\n",
    "#knn_retriever.k = train_k\n",
    "train_faiss_retriever = train_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':train_k} )\n",
    "train_ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[train_bm_retrievier, train_faiss_retriever], weights=weight\n",
    ")\n",
    "\n",
    "\n",
    "fewshot_k = 3\n",
    "\n",
    "k_folds = 4\n",
    "fold_results = []\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=52)\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "    fold_result = []\n",
    "    train_set = train_df.iloc[train_index]\n",
    "    val_set = train_df.iloc[val_index]\n",
    "\n",
    "    pred = run(train_ensemble_retriever, val_set, llm, verbose=True)\n",
    "    result = pd.DataFrame()\n",
    "    result['pred'] = [result['Answer'] for result in pred]\n",
    "    val_set.index = range(len(val_set))\n",
    "    result['gt'] = val_set['Answer']\n",
    "        \n",
    "    result = calculate_average_f1_score(result['gt'], result['pred'])\n",
    "    print(result)\n",
    "    fold_results.append(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from save_module import save\n",
    "\n",
    "\n",
    "# weight = [0.5,0.5]\n",
    "# test_faiss_db, test_bm_retrievier = make_db(test_df)\n",
    "\n",
    "# test_k = 2\n",
    "# test_bm_retrievier.k = test_k\n",
    "# #test_knn_retriever.k = test_k\n",
    "# test_faiss_retriever = test_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':test_k} )\n",
    "# test_ensemble_retriever = EnsembleRetriever(\n",
    "#     retrievers=[test_bm_retrievier, test_faiss_retriever], weights=weight\n",
    "# )\n",
    "\n",
    "\n",
    "# results = run(test_ensemble_retriever, test_df, llm, verbose=True)\n",
    "# save(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
