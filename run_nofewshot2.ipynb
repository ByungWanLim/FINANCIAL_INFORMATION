{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Vector stores\n",
    "import fitz  # PyMuPDF \n",
    "import pdfplumber\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter, MarkdownHeaderTextSplitter, MarkdownTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader ,UnstructuredPDFLoader\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever \n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever, OktBM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import pymupdf4llm\n",
    "import time\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from konlpy.tag import Kkma\n",
    "# etc\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import logging\n",
    "from PyPDF2 import PdfReader\n",
    "from seed_module import seed_everything\n",
    "from langchain_community.document_transformers import LongContextReorder\n",
    "import json\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n",
    "print(device)\n",
    "seed_everything(52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intfloat/multilingual-e5-small\n",
    "# jhgan/ko-sroberta-multitask\n",
    "\n",
    "def get_embedding():\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='jhgan/ko-sroberta-multitask',\n",
    "        model_kwargs={'device': device},\n",
    "        \n",
    "        encode_kwargs={'normalize_embeddings': True})\n",
    "    return embeddings\n",
    "def normalize_string(s):\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFC', s)\n",
    "        return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"�\", \" \").replace(\"\u0003\", \" \")  # 잘못된 인코딩 문자 제거\n",
    "    return text\n",
    "\n",
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Header 정보를 순서대로 추가\n",
    "        for header_level in range(1, 6):\n",
    "            header_key = f'Header{header_level}'\n",
    "            if header_key in doc.metadata:\n",
    "                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n",
    "        # 문서 내용 추가\n",
    "        context += doc.page_content\n",
    "        context += '\\n---\\n'\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path): \n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=True)\n",
    "    splits = md_header_splitter.split_text(md_text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800, chunk_overlap=40\n",
    "    )\n",
    "\n",
    "    splits = text_splitter.split_documents(splits)\n",
    "\n",
    "    for i in splits:\n",
    "        metadata = {'Source_path': pdf_path}\n",
    "        i.metadata = {**i.metadata, **metadata}\n",
    "    return splits\n",
    "\n",
    "\n",
    "def make_db(df):\n",
    "    documents = []\n",
    "    \n",
    "    pdf_files = df['Source_path'].unique()\n",
    "    for pdf_file in tqdm(pdf_files):\n",
    "        # 문서 로드\n",
    "        documents.extend(process_pdf(pdf_file))\n",
    "        \n",
    "    print(f\"Total number of documents: {len(documents)}\")\n",
    "\n",
    "    faiss = FAISS.from_documents(documents, embedding=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_db(df):\n",
    "    df = df.drop('SAMPLE_ID', axis=1)\n",
    "    df = df.drop('Source_path', axis=1)\n",
    "    df = df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", len(df))\n",
    "    to_vectorize = [\"\\n\\n\".join(normalize_string(value) for value in example.values()) for example in df]\n",
    "    faiss = FAISS.from_texts(to_vectorize, embedding=get_embedding())\n",
    "    # bm = BM25Retriever.from_texts(to_vectorize)\n",
    "    # knn = KNNRetriever.from_texts(to_vectorize, embeddings=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        # Header 정보를 순서대로 추가\n",
    "        for header_level in range(1, 6):\n",
    "            header_key = f'Header {header_level}'\n",
    "            if header_key in doc.metadata:\n",
    "                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n",
    "        \n",
    "        # 문서 내용 추가\n",
    "        context += doc.page_content\n",
    "        context += '\\n\\n'\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.77s/it]\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_pipeline(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    # 토크나이저 로드 및 설정\n",
    "        # 양자화 설정 적용\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,low_cpu_mem_usage=True)\n",
    "    \n",
    "    # # 일부 중요한 레이어는 FP16으로 유지\n",
    "    # for name, module in model.named_modules():\n",
    "    #     if \"attention\" in name or \"ffn\" in name:  # 중요한 레이어 식별 (예: attention 및 ffn)\n",
    "    #         module.to(torch.float16)  # 이 부분은 16비트로 유지\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id = terminators,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return llm\n",
    "# ghost-x/ghost-8b-beta-1608\n",
    "# OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n",
    "\n",
    "    #공백 제거\n",
    "    true_sentence = ''.join(true_sentence.split())\n",
    "    predicted_sentence = ''.join(predicted_sentence.split())\n",
    "    \n",
    "    true_counter = Counter(true_sentence)\n",
    "    predicted_counter = Counter(predicted_sentence)\n",
    "\n",
    "    #문자가 등장한 개수도 고려\n",
    "    if sum_mode:\n",
    "        true_positive = sum((true_counter & predicted_counter).values())\n",
    "        predicted_positive = sum(predicted_counter.values())\n",
    "        actual_positive = sum(true_counter.values())\n",
    "\n",
    "    #문자 자체가 있는 것에 focus를 맞춤\n",
    "    else:\n",
    "        true_positive = len((true_counter & predicted_counter).values())\n",
    "        predicted_positive = len(predicted_counter.values())\n",
    "        actual_positive = len(true_counter.values())\n",
    "\n",
    "    #f1 score 계산\n",
    "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_average_f1_score(true_sentences, predicted_sentences):\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n",
    "        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1_score += f1_score\n",
    "    \n",
    "    avg_precision = total_precision / len(true_sentences)\n",
    "    avg_recall = total_recall / len(true_sentences)\n",
    "    avg_f1_score = total_f1_score / len(true_sentences)\n",
    "    \n",
    "    return {\n",
    "        'average_precision': avg_precision,\n",
    "        'average_recall': avg_recall,\n",
    "        'average_f1_score': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_answer(response):\n",
    "    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.replace('**', '')\n",
    "        if line.startswith('Answer:'):\n",
    "            return line.replace('Answer:', '').strip()\n",
    "        if line.startswith('assistant:'):\n",
    "            return line.replace('assistant:', '').strip()\n",
    "    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "def rerun(question,context,answer,llm,num_repeat):\n",
    "    full_template = \"<|begin_of_text|>\"\n",
    "    full_template += \"\"\"<|start_header_id|>system<|end_header_id|>당신은 이전 답변을 검증하는 챗봇입니다. 질문과 문맥, 이전 답변을 참고해서 지시사항을 따르세요. 지시사항을 따를 때 서론 없이 출력하세요.<|eot_id|>\"\"\"\n",
    "    full_template += \"\"\"<|start_header_id|>user<|end_header_id|>Question: {question} \\n\\nContexts: {context} \\n\\nPrevious Answer: {answer} \\n\\n\"\"\"\n",
    "    full_template += \"\"\"{input}<|eot_id|>\"\"\"\n",
    "    full_template += \"\"\"<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=full_template)\n",
    "    chain = (\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(),\n",
    "        \"context\": RunnablePassthrough(),\n",
    "        \"answer\": RunnablePassthrough(),\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke({\"question\":question,\"context\":context,\"answer\":answer,\"input\":\"핵심 단어들을 바탕으로, 한 문장으로 간결한 요약문을 평문으로 작성하세요. 만약 한 문장이고 질문에 충분한 설명이 된다면 그대로 출력하세요.\"})\n",
    "\n",
    "def run(test_faiss,fewshot_retriever,train_faiss,dataset,llm,k=3,verbose=False):\n",
    "    results = []\n",
    "    \n",
    "    \n",
    "    source_path = dataset.iloc[0]['Source_path']\n",
    "    docs = test_faiss.similarity_search(\n",
    "    query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "    filter={\"Source_path\": source_path},\n",
    "    k = 99,\n",
    "    fetch_k = 20000\n",
    "    )\n",
    "    buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "    faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n",
    "    # faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n",
    "    # knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "    # knn_retriever.k = k\n",
    "    bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "    bm_retriever.k = k\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr,bm_retriever], weight=[0.5,0.5])\n",
    "\n",
    "    \n",
    "    for i, row in (dataset.iterrows()):\n",
    "        if source_path != row['Source_path']:   \n",
    "            source_path = row['Source_path']\n",
    "            docs = test_faiss.similarity_search(\n",
    "                query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "                filter={\"Source_path\": source_path},\n",
    "                k = 99,\n",
    "                fetch_k = 20000\n",
    "            )\n",
    "            buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "            faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n",
    "            # faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n",
    "            # knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "            # knn_retriever.k = k\n",
    "            bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "            bm_retriever.k = k\n",
    "\n",
    "            ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr,bm_retriever], weight=[0.5,0.5])\n",
    "\n",
    "            \n",
    "        full_template = \"<|begin_of_text|>\"\n",
    "        full_template += \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "당신은 유용한 금융 정보 Question-Answer Model 입니다.\n",
    "user의 문맥 정보에서 핵심 내용을 찾아 답변을 작성하세요. \n",
    "답을 지어내지 마세요. 만약 문맥 정보에 정답이 없다면 모른다고 답하세요.\n",
    "객관적이고 공식적인 문체를 사용하세요. 단위는 꼭 표기하세요.\n",
    "서론 없이 본론만 간결하게 평문으로 작성하세요.<|eot_id|>\n",
    "\"\"\"\n",
    "        question = row['Question']          \n",
    "        # full_template += \"\"\" \"\"\"\n",
    "        contexts = ensemble_retriever.invoke(normalize_string(question))\n",
    "        \n",
    "        # reordering = LongContextReorder()\n",
    "        # contexts = reordering.transform_documents(contexts)\n",
    "        \n",
    "        \n",
    "        contexts = format_docs(contexts)\n",
    "        \n",
    "        \n",
    "        full_template += \"<|start_header_id|>user<|end_header_id|>\"\n",
    "        full_template += \"Question: {input}\\n\\n<|eot_id|>\"\n",
    "        full_template += \"Contexts: {contexts}\"\n",
    "        full_template += \"\"\"<|eot_id|>\"\"\"\n",
    "        full_template += \"\"\"<|start_header_id|>assistant<|end_header_id>\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=full_template, input_variables=[\"contexts\",\"input\"])\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"contexts\": RunnablePassthrough(),\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        answer = qa_chain.invoke({'input':question,'contexts':contexts})\n",
    "        answer = extract_answer(answer)\n",
    "        lines = answer.split('\\n')\n",
    "        if  len(lines) > 1 or '|' in answer or ':' in answer or \"다음과 같습니다.\" in answer:\n",
    "            previous = answer\n",
    "            try:\n",
    "                before = calculate_f1_score(row['Answer'],answer)[2]\n",
    "            except:\n",
    "                before = None\n",
    "            answer = rerun(question=question,\n",
    "                           context=contexts,\n",
    "                           answer=answer,\n",
    "                           llm=llm,\n",
    "                           num_repeat=1)\n",
    "        answer = extract_answer(answer)\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Source\": row['Source']\n",
    "        })\n",
    "        if verbose:\n",
    "            print(f\"{i}/{len(dataset)}\")\n",
    "            print(\"Question: \", question, end=\" | \")\n",
    "            print(\"Context Number |\",len(contexts))\n",
    "            try:\n",
    "                print(calculate_f1_score(row['Answer'],answer)[2],end=\" | \")\n",
    "            except:\n",
    "                pass\n",
    "            print(\"Answer: \", results[-1]['Answer'])\n",
    "            try:\n",
    "                print(\"Before: \",before,\" | \",previous)  \n",
    "                \n",
    "                previous = None\n",
    "                before = None\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                print(\"REAL Answer: \",row['Answer'])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print()\n",
    "        torch.cuda.empty_cache()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케이폴드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import copy\n",
    "# k_folds = 4\n",
    "# fold_results = []\n",
    "# kf = KFold(n_splits=k_folds, shuffle=True, random_state=52)\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "#     fold_result = []\n",
    "#     train_set = train_df.iloc[train_index]\n",
    "#     val_set = train_df.iloc[val_index]\n",
    "\n",
    "#     faiss = make_db(val_set)\n",
    "\n",
    "#     pred = run(faiss,val_set, llm, verbose=True)\n",
    "#     result = pd.DataFrame()\n",
    "#     result['pred'] = [result['Answer'] for result in pred]\n",
    "#     val_set.index = range(len(val_set))\n",
    "#     result['gt'] = val_set['Answer']\n",
    "        \n",
    "#     result = calculate_average_f1_score(result['gt'], result['pred'])\n",
    "#     print(result)\n",
    "#     fold_results.append(result)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_db = fewshot_db(train_df)\n",
    "fewshot_retriever = fewshot_db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:11<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:653: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/98\n",
      "Question:  2022년 혁신창업사업화자금(융자)의 예산은 얼마인가요? | Context Number | 705\n",
      "Answer:  2022년 혁신창업사업화자금(융자의 예산은 2,300,000 백만원입니다.\n",
      "\n",
      "1/98\n",
      "Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 무엇인가요? | Context Number | 1600\n",
      "Answer:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 창업 활성화 및 고용 창출을 위하여 지원을 시작한 것입니다.\n",
      "\n",
      "2/98\n",
      "Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업근거는 어떤 법률에 근거하고 있나요? | Context Number | 1582\n",
      "Answer:  중소벤처기업부의 혁신창업사업화자금(융자) 사업근거는 중소기업진흥에 관한 법률 제66조, 제67조, 제74조, 중소기업창업지원법 제35조에 근거하고 있어요.\n",
      "\n",
      "3/98\n",
      "Question:  2010년에 신규 지원된 혁신창업사업화자금은 무엇인가요? | Context Number | 1682\n",
      "Answer:  2010년에 신규 지원된 혁신창업사업화자금은 재창업자금(실패 경영인에 대한 재기지원)이란 것을 의미합니다.\n",
      "\n",
      "4/98\n",
      "Question:  혁신창업사업화자금 중 2020년에 신규 지원된 자금은 무엇인가요? | Context Number | 1709\n",
      "Answer:  혁신창업사업화자금 중 2020년에 신규 지원된 자금은 없을 것 같습니다.\n",
      "\n",
      "5/98\n",
      "Question:  재창업자금이 재도약지원자금으로 이관된 연도는 언제인가요? | Context Number | 1685\n",
      "Answer:  재창업자금이 재도약지원자금으로 이관된 연도는 2015년입니다.\n",
      "\n",
      "6/98\n",
      "Question:  창업기반지원과 신청 대상이 중복인 자금이 어떤 것이며, 이 자금이 폐지된 연도는 언제인가요? | Context Number | 1813\n",
      "Answer:  \n",
      "\n",
      "7/98\n",
      "Question:  혁신창업사업화자금(융자) 사업을 시행하는 주체는 누구인가요? | Context Number | 1378\n",
      "Answer:  혁신창업사업화자금(융자) 사업을 시행하는 주체는 중소벤처기업진흥공단입니다.\n",
      "\n",
      "8/98\n",
      "Question:  혁신창업사업화자금(융자) 사업 집행절차는 어떻게 되나요? | Context Number | 1340\n",
      "Answer:  혁신창업사업화자금(융자) 사업 집행절차는 사전상담 및 신청, 사업계획수립/공고, 융자 실행 지원결정통보, 융자 실행 순입니다.\n",
      "Before:  None  |  혁신창업사업화자금(융자) 사업 집행절차는 다음과 같습니다.\n",
      "\n",
      "1.  사전상담 및 신청: 중기부, 중진공, 은행, 중소기업이 사전상담 및 신청을 함.\n",
      "2.  사업계획수립/공고: 중기부, 중진공이 사업계획을 수립하고 공고합니다.\n",
      "3.  융자 실행 지원결정통보: 중진공, 은행이 융자 실행을 지원결정통보합니다.\n",
      "4.  융자 실행: 중진공, 은행이 융자를 중소기업에게 제공합니다.\n",
      "\n",
      "이와 같은 절차를 통해 혁신창업사업화자금(융자)이 집행됩니다.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/98\n",
      "Question:  부모급여 지원 사업의 목적은 무엇인가요? | Context Number | 1668\n",
      "Answer:  1. 부모급여 지원 사업의 목적은 출산 및 양육으로 손실되는 소득을 보전하고 주 양육자의 직접 돌봄이 중요한 아동 발달의 특성에 따라 영아기 돌봄을 두텁게 지원하기 위하여 부모급여 지급입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "10/98\n",
      "Question:  부모급여(영아수당)의 2024년 확정된 예산은 몇백만원인가요? | Context Number | 2245\n",
      "Answer:  2024 년 부모급여(영아수당)의 예산은 2,888,694 백만원입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "11/98\n",
      "Question:  부모급여 지원 사업은 어떤 법령상 근거를 갖고 추진되고 있나요? | Context Number | 1617\n",
      "Answer:  부모급여 지원 사업의 법령상 근거는 아동수당법 제4조 제5항입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "12/98\n",
      "Question:  영아수당 도입에 대한 추진경위는 어떻게 되나요? | Context Number | 2245\n",
      "Answer:  영아수당 도입에 대한 추진경위는 예비타당성조사 통과(’21.8.), 근거법 마련(’21.12.), 영아수당 지원사업 시행(’22.1~), ‘부모급여 도입’ 대통령 공약사항 및 국정과제[*]에 포함(’22.5.) 등입니다.\n",
      "Before:  None  |  영아수당 도입에 대한 추진경위는 다음과 같습니다.\n",
      "\n",
      "13/98\n",
      "Question:  부모급여 지원사업은 언제부터 시행되었나요? | Context Number | 1666\n",
      "Answer:  부모급여 지원사업은 2022년 1월부터 시행되었습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "14/98\n",
      "Question:  보건복지부의 부모급여(영아수당) 지원 사업시행방법은 무엇이며, 사업 수혜자는 누구인가? | Context Number | 1617\n",
      "Answer:  1. 보건복지부의 부모급여(영아수당) 지원 사업시행방법은 지자체 보조이며, 사업 수혜자는 만 0~1세 아동입니다.\n",
      "Before:  None  |  1. 보건복지부의 부모급여(영아수당) 지원 사업시행방법은 gì이며, 사업 수혜자는 누구인가?\n",
      "① 사업 추진체계\n",
      "② 사업시행방법 : 지자체 보조\n",
      "③ 사업시행주체 : 보건복지부, 시‧도 및 시‧군‧구\n",
      "④ 사업 수혜자 : 만 0~1세 아동\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/98\n",
      "Question:  노인장기요양보험 사업 운영에 대한 목적은 무엇인가요? | Context Number | 2937\n",
      "Answer:  노인장기요양보험 사업 운영에 대한 목적은 노후의 건강증진 및 생활 안정을 도모하고 가족의 부담을 완화하여 국민 삶의 질을 향상하기 위하여 노인장기요양보험에 국고지원을 함으로써 효율적인 정책추진을 목적으로 합니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "16/98\n",
      "Question:  노인장기요양보험 운영지원에 대한 사업 내용을 설명해줘. | Context Number | 2343\n",
      "Answer:  노인장기요양보험 운영지원에 대한 사업 내용은 고령이나 노인성 질병으로 일상생활을 혼자서 수행하기 어려운 노인 등에게 신체 또는 가사 활동 등을 제공하는 노인장기요양보험에 대한 국고지원을 통해 효율적인 정책 추진으로 노후의 건강증진 및 생활 안정을 도모하고 가족의 부담을 완화하여 국민 삶의 질을 향상하는 것입니다.\n",
      "Before:  None  |  1. 노인장기요양보험 운영지원에 대한 사업 내용은 다음과 같습니다. \n",
      "\n",
      "노인장기요양보험 사업운영은 고령이나 노인성 질병으로 일상생활을 혼자서 수행하기 어려운 노인 등에게 신체 또는 가사 활동 등을 제공하는 노인장기요양보험에 대한 국고지원을 통해 효율적인 정책 추진으로 노후의 건강증진 및 생활 안정을 도모하고 가족의 부담을 완화하여 국민 삶의 질을 향상합니다.\n",
      "\n",
      "사업운영은 「노인장기요양보험법」 제58조에 따라 국가가 국민건강보험 공단에 지원하는 법정지원금(장기요양보험료 예상수입액의 20% 상당)을 포함합니다.\n",
      "\n",
      "또한, 공무원·사립학교 교원의 장기요양 보험료 국가부담분 및 차상위계층의 장기요양 보험료 지원, 기타 의료급여수급권자의 장기요양 급여 이용에 따른 급여 비용 및 관리운영비 국고지원(서울 50%, 기타지역 80%), 이재민, 의사상자, 국가유공자, 입양아동, 국가무형문화재보유자, 북한이탈주민 등에 대한 노인장기요양보험 사업추진에 필요한 경비, 장기요양기관 재무회계프로그램 구축·운영에 필요한 운영비 및 인건비를 포함합니다.\n",
      "\n",
      "17/98\n",
      "Question:  국고지원을 받는 기타 의료급여수급권자는 누구인가요? | Context Number | 2109\n",
      "Answer:  이 재원은 노인장기요양보험 운영지원, 공무원·사립학교교원 등 장기요양보험료 국가부담금, 기타의료급여 수급권자 급여비용 국가부담금, 장기요양기관 재무회계 프로그램 운영에 대한 국고지원을 받는 기타 의료급여수급권자는 없습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "18/98\n",
      "Question:  장기요양보험가입자 및 피부양자의 자격취득과 관련하여 어떤 법률을 준용해야 하는가? | Context Number | 1898\n",
      "Answer:  노인장기요양보험법 제11조에 따라 장기요양보험가입자·피부양자의 자격취득·상실, 장기요양보험료 등의 납부·징수 및 결손처분 등에 관하여 「국민건강보험법」 제5조, 제 6조, 제8조부터 제11조까지, 제69조제1항부터 제3항까지, 제76조부터 제86조까지 및 제110조를 준용한다.\n",
      "Before:  None  |  None\n",
      "\n",
      "19/98\n",
      "Question:  노인장기요양보험법이 언제 제정되고 공포되었나? | Context Number | 2362\n",
      "Answer:  노인장기요양보험법은 2007년 4월에 제정되었으며, 같은 해 7월에 공포되었다.\n",
      "Before:  None  |  None\n",
      "\n",
      "20/98\n",
      "Question:  장기요양인정점수 완화가 언제 이루어졌으며, 어떤 변화가 있었나? | Context Number | 3055\n",
      "Answer:  장기요양인정점수 완화가 2012년 7월에 이루어졌으며, 3등급 인정점수 완화가 이루어졌는데, 기존 55∼75점에서 53점∼75점으로 완화되었습니다.\n",
      "Before:  None  |  장기요양인정점수 완화가 언제 이루어졌으며 어떤 변화가 있었는지에 대한 정보는 다음과 같습니다.\n",
      "\n",
      "장기요양인정점수 완화는 2012년 7월에 이루어졌습니다. 그 때, 3등급 인정점수 완화가 이루어졌는데, 기존 55∼75점에서 53점∼75점으로 완화되었습니다.\n",
      "\n",
      "21/98\n",
      "Question:  장기요양기관 지정갱신제의 법적 근거가 언제 마련되었는가? | Context Number | 2348\n",
      "Answer:  장기요양기관 지정갱신제의 법적 근거가 2018년 12월에 마련되었고, 2025년에 시행되었습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "22/98\n",
      "Question:  22.10월에 요양보호사 1명당 시설수급자 인력배치기준이 개선된 내용은 무엇인가? | Context Number | 3404\n",
      "Answer:  요양보호사 1명당 시설수급자 인력배치기준이 개선된 내용은 22.10월에 요양보호사 1명당 시설수급자 2.5명에서 2.3명으로 개선되었다.\n",
      "Before:  None  |  None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/98\n",
      "Question:  에너지 바우처 제도의 주요 내용은 무엇인가요? | Context Number | 3140\n",
      "Answer:  에너지 바우처 제도의 주요 내용은 경제적 부담 등으로 에너지 이용에 어려움을 겪는 에너지 소외계층에게 전기·가스·지역난방 등 에너지 이용에 필요한 비용을 지원하는 제도입니다.\n",
      "Before:  None  |  에너지 바우처 제도의 주요 내용은 다음과 같습니다. \n",
      "\n",
      "1.  경제적 부담 등으로 에너지 이용에 어려움을 겪는 에너지 소외계층에게 전기·가스·지역난방 등 에너지 이용에 필요한 비용을 지원하는 제도입니다.\n",
      "2.  하절기바우처 : 하절기 냉방 등을 위한 전기요금을 가상카드 형태로 지원\n",
      "3.  동절기바우처 : 동절기 난방 등을 위한 연탄, 등유, LPG, 전기, 도시가스, 지역난방 등 연료비를 가상카드, 실물카드 형태로 지원\n",
      "4.  연탄쿠폰 : 연탄보일러를 사용하는 대상 가구에 연탄 구입 비용을 연탄쿠폰 형태로 지원\n",
      "5.  등유바우처 : 등유보일러를 사용하는 대상 가구에 동절기 난방을 위한 난방용 등유 구입비용을 실물카드 형태로 지원\n",
      "\n",
      "24/98\n",
      "Question:  에너지바우처 사업의 주요 수혜자는 누구인가요? | Context Number | 3297\n",
      "Answer:  에너지바우처 사업의 주요 수혜자는 노인, 장애인, 영유아, 임산부, 중증‧희귀‧중증난치질환자, 한부모, 소년소녀가정 포함 세대, 연탄쿠폰을 사용하는 기초생활수급자, 차상위계층, 기타 소외계층, 등유를 사용하는 생계‧의료급여(중위소득 40%이하) 수급세대 중, 한부모‧소년소녀가정세대입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "25/98\n",
      "Question:  2024년 에너지바우처 사업의 사업시행주체는 무엇인가요? | Context Number | 2901\n",
      "Answer:  2024 년 에너지바우처 사업의 사업시행주체는 한국에너지공단, 한국광해광업공단입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "26/98\n",
      "Question:  하절기바우처와 동절기바우처의 2024년 예산 규모는 각각 얼마인가요? | Context Number | 2581\n",
      "Answer:  하절기바우처와 동절기바우처의 2024년 예산 규모는 각각 60,950백만원과 600,521백만원입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "27/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 사업운영비 중 에너지복지 홍보에 얼마가 할당되었나요? | Context Number | 4527\n",
      "Answer:  2023년 에너지바우처 사업 예산에서 사업운영비 중 에너지복지 홍보에 할당된 금액은 448백만원입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "28/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 사업운영비 중 시스템 고도화에 얼마가 할당되었나요? | Context Number | 3135\n",
      "Answer:  2023년 에너지바우처 사업 예산에서 사업운영비 중 시스템 고도화에 520백만원이 할당되었으며, 2024년 예산에서 시스템 고도화에 705백만원이 할당되었음을 알 수 있습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "29/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 콜센터 운영에 얼마가 할당되었나요? | Context Number | 3520\n",
      "Answer:  에너지바우처 사업 예산에서 콜센터 운영에 할당된 금액은 502.7백만원입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "30/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 패널조사에 얼마가 할당되었나요? | Context Number | 2778\n",
      "Answer:  패널조사는 2023년 에너지바우처 사업 예산에서 133.5백만원으로 할당되었으며, 2024년 에너지바우처 사업 예산에서 131.7백만원으로 할당되었음을 알 수 있습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "31/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 에너지바우처 전달체계 구축에 얼마가 할당되었나요? | Context Number | 2443\n",
      "Answer:  2023 년 에너지바우처 사업 예산에서 에너지바우처 전달체계 구축에 37.5백만원이 할당되었으며, 2024 년 에너지바우처 사업 예산에서 에너지바우처 전달체계 구축에 34.6백만원이 할당되었음을 알 수 있습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "32/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 주택관리공단 운영지원에 얼마가 할당되었나요? | Context Number | 2443\n",
      "Answer:  주택관리공단 운영지원에 할당된 예산은 50백만원입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "33/98\n",
      "Question:  에너지바우처 사업의 향후 기대효과는 무엇인가요? | Context Number | 2892\n",
      "Answer:  ③ 향후(’24년도 이후) 기대효과 : 저소득층의 적정수준 에너지 접근성이 높아지고, 취약계층의 에너지 비용 부담 경감(약 115만 세대, 세대당 평균 36.7만원 내외)\n",
      "Before:  None  |  ③ 향후(’24년도 이후) 기대효과 : 저소득층의 적정수준 에너지 접근성이 높아지고, 취약계층의 에너지 비용 부담 경감(약 115만 세대, 세대당 평균 36.7만원 내외)\n",
      "\n",
      "34/98\n",
      "Question:  에너지바우처 사업에 대한 예비타당성조사를 어떤 조사기관이 수행했나요? | Context Number | 2006\n",
      "Answer:  에너지바우처 사업에 대한 예비타당성조사는 KDI(한국개발연구원 공공투자관리센터)에서 수행했습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "35/98\n",
      "Question:  21년 국정감사에서 에너지 바우처 사업에 대한 주요 지적사항은 무엇이었나요? | Context Number | 2792\n",
      "Answer:  21년 국정감사에서 에너지 바우처 사업에 대한 주요 지적사항은 저소득층의 폭염 피해 최소화를 위해 동절기 에너지바우처 일부를 하절기에 사용할 수 있도록 제도 개선하고, 수급자 변경 등으로 예산부족이 발생하지 않도록 급여 선정기준 등의 변경계획을 진행했습니다.\n",
      "Before:  None  |  21년 국정감사에서 에너지 바우처 사업에 대한 주요 지적사항은 다음과 같습니다.\n",
      "\n",
      "36/98\n",
      "Question:  21년 에너지바우처 사업에 대한 결산 지적사항은 무엇이었나요? | Context Number | 3846\n",
      "Answer:  2021년 에너지바우처 사업에 대한 결산 지적사항은 '저소득층의 폭염 피해 최소화를 위해 동절기 에너지바우처 일부를 하절기에 사용할 수 있도록 제도 개선'으로, 희망세대 대상으로 동절기 에너지바우처 중 일부를 하절기에 사용할 수 있도록 시스템이 개선되었다고 합니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "37/98\n",
      "Question:  에너지 바우처 사업의 향후 추진방향 중 '취약계층의 에너지 비용 부담 완화'를 위한 계획은 무엇이었나요? | Context Number | 3369\n",
      "Answer:  에너지 바우처 사업의 향후 추진방향 중 '취약계층의 에너지 비용 부담 완화'를 위한 계획은 취약계층 난방비 지원 특별대책에 따른 동절기 지원단가 2배 인상 지원(15.2→30.4만원)으로 동절기 에너지이용 소외계층의 동절기 에너지이용 부담 완화를 위한 것입니다.\n",
      "Before:  None  |  에너지 바우처 사업의 향후 추진방향 중 '취약계층의 에너지 비용 부담 완화'를 위한 계획은 다음과 같습니다.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/98\n",
      "Question:  행복주택출자 사업은 어떤 근거로 추진되고 있는가? | Context Number | 2125\n",
      "Answer:  행복주택출자 사업은 주택도시기금법 제9조 제1항 가목 및 공공주택 특별법 제2조 1호 가목에 근거하고 있습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "39/98\n",
      "Question:  행복주택출자 사업은 어떤 목적으로 시행되는가? | Context Number | 2125\n",
      "Answer:  행복주택출자 사업은 국민의 행복주거 실현을 위한 보편적 주거복지 정책의 일환으로 도심 내 다양한 부지를 활용하여 행복주택을 공급하는 목적으로 시행되고 있습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "40/98\n",
      "Question:  행복주택출자 사업의 주요 수혜자는 누구인가? | Context Number | 2125\n",
      "Answer:  행복주택출자 사업의 주요 수혜자는 대학생・사회초년생・신혼부부 등 젊은층(80%), 고령자 및 주거취약계층(20%)입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "41/98\n",
      "Question:  행복주택출자 사업의 사업비 추이는 어떠한가? | Context Number | 2245\n",
      "Answer:  행복주택출자 사업의 최근 5년간 투입된 사업비는 1,267,123백만원, 1,105,291백만원, 775,293백만원, 684,607백만원, 528,783백만원입니다.\n",
      "Before:  None  |  행복주택출자 사업의 최근 5년간 투입된 사업비는 다음과 같습니다.\n",
      "\n",
      "42/98\n",
      "Question:  행복주택출자 사업의 사업시행주체는 누구인가? | Context Number | 2613\n",
      "Answer:  행복주택출자 사업의 사업시행주체는 한국토지주택공사(LH)와 지자체(지방공사)입니다.\n",
      "Before:  None  |  None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from save_module import save\n",
    "train_faiss = make_db(train_df)\n",
    "test_faiss= make_db(test_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = run(test_faiss,fewshot_retriever,train_faiss, test_df, llm, k=3,verbose=True)\n",
    "save(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
