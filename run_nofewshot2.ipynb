{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Vector stores\n",
    "import fitz  # PyMuPDF \n",
    "import pdfplumber\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter, MarkdownHeaderTextSplitter, MarkdownTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader ,UnstructuredPDFLoader\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever \n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever, OktBM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import pymupdf4llm\n",
    "import time\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from konlpy.tag import Kkma\n",
    "# etc\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import logging\n",
    "from PyPDF2 import PdfReader\n",
    "import json\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intfloat/multilingual-e5-small\n",
    "# jhgan/ko-sroberta-multitask\n",
    "\n",
    "def get_embedding():\n",
    "    \n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='jhgan/ko-sroberta-multitask',\n",
    "        model_kwargs={'device': device},\n",
    "        \n",
    "        encode_kwargs={'normalize_embeddings': True})\n",
    "    return embeddings\n",
    "def normalize_string(s):\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFC', s)\n",
    "        return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"�\", \" \").replace(\"\u0003\", \" \")  # 잘못된 인코딩 문자 제거\n",
    "    return text\n",
    "\n",
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in docs:\n",
    "        # Header 정보를 순서대로 추가\n",
    "        for header_level in range(1, 6):\n",
    "            header_key = f'Header{header_level}'\n",
    "            if header_key in doc.metadata:\n",
    "                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n",
    "        # 문서 내용 추가\n",
    "        context += doc.page_content\n",
    "        context += '\\n---\\n'\n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_path): \n",
    "    md_text = pymupdf4llm.to_markdown(pdf_path)\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        (\"####\", \"Header 4\"),\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=True)\n",
    "    splits = md_header_splitter.split_text(md_text)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=700, chunk_overlap=60\n",
    "    )\n",
    "\n",
    "    splits = text_splitter.split_documents(splits)\n",
    "\n",
    "    for i in splits:\n",
    "        metadata = {'Source_path': pdf_path}\n",
    "        i.metadata = {**i.metadata, **metadata}\n",
    "    return splits\n",
    "\n",
    "\n",
    "def make_db(df):\n",
    "    documents = []\n",
    "    \n",
    "    pdf_files = df['Source_path'].unique()\n",
    "    for pdf_file in tqdm(pdf_files):\n",
    "        # 문서 로드\n",
    "        documents.extend(process_pdf(pdf_file))\n",
    "        \n",
    "    print(f\"Total number of documents: {len(documents)}\")\n",
    "\n",
    "    faiss = FAISS.from_documents(documents, embedding=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_db(df):\n",
    "    df = df.drop('SAMPLE_ID', axis=1)\n",
    "    df = df.drop('Source_path', axis=1)\n",
    "    df = df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", len(df))\n",
    "    to_vectorize = [\"\\n\\n\".join(normalize_string(value) for value in example.values()) for example in df]\n",
    "    faiss = FAISS.from_texts(to_vectorize, embedding=get_embedding())\n",
    "    # bm = BM25Retriever.from_texts(to_vectorize)\n",
    "    # knn = KNNRetriever.from_texts(to_vectorize, embeddings=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    context = \"\"\n",
    "    for doc in docs:\n",
    "        # Header 정보를 순서대로 추가\n",
    "        for header_level in range(1, 6):\n",
    "            header_key = f'Header {header_level}'\n",
    "            if header_key in doc.metadata:\n",
    "                context += f\"{header_key}: {doc.metadata[header_key]}\\n\"\n",
    "        \n",
    "        # 문서 내용 추가\n",
    "        context += doc.page_content\n",
    "        context += '\\n---\\n'\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.87s/it]\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_pipeline(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    # 토크나이저 로드 및 설정\n",
    "        # 양자화 설정 적용\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,low_cpu_mem_usage=True)\n",
    "    \n",
    "    # # 일부 중요한 레이어는 FP16으로 유지\n",
    "    # for name, module in model.named_modules():\n",
    "    #     if \"attention\" in name or \"ffn\" in name:  # 중요한 레이어 식별 (예: attention 및 ffn)\n",
    "    #         module.to(torch.float16)  # 이 부분은 16비트로 유지\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id = terminators,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return llm\n",
    "# ghost-x/ghost-8b-beta-1608\n",
    "# OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n",
    "\n",
    "    #공백 제거\n",
    "    true_sentence = ''.join(true_sentence.split())\n",
    "    predicted_sentence = ''.join(predicted_sentence.split())\n",
    "    \n",
    "    true_counter = Counter(true_sentence)\n",
    "    predicted_counter = Counter(predicted_sentence)\n",
    "\n",
    "    #문자가 등장한 개수도 고려\n",
    "    if sum_mode:\n",
    "        true_positive = sum((true_counter & predicted_counter).values())\n",
    "        predicted_positive = sum(predicted_counter.values())\n",
    "        actual_positive = sum(true_counter.values())\n",
    "\n",
    "    #문자 자체가 있는 것에 focus를 맞춤\n",
    "    else:\n",
    "        true_positive = len((true_counter & predicted_counter).values())\n",
    "        predicted_positive = len(predicted_counter.values())\n",
    "        actual_positive = len(true_counter.values())\n",
    "\n",
    "    #f1 score 계산\n",
    "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_average_f1_score(true_sentences, predicted_sentences):\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n",
    "        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1_score += f1_score\n",
    "    \n",
    "    avg_precision = total_precision / len(true_sentences)\n",
    "    avg_recall = total_recall / len(true_sentences)\n",
    "    avg_f1_score = total_f1_score / len(true_sentences)\n",
    "    \n",
    "    return {\n",
    "        'average_precision': avg_precision,\n",
    "        'average_recall': avg_recall,\n",
    "        'average_f1_score': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_answer(response):\n",
    "    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.replace('**', '')\n",
    "        if line.startswith('Answer:'):\n",
    "            return line.replace('Answer:', '').strip()\n",
    "        if line.startswith('assistant:'):\n",
    "            return line.replace('assistant:', '').strip()\n",
    "    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "def rerun(question,context,answer,llm,num_repeat):\n",
    "    full_template = \"<|begin_of_text|>\"\n",
    "    full_template += \"\"\"<|start_header_id|>system<|end_header_id|>당신은 이전 답변을 검증하는 챗봇입니다. 질문과 문맥, 이전 답변을 참고해서 지시사항을 따르세요. 지시사항을 따를 때 서론 없이 출력하세요.<|eot_id|>\"\"\"\n",
    "    full_template += f\"\"\"<|start_header_id|>user<|end_header_id|>Question: {question} \\n\\nContexts: {context} \\n\\nPrevious Answer: {answer} \\n\\n\"\"\"\n",
    "    full_template += \"\"\"{input}<|eot_id|>\"\"\"\n",
    "    full_template += \"\"\"<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate(template=full_template)\n",
    "    chain = (\n",
    "    {\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "    return chain.invoke(\"핵심 단어들을 바탕으로, 한 문장으로 요약하세요. 만약 한 문장이라면 그대로 출력하세요.\")\n",
    "\n",
    "def run(faiss,dataset,llm,k=3,verbose=False):\n",
    "    results = []\n",
    "    source_path = dataset.iloc[0]['Source_path']\n",
    "    docs = faiss.similarity_search(\n",
    "    query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "    filter={\"Source_path\": source_path},\n",
    "    k = 99,\n",
    "    fetch_k = 20000\n",
    "    )\n",
    "    buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "    faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n",
    "    # faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n",
    "    # knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "    # knn_retriever.k = k\n",
    "    bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "    bm_retriever.k = k\n",
    "\n",
    "    ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr,bm_retriever], weight=[0.5,0.5])\n",
    "\n",
    "    \n",
    "    for i, row in (dataset.iterrows()):\n",
    "        if source_path != row['Source_path']:   \n",
    "            source_path = row['Source_path']\n",
    "            docs = faiss.similarity_search(\n",
    "                query=\"\",  # 유사도 기반이 아닌 메타데이터 필터링만 사용하므로 query는 빈 값으로\n",
    "                filter={\"Source_path\": source_path},\n",
    "                k = 99,\n",
    "                fetch_k = 20000\n",
    "            )\n",
    "            buff_faiss = FAISS.from_documents(docs, embedding=get_embedding())\n",
    "            faiss_retriever_mmr = buff_faiss.as_retriever(search_type=\"mmr\",search_kwargs={\"k\": k})\n",
    "            # faiss_retriever_sim = buff_faiss.as_retriever(search_kwargs={\"k\": k})\n",
    "            # knn_retriever = KNNRetriever.from_documents(docs, embeddings=get_embedding())\n",
    "            # knn_retriever.k = k\n",
    "            bm_retriever = OktBM25Retriever.from_documents(docs)\n",
    "            bm_retriever.k = k\n",
    "\n",
    "            ensemble_retriever = EnsembleRetriever(retrievers=[faiss_retriever_mmr,bm_retriever], weight=[0.5,0.5])\n",
    "\n",
    "            \n",
    "        full_template = \"<|begin_of_text|>\"\n",
    "        full_template += \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "당신은 유용한 금융 정보 QnA 챗봇입니다.\n",
    "문맥 정보를 활용해서 답변을 작성하세요. \n",
    "객관적이고 공식적인 문체를 사용하세요.\n",
    "서론 없이 핵심 내용을 간결하게 평문으로 작성하세요.<|eot_id|>\n",
    "\"\"\"\n",
    "        question = row['Question']          \n",
    "        # full_template += \"\"\" \"\"\"\n",
    "        contexts = ensemble_retriever.invoke(normalize_string(question))\n",
    "        contexts = format_docs(contexts)\n",
    "        \n",
    "        full_template += \"<|start_header_id|>user<|end_header_id|>\"\n",
    "        full_template += \"Question: {input}\\n\\n<|eot_id|>\"\n",
    "        full_template += \"Contexts: {contexts}\"\n",
    "        full_template += \"\"\"<|eot_id|>\"\"\"\n",
    "        full_template += \"\"\"<|start_header_id|>assistant<|end_header_id>\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=full_template, input_variables=[\"contexts\",\"input\"])\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"contexts\": RunnablePassthrough(),\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        answer = qa_chain.invoke({'input':question,'contexts':contexts})\n",
    "        answer = extract_answer(answer)\n",
    "        lines = answer.split('\\n')\n",
    "        if  len(lines) > 1 or '|' in answer or ':' in answer:\n",
    "            previous = answer\n",
    "            try:\n",
    "                before = calculate_f1_score(row['Answer'],answer)[2]\n",
    "            except:\n",
    "                before = None\n",
    "            answer = rerun(question=question,\n",
    "                           context=contexts,\n",
    "                           answer=answer,\n",
    "                           llm=llm,\n",
    "                           num_repeat=1)\n",
    "        answer = extract_answer(answer)\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Source\": row['Source']\n",
    "        })\n",
    "        if verbose:\n",
    "            print(f\"{i}/{len(dataset)}\")\n",
    "            print(\"Question: \", question, end=\" | \")\n",
    "            print(\"Context Number |\",len(contexts))\n",
    "            try:\n",
    "                print(calculate_f1_score(row['Answer'],answer)[2],end=\" | \")\n",
    "            except:\n",
    "                pass\n",
    "            print(\"Answer: \", results[-1]['Answer'])\n",
    "            try:\n",
    "                print(\"Before: \",before,\" | \",previous)  \n",
    "                \n",
    "                previous = None\n",
    "                before = None\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                print(\"REAL Answer: \",row['Answer'])\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            print()\n",
    "        torch.cuda.empty_cache()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 케이폴드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "# import copy\n",
    "# k_folds = 4\n",
    "# fold_results = []\n",
    "# kf = KFold(n_splits=k_folds, shuffle=True, random_state=52)\n",
    "# for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "#     fold_result = []\n",
    "#     train_set = train_df.iloc[train_index]\n",
    "#     val_set = train_df.iloc[val_index]\n",
    "\n",
    "#     faiss = make_db(val_set)\n",
    "\n",
    "#     pred = run(faiss,val_set, llm, verbose=True)\n",
    "#     result = pd.DataFrame()\n",
    "#     result['pred'] = [result['Answer'] for result in pred]\n",
    "#     val_set.index = range(len(val_set))\n",
    "#     result['gt'] = val_set['Answer']\n",
    "        \n",
    "#     result = calculate_average_f1_score(result['gt'], result['pred'])\n",
    "#     print(result)\n",
    "#     fold_results.append(result)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:12<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of documents: 228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:653: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/98\n",
      "Question:  2022년 혁신창업사업화자금(융자)의 예산은 얼마인가요? | Context Number | 821\n",
      "Answer:  2022년 혁신창업사업화자금(융자)의 예산은 2,300,000백만원입니다.\n",
      "\n",
      "1/98\n",
      "Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 무엇인가요? | Context Number | 1060\n",
      "Answer:  중소벤처기업부의 혁신창업사업화자금(융자) 사업목적은 창업기반지원 및 개발기술사업화입니다.\n",
      "\n",
      "2/98\n",
      "Question:  중소벤처기업부의 혁신창업사업화자금(융자) 사업근거는 어떤 법률에 근거하고 있나요? | Context Number | 1474\n",
      "Answer:  중소벤처기업부의 혁신창업사업화자금(융자) 사업근거는 중소기업진흥에 관한 법률 제66조, 제67조, 제74조 및 중소기업창업지원법 제35조에 근거하고 있습니다.\n",
      "\n",
      "3/98\n",
      "Question:  2010년에 신규 지원된 혁신창업사업화자금은 무엇인가요? | Context Number | 1596\n",
      "Answer:  2010년에 신규 지원된 혁신창업사업화자금은 재창업자금(실패 경영인에 대한 재기지원)이였습니다.\n",
      "\n",
      "4/98\n",
      "Question:  혁신창업사업화자금 중 2020년에 신규 지원된 자금은 무엇인가요? | Context Number | 1242\n",
      "Answer:  2020 년에 신규 지원된 혁신창업사업화자금은 미래기술육성자금, 고성장촉진자금입니다.\n",
      "\n",
      "5/98\n",
      "Question:  재창업자금이 재도약지원자금으로 이관된 연도는 언제인가요? | Context Number | 1454\n",
      "Answer:  2015년 재창업자금이 재도약지원자금으로 이관된 연도입니다.\n",
      "\n",
      "6/98\n",
      "Question:  창업기반지원과 신청 대상이 중복인 자금이 어떤 것이며, 이 자금이 폐지된 연도는 언제인가요? | Context Number | 1790\n",
      "Answer:  창업기반지원과 신청 대상이 중복인 자금은 일자리창출촉진자금입니다. 이 자금은 2023년 1월에 폐지되었습니다.\n",
      "\n",
      "7/98\n",
      "Question:  혁신창업사업화자금(융자) 사업을 시행하는 주체는 누구인가요? | Context Number | 1413\n",
      "Answer:  중소벤처기업진흥공단\n",
      "\n",
      "8/98\n",
      "Question:  혁신창업사업화자금(융자) 사업 집행절차는 어떻게 되나요? | Context Number | 945\n",
      "Answer:  혁신창업사업화자금(융자) 사업 집행절차는 사전상담 및 신청, 사업계획수립/공고, 융자 실행 지원결정통보, 융자 실행의 4단계로 진행됩니다.\n",
      "Before:  None  |  혁신창업사업화자금(융자) 사업 집행절차는 다음과 같습니다.\n",
      "\n",
      "1.  사전상담 및 신청: 중기부, 중진공, 중소기업이 사전상담 및 신청을 합니다.\n",
      "2.  사업계획수립/공고: 중기부, 중진공이 사업계획을 수립하고 공고합니다.\n",
      "3.  융자 실행 지원결정통보: 중진공, 은행이 중소기업에게 융자 지원결정통보를 합니다.\n",
      "4.  융자 실행: 중진공, 은행이 중소기업에게 융자를 제공합니다.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/98\n",
      "Question:  부모급여 지원 사업의 목적은 무엇인가요? | Context Number | 2339\n",
      "Answer:  부모급여 지원 사업의 목적은 출산 및 양육으로 손실되는 소득을 보전하고, 주 양육자의 직접 돌봄이 중요한 아동발달의 특성에 따라 영아기 돌봄을 두텁게 지원하기 위해 부모급여 지급입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "10/98\n",
      "Question:  부모급여(영아수당)의 2024년 확정된 예산은 몇백만원인가요? | Context Number | 1710\n",
      "Answer:  2024년 부모급여(영아수당)의 확정된 예산은 2,888,694백만원입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "11/98\n",
      "Question:  부모급여 지원 사업은 어떤 법령상 근거를 갖고 추진되고 있나요? | Context Number | 1766\n",
      "Answer:  부모급여 지원 사업은 아동수당법 제4조 제5항에 근거하고 있습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "12/98\n",
      "Question:  영아수당 도입에 대한 추진경위는 어떻게 되나요? | Context Number | 2287\n",
      "Answer:  영아수당 도입에 대한 추진경위는 아동수당법 제4조제5항으로 근거를 두고 2021년 8월에 예비타당성조사 통과, 2021년 12월에 근거법을 마련, 2022년 1월부터 영아수당 지원사업을 시행, 2022년 5월에 '부모급여 도입' 대통령 공약사항 및 국정과제에 포함, 2023년 1월부터 부모급여 지원사업을 시행, 2023년 5월에 부모급여 지급근거를 규정한 아동수당법을 개정했습니다.\n",
      "Before:  None  |  영아수당 도입에 대한 추진경위는 다음과 같습니다.\n",
      "\n",
      "① 법령상 근거 및 조항 적시 : 아동수당법 제4조제5항\n",
      "\n",
      "② 추진경위\n",
      "\n",
      "‧  예비타당성조사 통과(’21.8.), 근거법 마련(’21.12.)\n",
      "\n",
      "‧  영아수당 지원사업 시행(’22.1~)\n",
      "\n",
      "‧  ‘부모급여 도입’ 대통령 공약사항 및 국정과제[*]에 포함(’22.5.)\n",
      "\n",
      "‧  (국정과제46-1) ’24년부터0～11개월아동에월100만원부모급여지급으로가정양육지원 및 부모의경제적부담을획기적으로완화(’23년70만원, ’24년100만원으로단계적확대)\n",
      "\n",
      "‧  부모급여 지원사업 시행(’23.1~)\n",
      "\n",
      "‧  부모급여 지급근거를 규정한 ｢아동수당법｣ 개정(’23.5)\n",
      "\n",
      "13/98\n",
      "Question:  부모급여 지원사업은 언제부터 시행되었나요? | Context Number | 1497\n",
      "Answer:  부모급여 지원사업은 2022년 1월부터 시행되었습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "14/98\n",
      "Question:  보건복지부의 부모급여(영아수당) 지원 사업시행방법은 무엇이며, 사업 수혜자는 누구인가? | Context Number | 1818\n",
      "Answer:  보건복지부의 부모급여(영아수당) 지원 사업시행방법은 다음과 같습니다.\n",
      "Before:  None  |  None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/98\n",
      "Question:  노인장기요양보험 사업 운영에 대한 목적은 무엇인가요? | Context Number | 2705\n",
      "Answer:  노인장기요양보험 사업 운영의 목적은 노인 및 고령자에게 신체 또는 가사 활동을 제공하여 노후의 건강증진 및 생활 안정을 도모하고 가족의 부담을 완화하여 국민 삶의 질을 향상하는 것입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "16/98\n",
      "Question:  노인장기요양보험 운영지원에 대한 사업 내용을 설명해줘. | Context Number | 2509\n",
      "Answer:  노인장기요양보험 운영지원에 대한 사업 내용은 다음과 같습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "17/98\n",
      "Question:  국고지원을 받는 기타 의료급여수급권자는 누구인가요? | Context Number | 2692\n",
      "Answer:  국고지원을 받는 기타 의료급여수급권자는 '국민기초생활 보장법'에 의한 의료급여수급권자를 제외한 이재민, 의사상자, 국가유공자, 입양아동, 국가무형문화재보유자, 북한이탈주민등입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "18/98\n",
      "Question:  장기요양보험가입자 및 피부양자의 자격취득과 관련하여 어떤 법률을 준용해야 하는가? | Context Number | 2218\n",
      "Answer:  장기요양보험가입자 및 피부양자의 자격취득과 관련하여 '노인장기요양보험법' 제4조, 제11조, 제35조의2, 제58조 및 같은 법 시행령 제28조, '국민건강보험법' 제76조를 준용합니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "19/98\n",
      "Question:  노인장기요양보험법이 언제 제정되고 공포되었나? | Context Number | 3525\n",
      "Answer:  노인장기요양보험법은 2007년 4월에 제정되어 2008년 7월에 공포되었습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "20/98\n",
      "Question:  장기요양인정점수 완화가 언제 이루어졌으며, 어떤 변화가 있었나? | Context Number | 2643\n",
      "Answer:  장기요양인정점수 완화는 2013년 7월에 이루어졌습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "21/98\n",
      "Question:  장기요양기관 지정갱신제의 법적 근거가 언제 마련되었는가? | Context Number | 1785\n",
      "Answer:  장기요양기관 지정갱신제의 법적 근거는 2018년 12월에 마련되었으며, 2025년부터 시행됩니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "22/98\n",
      "Question:  22.10월에 요양보호사 1명당 시설수급자 인력배치기준이 개선된 내용은 무엇인가? | Context Number | 1908\n",
      "Answer:  요양보호사 1명당 시설수급자 인력배치기준이 22.10월에 개선된 내용은 요양보호사 1명당 시설수급자 2.5명에서 2.3명으로 개선된 것입니다.\n",
      "Before:  None  |  None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/98\n",
      "Question:  에너지 바우처 제도의 주요 내용은 무엇인가요? | Context Number | 3089\n",
      "Answer:  에너지바우처 제도의 주요 내용은 에너지법 제16조의2, 제16조의3 및 에너지 및 지원사업 특별회계법 제2조에 근거하여 모든 국민에게 에너지를 보편적으로 공급하도록 하기 위하여 노인, 장애인, 영유아, 임산부, 중증·희귀‧중증난치질환자, 한부모, 소년소녀가정 포함 세대에게 에너지비용을 지원하는 사업입니다.\n",
      "Before:  None  |  에너지 바우처 제도의 주요 내용은 다음과 같습니다.\n",
      "\n",
      "① 사업 근거와 추진 경위\n",
      "\n",
      "에너지바우처는 에너지법 제16조의2, 제16조의3 및 에너지 및 지원사업 특별회계법 제2조에 근거하여 시행하는 사업입니다. 사업의 목적은 모든 국민에게 에너지를 보편적으로 공급하도록 하기 위하여 에너지이용 소외계층에 속하는 사람에게 에너지이용권을 발급하는 것입니다.\n",
      "\n",
      "② 사업 추진 체계\n",
      "\n",
      "사업의 추진은 한국에너지공단, 한국광해광업공단이 공동으로 수행하고, 사업의 수혜자는 노인, 장애인, 영유아, 임산부, 중증·희귀‧중증난치질환자, 한부모, 소년소녀가정 포함 세대입니다.\n",
      "\n",
      "③ 사업의 주요 내용\n",
      "\n",
      "에너지바우처는 동·하절기 에너지비용을 지원하는 사업으로, 2023년 예산은 바우처 사용(~'24.4월) 등 지원 완료 후 최종 정산 예정입니다.\n",
      "\n",
      "④ 사업의 효과\n",
      "\n",
      "에너지바우처는 사회배려계층 하계누진부담완화 사업을 시행하고, 외국인을 에너지바우처 세대원에 포함하여 보건복지부와 사회보장제도 변경협의를 완료하였습니다. 또한, 에너지바우처 지원대상 세대원에 국민기초생활법상 수급자인 외국인 포함, 동절기 에너지바우처 지원단가 8.2%인상, 주거·교육급여 수급세대를 에너지바우처 지원대상에 포함하기 위해 보건복지부와 사회보장제도 변경협의를 완료하였습니다.\n",
      "\n",
      "24/98\n",
      "Question:  에너지바우처 사업의 주요 수혜자는 누구인가요? | Context Number | 1744\n",
      "Answer:  에너지바우처 사업의 주요 수혜자는 노인, 장애인, 영유아, 임산부, 중증·희귀‧중증난치질환자, 한부모, 소년소녀가정 포함 세대, 연탄을 사용하는 기초생활수급자, 차상위계층, 기타 소외계층입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "25/98\n",
      "Question:  2024년 에너지바우처 사업의 사업시행주체는 무엇인가요? | Context Number | 2911\n",
      "Answer:  2024년 에너지바우처 사업의 사업시행주체는 한국에너지공단, 한국광해광업공단입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "26/98\n",
      "Question:  하절기바우처와 동절기바우처의 2024년 예산 규모는 각각 얼마인가요? | Context Number | 3081\n",
      "Answer:  하절기바우처와 동절기바우처의 2024년 예산 규모는 각각 60,950백만원, 600,521백만원입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "27/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 사업운영비 중 에너지복지 홍보에 얼마가 할당되었나요? | Context Number | 3028\n",
      "Answer:  2023년 에너지바우처 사업 예산에서 사업운영비 중 에너지복지 홍보에 할당된 금액은 328백만원에서 448백만원으로 증가하였습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "28/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 사업운영비 중 시스템 고도화에 얼마가 할당되었나요? | Context Number | 2575\n",
      "Answer:  2023년 에너지바우처 사업 예산에서 시스템 고도화에 할당된 금액은 520백만원에서 705백만원으로 증가했습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "29/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 콜센터 운영에 얼마가 할당되었나요? | Context Number | 3168\n",
      "Answer:  2023년 에너지바우처 사업 예산에서 콜센터 운영에 502.7백만원이 할당되었어요.\n",
      "Before:  None  |  None\n",
      "\n",
      "30/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 패널조사에 얼마가 할당되었나요? | Context Number | 2631\n",
      "Answer:  2023 년 에너지바우처 사업 예산에서 패널조사에 할당된 금액은 131.7 백만원입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "31/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 에너지바우처 전달체계 구축에 얼마가 할당되었나요? | Context Number | 2735\n",
      "Answer:  2023년 에너지바우처 사업 예산에서 에너지바우처 전달체계 구축에 할당된 예산은 37.5백만원이었습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "32/98\n",
      "Question:  2023년 에너지바우처 사업 예산에서 주택관리공단 운영지원에 얼마가 할당되었나요? | Context Number | 2699\n",
      "Answer:  2023년 에너지바우처 사업 예산에서 주택관리공단 운영지원에 50백만원이 할당되었습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "33/98\n",
      "Question:  에너지바우처 사업의 향후 기대효과는 무엇인가요? | Context Number | 3136\n",
      "Answer:  저소득층의 적정수준 에너지 접근성이 높아지고, 취약계층의 에너지 비용 부담 경감이 예상됩니다.\n",
      "Before:  None  |  에너지바우처 사업의 향후 기대효과는 다음과 같습니다.\n",
      "\n",
      "저소득층의 적정수준 에너지 접근성이 높아지고, 취약계층의 에너지 비용 부담 경감(약 115만 세대, 세대당 평균 36.7만원 내외)이 예상됩니다.\n",
      "\n",
      "34/98\n",
      "Question:  에너지바우처 사업에 대한 예비타당성조사를 어떤 조사기관이 수행했나요? | Context Number | 1911\n",
      "Answer:  에너지바우처 사업에 대한 예비타당성조사는 KDI(한국개발연구원 공공투자관리센터)가 수행했습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "35/98\n",
      "Question:  21년 국정감사에서 에너지 바우처 사업에 대한 주요 지적사항은 무엇이었나요? | Context Number | 2335\n",
      "Answer:  저소득층을 위한 동절기 에너지 바우처의 사용을 위한 제도 개선, 수급자 변경 등으로 예산 부족을 방지하기 위한 급여 선정 기준 등의 변경 계획, 기후변화와 에너지 가격 상승에 따른 취약 계층의 에너지 비용 부담 완화 등이 국정감사에서 에너지 바우처 사업에 대한 주요 지적 사항입니다.\n",
      "Before:  None  |  2021년 국정감사에서 에너지 바우처 사업에 대한 주요 지적 사항은 다음과 같습니다.\n",
      "\n",
      "저소득층의 폭염 피해 최소화를 위해 동절기 에너지 바우처 일부를 하절기에 사용할 수 있도록 제도 개선할 것\n",
      "\n",
      "수급자 변경 등으로 예산 부족이 발생하지 않도록 급여 선정기준 등의 변경 계획 등에 대하여 보건복지부와 지자체와 면밀히 소통하여 예산을 편성하고 사업을 추진\n",
      "\n",
      "기후변화와 에너지 가격 상승에 따른 취약 계층의 에너지 비용 부담 완화를 위해 지속적 지원 단가 현실화 추진\n",
      "\n",
      "재정 사업 자율 평가(2022년): 보통(78.1점)\n",
      "\n",
      "국고 보조 사업 연장 평가(2021년): 지속 지원(87.8점)\n",
      "\n",
      "국정감사에서 에너지 바우처 사업에 대한 주요 지적 사항은 저소득층을 위한 동절기 에너지 바우처의 사용을 위한 제도 개선, 수급자 변경 등으로 예산 부족을 방지하기 위한 급여 선정 기준 등의 변경 계획, 기후변화와 에너지 가격 상승에 따른 취약 계층의 에너지 비용 부담 완화 등으로 주요한 내용입니다.\n",
      "\n",
      "36/98\n",
      "Question:  21년 에너지바우처 사업에 대한 결산 지적사항은 무엇이었나요? | Context Number | 3630\n",
      "Answer:  저소득층의 폭염 피해 최소화를 위해 동절기 에너지바우처 일부를 하절기에 사용할 수 있도록 제도 개선이 필요하고, 수급자 변경 등으로 예산 부족이 발생하지 않도록 급여 선정기준 등의 변경계획에 대해 보건복지부와 지자체와 면밀히 소통하여 예산을 편성하고 사업을 추진할 필요성이 있습니다.\n",
      "Before:  None  |  2021년 에너지바우처 사업에 대한 결산 지적사항은 다음과 같습니다.\n",
      "\n",
      "1.  저소득층의 폭염 피해 최소화를 위해 동절기 에너지바우처 일부를 하절기에 사용할 수 있도록 제도 개선이 필요하다.\n",
      "2.  수급자 변경 등으로 예산 부족이 발생하지 않도록 급여 선정기준 등의 변경계획에 대해 보건복지부와 지자체와 면밀히 소통하여 예산을 편성하고 사업을 추진할 필요성이 있다.\n",
      "\n",
      "37/98\n",
      "Question:  에너지 바우처 사업의 향후 추진방향 중 '취약계층의 에너지 비용 부담 완화'를 위한 계획은 무엇이었나요? | Context Number | 3213\n",
      "Answer:  에너지 바우처 사업의 향후 추진방향 중 '취약계층의 에너지 비용 부담 완화'를 위한 계획은 기후변화와 에너지가격 상승에 따른 취약계층의 에너지 비용 부담 완화를 위해 지속적 지원단가 현실화 추진하는 것입니다.\n",
      "Before:  None  |  None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/98\n",
      "Question:  행복주택출자 사업은 어떤 근거로 추진되고 있는가? | Context Number | 1851\n",
      "Answer:  행복주택출자 사업은 주택도시기금법 제9조제1항가목과 공공주택특별법 제2조1호가목에 근거하고 있습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "39/98\n",
      "Question:  행복주택출자 사업은 어떤 목적으로 시행되는가? | Context Number | 1966\n",
      "Answer:  행복주택출자 사업은 국민의 행복주거 실현을 위한 보편적 주거복지 정책의 일환으로 도심 내 다양한 부지를 활용하여 행복주택을 공급하는 목적으로 시행됩니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "40/98\n",
      "Question:  행복주택출자 사업의 주요 수혜자는 누구인가? | Context Number | 1851\n",
      "Answer:  행복주택 출자 사업의 주요 수혜자는 대학생・사회초년생・신혼부부 등 젊은층(80%), 고령자 및 주거취약계층(20%)입니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "41/98\n",
      "Question:  행복주택출자 사업의 사업비 추이는 어떠한가? | Context Number | 2240\n",
      "Answer:  행복주택출자 사업의 사업비는 다음과 같습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "42/98\n",
      "Question:  행복주택출자 사업의 사업시행주체는 누구인가? | Context Number | 2342\n",
      "Answer:  행복주택출자 사업의 사업시행주체는 한국토지주택공사(LH)와 지자체(지방공사)입니다.\n",
      "Before:  None  |  None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43/98\n",
      "Question:  국고보조사업의 보조율은 어떠한 기준에 따라 운용되는가? | Context Number | 2888\n",
      "Answer:  국고보조사업의 보조율은 「보조금법」과 일부 개별 법령에 근거하여 운영되며, 보조금법에 의해 지방자치단체의 재정여건을 고려하여 기준보조율과 차등보조율을 적용하여 운용됩니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "44/98\n",
      "Question:  프랑스의 재정조정제도에서 최근 강조되는 형평교부금은 어떤 역할을 하는가? | Context Number | 3600\n",
      "Answer:  수평적 형평교부금의 역할을 강조하는 프랑스의 재정조정제도에서, 최근 강조되는 형평교부금은 중앙정부의 재정적자로 인한 수평적 형평교부금의 역할을 강조하고 있습니다.\n",
      "Before:  None  |  None\n",
      "\n",
      "45/98\n",
      "Question:  지방재정조정제도의 목적은 무엇인가? | Context Number | 3231\n",
      "Answer:  지방재정조정제도의 목적은 중앙-지방 간 재정 불균형을 완화하고 지방 정부가 본연의 기능을 수행하고 국가 균형발전을 위해 재정을 조정하는 일련의 조치를 의미합니다.\n",
      "Before:  None  |  None\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_project\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46/98\n",
      "Question:  국제적으로 성과중심 재정관리 강화 움직임이 확산된 시기는 언제인가? | Context Number | 2913\n",
      "Answer:  2007년\n",
      "Before:  None  |  None\n",
      "\n",
      "47/98\n",
      "Question:  한국의 재정사업 성과관리제도는 어떠한 법을 통해 운영되고 있으며, 성과관리 기본계획과 추진계획은 어떻게 의무화되었는가? | Context Number | 2350\n",
      "Answer:  한국의 재정사업 성과관리제도는 「국가재정법」을 통해 운영되고 있으며, 성과관리 기본계획과 추진계획은 2021년 12월 법 개정을 통해 의무화되었다.\n",
      "Before:  None  |  None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from save_module import save\n",
    "\n",
    "faiss= make_db(test_df)\n",
    "\n",
    "results = run(faiss, test_df, llm, verbose=True)\n",
    "save(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
