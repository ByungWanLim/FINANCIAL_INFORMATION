{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import os\n",
    "from glob import glob\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFacePipeline ,HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import (\n",
    "    FewShotPromptTemplate,\n",
    "    ChatPromptTemplate,\n",
    "    FewShotChatMessagePromptTemplate\n",
    ")\n",
    "import bitsandbytes as bnb\n",
    "import pickle\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "def get_embedding():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='intfloat/multilingual-e5-small',\n",
    "        model_kwargs={'device': 'mps'},\n",
    "        encode_kwargs={'normalize_embeddings': True}\n",
    "        )\n",
    "    return embeddings\n",
    "\n",
    "def load_and_vectorize(csv_path, db_path):\n",
    "    if os.path.exists(db_path) and os.path.exists(db_path + '_metadata.pkl'):\n",
    "        print(\"Loading FAISS DB from:\", db_path)\n",
    "        db, metadata = load_faiss_db(db_path)\n",
    "        return db\n",
    "\n",
    "    # CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ ë°ì´í„°í”„ë ˆì„ ìƒì„±\n",
    "    train_df = pd.read_csv(csv_path)\n",
    "    train_df.drop('SAMPLE_ID', axis=1, inplace=True)\n",
    "    trainset = train_df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", trainset[:3])\n",
    "    \n",
    "    # ë²¡í„°í™”í•  í…ìŠ¤íŠ¸ ìƒì„±\n",
    "    to_vectorize = [\"\\n\\n\".join(example.values()) for example in trainset]\n",
    "    \n",
    "    # ë²¡í„°í™” ë° FAISS DB ìƒì„±\n",
    "    fewshow_vectordb = FAISS.from_texts(to_vectorize, embedding=get_embedding(), metadatas=trainset)\n",
    "    \n",
    "    # FAISS DB ì €ì¥\n",
    "    save_faiss_db(fewshow_vectordb, db_path)\n",
    "    \n",
    "    return fewshow_vectordb\n",
    "\n",
    "def save_faiss_db(db, db_path):\n",
    "    db.save_local(db_path)\n",
    "    with open(db_path + '_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(db, f)\n",
    "\n",
    "def load_faiss_db(db_path):\n",
    "    db = FAISS.load_local(db_path, embeddings=get_embedding(), allow_dangerous_deserialization=True)\n",
    "    with open(db_path + '_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return db, metadata\n",
    "\n",
    "def load_chunks_make_docdb(pdf_directory, db_path):\n",
    "    if os.path.exists(db_path) and os.path.exists(db_path + '_metadata.pkl'):\n",
    "        print(\"Loading FAISS DB from:\", db_path)\n",
    "        db, metadata = load_faiss_db(db_path)\n",
    "        return db\n",
    "\n",
    "    print(\"Loading PDF files from:\", pdf_directory)\n",
    "    documents = []\n",
    "\n",
    "    # PDF íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ ë¶„í• \n",
    "    pdf_files = glob(os.path.join(pdf_directory, '*.pdf').replace('\\\\', '/'))\n",
    "    for pdf_file in pdf_files:\n",
    "        loader = PyPDFLoader(pdf_file)\n",
    "        pdf_documents = loader.load()\n",
    "        documents.extend(pdf_documents)\n",
    "    \n",
    "    # ë¶„í• ëœ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ FAISS DBì— ì €ì¥\n",
    "    chunk_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    chunks = chunk_splitter.split_documents(documents)\n",
    "    print(\"Done.\", len(chunks), \"chunks\")\n",
    "    \n",
    "    print(\"Creating FAISS DB\")\n",
    "    # FAISS DB ìƒì„± ë° ì €ì¥\n",
    "    db = FAISS.from_documents(chunks, embedding=get_embedding())\n",
    "    save_faiss_db(db, db_path)\n",
    "    print(\"Done.\")\n",
    "    \n",
    "    return db\n",
    "\n",
    "def make_fewshot_prompt(fewshot_vectordb, k = 3):\n",
    "    # Semantic Similarity Example Selector ì„¤ì •\n",
    "    example_prompt = PromptTemplate.from_template(\"Question: {Question}\\nAnswer: {Answer}\\nSource: {Source}\")\n",
    "\n",
    "    example_selector = SemanticSimilarityExampleSelector(\n",
    "        vectorstore=fewshot_vectordb,\n",
    "        k=k,\n",
    "    )\n",
    "\n",
    "    # FewShotPromptTemplate ìƒì„±\n",
    "    fewshot_prompt = FewShotPromptTemplate(\n",
    "        example_selector=example_selector,\n",
    "        example_prompt=example_prompt,\n",
    "        suffix=\"Question: {input}\",\n",
    "        input_variables=[\"input\"],\n",
    "    )\n",
    "    return fewshot_prompt\n",
    "\n",
    "def make_dict(dir='train.csv'):\n",
    "    df = pd.read_csv(dir)\n",
    "    df.drop('SAMPLE_ID', axis=1, inplace=True)\n",
    "    \n",
    "    return df.to_dict(orient='records')\n",
    "\n",
    "def setup_llm_pipeline(model_id):\n",
    "    # í† í¬ë‚˜ì´ì € ë¡œë“œ ë° ì„¤ì •\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    \n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    # ëª¨ë¸ ë¡œë“œ ë° ì–‘ìí™” ì„¤ì • ì ìš©\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        #trust_remote_code=True \n",
    "        )\n",
    "\n",
    "    # HuggingFacePipeline ê°ì²´ ìƒì„±\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        #model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "        do_sample = True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        return_full_text=False,\n",
    "        # eos_token_id=terminators,\n",
    "        max_new_tokens=128,\n",
    "        pad_token_id=tokenizer.eos_token_id  # íŒ¨ë”© í† í°ì„ EOS í† í° IDë¡œ ì„¤ì •\n",
    "    )\n",
    "\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS DB from: ./fewshot_faiss_db\n",
      "Loading FAISS DB from: ./train_faiss_db\n",
      "Loading FAISS DB from: ./test_faiss_db\n"
     ]
    }
   ],
   "source": [
    "fewshot_db = load_and_vectorize('train.csv', './fewshot_faiss_db')\n",
    "fewshot_prompt = make_fewshot_prompt(fewshot_db)\n",
    "\n",
    "train_db = load_chunks_make_docdb('./train_source', './train_faiss_db')\n",
    "train_retriever = train_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1})\n",
    "\n",
    "test_db = load_chunks_make_docdb('./test_source', './test_faiss_db')\n",
    "test_retriver = test_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 3})\n",
    "\n",
    "train_dict = make_dict('train.csv')\n",
    "test_dict = make_dict('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Source': 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€_ì°½ì—…ì‚¬ì—…í™”ì§€ì›', 'Source_path': './train_source/ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€_ì°½ì—…ì‚¬ì—…í™”ì§€ì›.pdf', 'Question': 'ì°½ì—…ì‚¬ì—…í™”ì§€ì› ì‚¬ì—…ì˜ ë²•ë ¹ìƒ ê·¼ê±°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?', 'Answer': 'ã€Œì¤‘ì†Œê¸°ì—…ì°½ì—… ì§€ì›ë²•ã€ ì œ10ì¡°(ì°½ì—… í™œì„±í™” ì§€ì›ì‚¬ì—…ì˜ ì¶”ì§„ ë“±)ê³¼ ã€Œê¸°ìˆ ì˜ ì´ì „ ë° ì‚¬ì—…í™” ì´‰ì§„ì— ê´€í•œ ë²•ë¥ ã€ ì œ15ì¡°(ê¸°ìˆ ì´ì „ã†ì‚¬ì—…í™” ì´‰ì§„ì‚¬ì—…ì˜ ì¶”ì§„)'}, {'Source': 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€_ì°½ì—…ì‚¬ì—…í™”ì§€ì›', 'Source_path': './train_source/ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€_ì°½ì—…ì‚¬ì—…í™”ì§€ì›.pdf', 'Question': 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ì˜ ì°½ì—…ì‚¬ì—…í™”ì§€ì› ì‚¬ì—… ì§‘í–‰ì ˆì°¨ì—ì„œ ì˜ˆë¹„ì°½ì—…ìã†ì°½ì—…ã†ë²¤ì²˜ê¸°ì—…ê³¼ ì£¼ê´€ê¸°ê´€ì´ ê´€ì—¬í•˜ëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?', 'Answer': 'ì˜ˆë¹„ì°½ì—…ìã†ì°½ì—…ã†ë²¤ì²˜ê¸°ì—…ê³¼ ì£¼ê´€ê¸°ê´€ì€ ì°½ì—… ë° ì‚¬ì—…í™” ë¶€ë¶„ì— ê´€ì—¬í•œë‹¤.'}, {'Source': 'ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€_ì°½ì—…ì‚¬ì—…í™”ì§€ì›', 'Source_path': './train_source/ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€_ì°½ì—…ì‚¬ì—…í™”ì§€ì›.pdf', 'Question': \"ì–¸ì œ ì²­ë…„ ì¼ìë¦¬ ëŒ€ì±…ì˜ ì¼í™˜ìœ¼ë¡œ ê¸°ìˆ í˜ì‹  ê¸°ë°˜ ì˜ˆë¹„ì°½ì—…ìì˜ ì‚¬ì—…í™”ë¥¼ ì§€ì›í•˜ëŠ” 'ê¸°ìˆ í˜ì‹ í˜• ì°½ì—…ê¸°ì—… ì§€ì›ì‚¬ì—…'ì´ ì‹ ì„¤ë˜ì—ˆë‚˜ìš”?\", 'Answer': \"ê¸°ìˆ í˜ì‹ í˜• ì°½ì—…ê¸°ì—… ì§€ì›ì‚¬ì—…(í˜„, ì˜ˆë¹„ì°½ì—…íŒ¨í‚¤ì§€)'ì€ '18ë…„ì— ì‹ ì„¤ë˜ì—ˆìŠµë‹ˆë‹¤.\"}]\n"
     ]
    }
   ],
   "source": [
    "i=2\n",
    "def get_example(fewshot_db,k=3):\n",
    "    example_selector = SemanticSimilarityExampleSelector(\n",
    "        vectorstore=fewshot_db,\n",
    "        k=3,\n",
    "    )\n",
    "\n",
    "    # The prompt template will load examples by passing the input do the `select_examples` method\n",
    "    ex = example_selector.select_examples({\"input\": test_dict[i]['Question']})\n",
    "\n",
    "    return ex\n",
    "few_shot_prompt = get_example(fewshot_db,k=3)\n",
    "print(few_shot_prompt)\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage , merge_content\n",
    "def concat_context(ex,train_retriever):\n",
    "    for i , entry in enumerate(ex):\n",
    "        print('Q:', i)\n",
    "        context = train_retriever.invoke(entry['Question'])\n",
    "        \n",
    "        ex[i]['Question'] = context[0].page_content + '\\n\\n' + entry['Question']\n",
    "        print(entry['Question'])    \n",
    "    return ex\n",
    "\n",
    "few_shot_prompt = concat_context(few_shot_prompt,train_retriever)\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a wondrous wizard of math.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{context}\\n\\n{input}\"),\n",
    "    ]\n",
    ")\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "#llm = setup_llm_pipeline('yanolja/EEVE-Korean-Instruct-2.8B-v1.0')\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": test_retriver | format_docs,\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    |final_prompt \n",
    "    | llm\n",
    "    \n",
    "    )\n",
    "\n",
    "chain.invoke({\"input\": test_dict[i]['Qusetion']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='ì°½ì—…ì‚¬ì—…í™”ì§€ì› ì‚¬ì—…ì˜ ë²•ë ¹ìƒ ê·¼ê±°ëŠ” ë¬´ì—‡ì¸ê°€ìš”?'), AIMessage(content='ã€Œì¤‘ì†Œê¸°ì—…ì°½ì—… ì§€ì›ë²•ã€ ì œ10ì¡°(ì°½ì—… í™œì„±í™” ì§€ì›ì‚¬ì—…ì˜ ì¶”ì§„ ë“±)ê³¼ ã€Œê¸°ìˆ ì˜ ì´ì „ ë° ì‚¬ì—…í™” ì´‰ì§„ì— ê´€í•œ ë²•ë¥ ã€ ì œ15ì¡°(ê¸°ìˆ ì´ì „ã†ì‚¬ì—…í™” ì´‰ì§„ì‚¬ì—…ì˜ ì¶”ì§„)'), HumanMessage(content='ì¤‘ì†Œë²¤ì²˜ê¸°ì—…ë¶€ì˜ ì°½ì—…ì‚¬ì—…í™”ì§€ì› ì‚¬ì—… ì§‘í–‰ì ˆì°¨ì—ì„œ ì˜ˆë¹„ì°½ì—…ìã†ì°½ì—…ã†ë²¤ì²˜ê¸°ì—…ê³¼ ì£¼ê´€ê¸°ê´€ì´ ê´€ì—¬í•˜ëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?'), AIMessage(content='ì˜ˆë¹„ì°½ì—…ìã†ì°½ì—…ã†ë²¤ì²˜ê¸°ì—…ê³¼ ì£¼ê´€ê¸°ê´€ì€ ì°½ì—… ë° ì‚¬ì—…í™” ë¶€ë¶„ì— ê´€ì—¬í•œë‹¤.'), HumanMessage(content=\"ì–¸ì œ ì²­ë…„ ì¼ìë¦¬ ëŒ€ì±…ì˜ ì¼í™˜ìœ¼ë¡œ ê¸°ìˆ í˜ì‹  ê¸°ë°˜ ì˜ˆë¹„ì°½ì—…ìì˜ ì‚¬ì—…í™”ë¥¼ ì§€ì›í•˜ëŠ” 'ê¸°ìˆ í˜ì‹ í˜• ì°½ì—…ê¸°ì—… ì§€ì›ì‚¬ì—…'ì´ ì‹ ì„¤ë˜ì—ˆë‚˜ìš”?\"), AIMessage(content=\"ê¸°ìˆ í˜ì‹ í˜• ì°½ì—…ê¸°ì—… ì§€ì›ì‚¬ì—…(í˜„, ì˜ˆë¹„ì°½ì—…íŒ¨í‚¤ì§€)'ì€ '18ë…„ì— ì‹ ì„¤ë˜ì—ˆìŠµë‹ˆë‹¤.\")]\n",
      "messages=[HumanMessage(content='2023ë…„ì— ìœµì ê·œëª¨ê°€ ë‘ ë²ˆì§¸ë¡œ í° ë¶„ì•¼ëŠ” ë¬´ì—‡ì´ë©°, ì–´ë–¤ ê·œëª¨ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ì „ì²´ ìœµì ê·œëª¨ ì¤‘ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì€ ì–¼ë§ˆì¸ê°€?'), AIMessage(content='ì‚°ì—…Â·ì¤‘ì†Œê¸°ì—…ë°ì—ë„ˆì§€ ë¶„ì•¼ë¡œ 8ì¡° 4,737ì–µì› ê·œëª¨ì´ë©°, ì „ì²´ ìœµì ê·œëª¨ ì¤‘ 20.3%ë¥¼ ì°¨ì§€í•œë‹¤.'), HumanMessage(content='ê³µê³µë¶€ë¬¸ë¶€ì±„(D3)ëŠ” ì–´ë–¤ ë²”ìœ„ë¥¼ í¬ê´„í•˜ëŠ” ì§€í‘œì¸ê°€?'), AIMessage(content='ê³µê³µë¶€ë¬¸ë¶€ì±„(D3)ëŠ” ì¤‘ì•™ ì •ë¶€ì™€ ì§€ë°©ìì¹˜ë‹¨ì²´(ì§€ë°©êµìœ¡ì¬ì •)ì˜ ëª¨ë“  íšŒê³„Â·ê¸°ê¸ˆ ë° ë¹„ì˜ë¦¬ê³µê³µê¸°ê´€, ë¹„ê¸ˆìœµ ê³µê¸°ì—…ì„ ë¶€ì±„ë¥¼ í¬ê´„í•œë‹¤.'), HumanMessage(content='ì˜ˆì‚°ì´ê³„ë€ ë¬´ì—‡ì„ ë§í•˜ë‚˜ìš”?'), AIMessage(content='ì¤‘ì•™ì •ë¶€ì˜ ì¬ì •ì§€ì¶œ ì „ì²´ ê¸ˆì•¡ì„ ë§í•˜ë©° ì¼ë°˜íšŒê³„, íŠ¹ë³„íšŒê³„ì˜ ì§€ì¶œì„ ì „ë¶€ í•©ì‚°í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚°ì¶œí•¨')]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=fewshot_db,\n",
    "    k=3,\n",
    ")\n",
    "\n",
    "# The prompt template will load examples by passing the input do the `select_examples` method\n",
    "example_selector.select_examples({\"input\": \"horse\"})\n",
    "# Define the few-shot prompt.\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    # The input variables select the values to pass to the example_selector\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    # Define how each example will be formatted.\n",
    "    # In this case, each example will become 2 messages:\n",
    "    # 1 human, and 1 AI\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{Question}\"), (\"ai\", \"{Answer}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "print(few_shot_prompt.invoke(input=f\"{test_dict[i]['Question']}\").to_messages())\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful QA assistant.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print(few_shot_prompt.invoke(input=\"What's 3 ğŸ¦œ 3?\"))\n",
    "chain = final_prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": \"What's 3 ğŸ¦œ 3?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(llm):\n",
    "    chain = (\n",
    "        \n",
    "    )\n",
    "    \n",
    "def run(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    fewshot_db = load_and_vectorize('train.csv', './fewshot_faiss_db')\n",
    "    fewshot_prompt = make_fewshot_prompt(fewshot_db)\n",
    "    \n",
    "    train_db = load_chunks_make_docdb('./train_source', './train_faiss_db')\n",
    "    train_retriever = train_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 1})\n",
    "    \n",
    "    test_db = load_chunks_make_docdb('./test_source', './test_faiss_db')\n",
    "    test_retriver = test_db.as_retriever(search_type = \"mmr\",search_kwargs={'k': 3})\n",
    "    \n",
    "    train_dict = make_dict('train.csv')\n",
    "    test_dict = make_dict('test.csv')\n",
    "    \n",
    "    llm = setup_llm_pipeline(model_id)\n",
    "    results =[]\n",
    "    for i in tqdm(range(len(test_dict))):\n",
    "        print(test_dict[i]['Question'])\n",
    "        fewshot_str = make_fewshot_string(fewshot_prompt, train_retriever, test_dict[i])\n",
    "        # print(fewshot_str)\n",
    "        \n",
    "        \n",
    "        prompt = PromptTemplate.from_template(full_template)\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"context\": test_retriver | format_docs,\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "        answer = qa_chain.invoke(test_dict[i]['Question'])\n",
    "        results.append({\n",
    "            \"Question\": test_dict[i]['Question'],\n",
    "            \"Answer\": qa_chain.invoke(test_dict[i]['Question']),\n",
    "            \"Source\": test_dict[i]['Source']\n",
    "        \n",
    "            })\n",
    "        print(\"================================================\")\n",
    "        print(\"Questions: \",results[-1]['Question'])\n",
    "        print(\"Answer: \",results[-1]['Answer'])\n",
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
