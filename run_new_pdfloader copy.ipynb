{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\llm_titan\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "import torch\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Vector stores\n",
    "import fitz  # PyMuPDF\n",
    "import pdfplumber\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, KonlpyTextSplitter\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader, PDFPlumberLoader ,UnstructuredPDFLoader\n",
    "from langchain_community.retrievers import BM25Retriever, KNNRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "from langchain.docstore.document import Document\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import time\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from konlpy.tag import Kkma\n",
    "# etc\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import logging\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "# intfloat/multilingual-e5-small\n",
    "# jhgan/ko-sroberta-multitask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "def get_embedding():\n",
    "    device = 'cuda' if torch.cuda.is_available() and 'mps' in torch.cuda.get_device_capability() else 'cpu'  # GPU 사용 가능 여부 및 MPS 지원 여부 확인\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name='jhgan/ko-sroberta-multitask',\n",
    "        model_kwargs={'device': device},\n",
    "        encode_kwargs={'normalize_embeddings': True})\n",
    "    return embeddings\n",
    "def normalize_string(s):\n",
    "    try:\n",
    "        normalized = unicodedata.normalize('NFC', s)\n",
    "        return normalized.encode('utf-8', errors='replace').decode('utf-8')\n",
    "    except Exception:\n",
    "        return s\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"�\", \"\").replace(\"\u0003\", \"\")  # 잘못된 인코딩 문자 제거\n",
    "    text = ' '.join(text.split())  # 여러 공백을 하나로 줄임\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_extract_text_from_pdf(pdf_path):\n",
    "    documents = []\n",
    "    try:\n",
    "        # PDF를 이미지로 변환\n",
    "        images = convert_from_path(pdf_path, dpi=500)\n",
    "        \n",
    "        # 각 페이지에서 OCR로 텍스트 추출\n",
    "        for i, image in enumerate(images):\n",
    "            text = pytesseract.image_to_string(image, lang='kor+eng')\n",
    "            text = clean_text(normalize_string(text))\n",
    "            # 각 페이지마다 별도의 Document 생성\n",
    "            documents.append(Document(page_content=text, metadata={\"source\": pdf_path, \"page\": i + 1}))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def load_pdf_file(pdf_file):\n",
    "    try:\n",
    "        # OCR을 이용한 텍스트 추출\n",
    "        documents = ocr_extract_text_from_pdf(pdf_file)\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping corrupted file {pdf_file}: {e}\")\n",
    "        return []\n",
    "\n",
    "def split_documents_with_konlpy(documents):\n",
    "    adjusted_documents = []\n",
    "    konlpy_splitter = KonlpyTextSplitter(separator='\\n\\n')\n",
    "\n",
    "    for doc in documents:\n",
    "        # KonlpyTextSplitter로 문장 단위 분리\n",
    "        sentences = konlpy_splitter.split_text(doc.page_content)\n",
    "        for sentence in sentences:\n",
    "            adjusted_documents.append(Document(page_content=sentence, metadata=doc.metadata))\n",
    "    \n",
    "    return adjusted_documents\n",
    "\n",
    "def make_db(df, chunk_name=\"train\", chunk_size=800, chunk_overlap=60):\n",
    "    documents = []\n",
    "    chunk_file_path = f\"{chunk_name}_chunks.json\"\n",
    "\n",
    "    if os.path.exists(chunk_file_path):\n",
    "        print(f\"Loading chunks from {chunk_file_path}...\")\n",
    "        documents = load_chunks(chunk_file_path)\n",
    "    else:\n",
    "        pdf_files = df['Source_path'].unique()\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = list(tqdm(executor.map(load_pdf_file, pdf_files), total=len(pdf_files), desc=\"Processing PDFs\"))\n",
    "        \n",
    "        for result in results:\n",
    "            documents.extend(result)\n",
    "        \n",
    "        # KonlpyTextSplitter로 문장 단위로 먼저 분할\n",
    "        documents = split_documents_with_konlpy(documents)\n",
    "\n",
    "        # 청크 데이터를 저장\n",
    "        save_chunks(documents, chunk_file_path)\n",
    "    \n",
    "    # make_db에서 RecursiveCharacterTextSplitter로 청크를 분할\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \".\"]\n",
    "    )\n",
    "    documents = recursive_splitter.split_documents(documents)\n",
    "    \n",
    "    faiss = FAISS.from_documents(documents, embedding=get_embedding())\n",
    "    bm = KiwiBM25Retriever.from_documents(documents)\n",
    "    \n",
    "    return faiss, bm\n",
    "\n",
    "def save_chunks(documents, path):\n",
    "    chunk_data = []\n",
    "    for doc in documents:\n",
    "        chunk_data.append({\n",
    "            \"page_content\": doc.page_content,\n",
    "            \"metadata\": doc.metadata\n",
    "        })\n",
    "    \n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunk_data, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"Chunks saved to {path}\")\n",
    "\n",
    "def load_chunks(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        chunk_data = json.load(f)\n",
    "    \n",
    "    documents = []\n",
    "    for chunk in chunk_data:\n",
    "        documents.append(Document(page_content=chunk[\"page_content\"], metadata=chunk[\"metadata\"]))\n",
    "    \n",
    "    print(f\"Chunks loaded from {path}\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_db(df):\n",
    "    df = df.drop('SAMPLE_ID', axis=1)\n",
    "    df = df.drop('Source_path', axis=1)\n",
    "    df = df.to_dict(orient='records')\n",
    "    print(\"Loaded Fewshot Set:\", len(df))\n",
    "    to_vectorize = [\"\\n\\n\".join(normalize_string(value) for value in example.values()) for example in df]\n",
    "    faiss = FAISS.from_texts(to_vectorize, embedding=get_embedding())\n",
    "    # bm = BM25Retriever.from_texts(to_vectorize)\n",
    "    # knn = KNNRetriever.from_texts(to_vectorize, embeddings=get_embedding())\n",
    "    return faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv', encoding='utf-8')\n",
    "test_df = pd.read_csv('test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pypdf 이건 x\n",
    "\n",
    "표가 많기 때문에\n",
    "\n",
    "PyMuPDFLoader 나 PDFPlumberLoader를 쓰고\n",
    "\n",
    "표를 이미지로 불러오기도 하는데 이런경우 추가적인 전처리가 필요함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 하나의 문자열로 포맷팅\"\"\"\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        #context += f\"Document {i+1}\\n\"\n",
    "        doc.page_content = doc.page_content.replace(\"{\", \"(\")\n",
    "        doc.page_content = doc.page_content.replace(\"}\", \")\")\n",
    "        \n",
    "        context += doc.page_content\n",
    "        context += '\\n\\n'\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:03<00:00, 15.78s/it]\n"
     ]
    }
   ],
   "source": [
    "def setup_llm_pipeline(model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n",
    "    # 토크나이저 로드 및 설정\n",
    "        # 양자화 설정 적용\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config,low_cpu_mem_usage=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        temperature = 0.5,\n",
    "        max_new_tokens=1024,\n",
    "        eos_token_id = terminators,\n",
    "        pad_token_id = tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "    return llm\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def template_extractor(df):\n",
    "#     full_template = \"<|begin_of_text|>\"\n",
    "#     full_template += \"\"\"<|start_header_id|>system<|end_header_id|>아래 문장들에서 나타나는 답변 문체들을 찾아주세요.<|eot_id|>\"\"\"\n",
    "#     for i, answer in enumerate(df['Answer']):\n",
    "#         full_template += f\"{answer}<|eot_id|>\"\n",
    "#     #buff_str += f\"<|start_header_id|>assistant<|end_header_id>\"\n",
    "#     template = PromptTemplate(full_template)\n",
    "#     chain =(\n",
    "#         template\n",
    "#         | llm\n",
    "#         | StrOutputParser()\n",
    "#     )\n",
    "#     return chain\n",
    "\n",
    "# chain = template_extractor(train_df).invoke()\n",
    "# print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response):\n",
    "    # AI: 로 시작하는 줄을 찾아 그 이후의 텍스트만 추출\n",
    "    lines = response.split('\\n')\n",
    "    for line in lines:\n",
    "        line = line.replace('**', '')\n",
    "        if line.startswith('Answer:'):\n",
    "            return line.replace('Answer:', '').strip()\n",
    "        if line.startswith('assistant:'):\n",
    "            return line.replace('assistant:', '').strip()\n",
    "    return response.strip()  # AI: 를 찾지 못한 경우 전체 응답을 정리해서 반환\n",
    "\n",
    "def fewshot_ex(fewshot_retriever, train_retriever, q, verbose=False):\n",
    "    query = normalize_string(q)\n",
    "    fewshot_results = fewshot_retriever.invoke(normalize_string(query)) #Document(page_content='중소벤처기업부_창업사업화지원\\n\\n창업사업화지원의 사업목적은 무엇인가?\\n\\n창업사업화지원의 사업목적은 창업기업의 성장단계별, 초격차 분야별, 글로벌화 지원체계를 구축‧운영하여 혁신 기술창업을 활성화하고 창업기업 성장 및 생존율 제고하는 것이다.')\n",
    "    fewshot_str = \"\"\n",
    "    i = 1\n",
    "    for result in fewshot_results:\n",
    "    \n",
    "        result = result.page_content.split('\\n\\n')\n",
    "        buff_str = f\"<|start_header_id|>system<|end_header_id|>Example{i}\\n<|eot_id|>\\n\\n\"\n",
    "        i+=1\n",
    "        buff_str += \"<|start_header_id|>user<|end_header_id|>\"\n",
    "        question = result[1]\n",
    "        buff_str += f\"Question: {question}\\n\\n\"\n",
    "        if train_retriever is not None:\n",
    "            buff_str += f\"Context:\"\n",
    "            docs = train_retriever.invoke(normalize_string(question))\n",
    "            if verbose:\n",
    "                print(\"Fewshot Q |\",len(docs),\"|\",question)\n",
    "            buff_str += format_docs(docs)\n",
    "            buff_str += \"<|eot_id|>\\n\\n\"\n",
    "        else: \n",
    "            buff_str += \"<|eot_id|>\\n\\n\"\n",
    "            if verbose:\n",
    "                print(\"Fewshot Q |\",question)\n",
    "        buff_str += f\"<|start_header_id|>assistant<|end_header_id>{result[2]}<|eot_id|>\"\n",
    "        fewshot_str += buff_str    \n",
    "        \n",
    "    return fewshot_str\n",
    "\n",
    "def run (train,test,fewshot,dataset,llm,verbose=False):\n",
    "    results = []\n",
    "    for i, row in (dataset.iterrows()):\n",
    "\n",
    "        full_template = \"<|begin_of_text|>\"\n",
    "        full_template += \"\"\"<|start_header_id|>system<|end_header_id|>\n",
    "You are the financial expert who helps me with my financial information Q&As.\n",
    "You earn 10 points when you answer me and follow the rules and lose 7 points when you don't.\n",
    "Please use contexts to answer the question.\n",
    "Please your answers should be concise.\n",
    "Please answers must be written in Korean.\n",
    "Please answer in 1-3 sentences.\n",
    "Answer like the question-answer examples given.\n",
    "<|eot_id|>\n",
    "\"\"\"\n",
    "        question = row['Question']        \n",
    "        if verbose:\n",
    "            print(f\"====={i}/{len(dataset)}{'='*255}\")\n",
    "            print(\"Question: \", question)\n",
    "        \n",
    "        fewshot_str = fewshot_ex(fewshot, train, question,verbose)\n",
    "        full_template += fewshot_str\n",
    "        full_template += \"\"\" \"\"\"\n",
    "        contexts = test.invoke(normalize_string(question))\n",
    "        if verbose:\n",
    "            print(\"Context Number|\",len(contexts),end=\"|\")\n",
    "        contexts = format_docs(contexts)\n",
    "        full_template += \"\"\"<|start_header_id|>user<|end_header_id|>Question: {input}\\n\\n\"\"\"\n",
    "        full_template += f\"\"\"Context: {contexts}<|eot_id|>\\n\\n\"\"\"\n",
    "        full_template += \"\"\"<|start_header_id|>assistant<|end_header_id>\"\"\"\n",
    "        \n",
    "        prompt = PromptTemplate(template=full_template, input_variables=[\"input\"])\n",
    "        qa_chain = (\n",
    "        {\n",
    "            \"input\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "        )\n",
    "\n",
    "        answer = qa_chain.invoke(input=question)\n",
    "        answer = extract_answer(answer)\n",
    "        results.append({\n",
    "            \"Question\": question,\n",
    "            \"Answer\": answer,\n",
    "            \"Source\": row['Source']\n",
    "        })\n",
    "        if verbose:\n",
    "            print(\"=====Answer=====\\n\", results[-1]['Answer'])\n",
    "            try:\n",
    "                print(\"=====REAL Answer=====\\n\",row['Answer'])\n",
    "            except:\n",
    "                pass\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def calculate_f1_score(true_sentence, predicted_sentence, sum_mode=True):\n",
    "\n",
    "    #공백 제거\n",
    "    true_sentence = ''.join(true_sentence.split())\n",
    "    predicted_sentence = ''.join(predicted_sentence.split())\n",
    "    \n",
    "    true_counter = Counter(true_sentence)\n",
    "    predicted_counter = Counter(predicted_sentence)\n",
    "\n",
    "    #문자가 등장한 개수도 고려\n",
    "    if sum_mode:\n",
    "        true_positive = sum((true_counter & predicted_counter).values())\n",
    "        predicted_positive = sum(predicted_counter.values())\n",
    "        actual_positive = sum(true_counter.values())\n",
    "\n",
    "    #문자 자체가 있는 것에 focus를 맞춤\n",
    "    else:\n",
    "        true_positive = len((true_counter & predicted_counter).values())\n",
    "        predicted_positive = len(predicted_counter.values())\n",
    "        actual_positive = len(true_counter.values())\n",
    "\n",
    "    #f1 score 계산\n",
    "    precision = true_positive / predicted_positive if predicted_positive > 0 else 0\n",
    "    recall = true_positive / actual_positive if actual_positive > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def calculate_average_f1_score(true_sentences, predicted_sentences):\n",
    "    \n",
    "    total_precision = 0\n",
    "    total_recall = 0\n",
    "    total_f1_score = 0\n",
    "    \n",
    "    for true_sentence, predicted_sentence in zip(true_sentences, predicted_sentences):\n",
    "        precision, recall, f1_score = calculate_f1_score(true_sentence, predicted_sentence)\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1_score += f1_score\n",
    "    \n",
    "    avg_precision = total_precision / len(true_sentences)\n",
    "    avg_recall = total_recall / len(true_sentences)\n",
    "    avg_f1_score = total_f1_score / len(true_sentences)\n",
    "    \n",
    "    return {\n",
    "        'average_precision': avg_precision,\n",
    "        'average_recall': avg_recall,\n",
    "        'average_f1_score': avg_f1_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No chunks. Make chunks ./train_chunks.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PDFs:   0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error extracting text from ./train_source/중소벤처기업부_창업사업화지원.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/국토교통부_소규모주택정비사업.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/고용노동부_내일배움카드(일반).pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/고용노동부_청년일자리창출지원.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/보건복지부_생계급여.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/국토교통부_민간임대(융자).pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/국토교통부_전세임대(융자).pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/보건복지부_노인일자리 및 사회활동지원.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/고용노동부_조기재취업수당.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/「FIS 이슈 & 포커스」 23-3호 《조세지출 연계관리》.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "Error extracting text from ./train_source/「FIS 이슈 & 포커스」 22-3호 《재정융자사업》.pdf: tesseract is not installed or it's not in your PATH. See README file for more information.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import copy\n",
    "\n",
    "weight = [0.5,0.5]\n",
    "train_faiss_db, train_bm_retrievier = make_db(train_df,'./train') \n",
    "\n",
    "train_k = 1\n",
    "train_bm_retrievier.k = train_k\n",
    "#knn_retriever.k = train_k\n",
    "train_faiss_retriever = train_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':train_k} )\n",
    "train_ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[train_bm_retrievier, train_faiss_retriever], weights=weight\n",
    ")\n",
    "\n",
    "test_bm_retrievier = copy.deepcopy(train_bm_retrievier)\n",
    "test_k = 2\n",
    "test_bm_retrievier.k = test_k\n",
    "#test_knn_retriever.k = test_k\n",
    "test_faiss_retriever = train_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':test_k} )\n",
    "test_ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[test_bm_retrievier, test_faiss_retriever], weights=weight\n",
    ")\n",
    "\n",
    "fewshot_k = 3\n",
    "\n",
    "k_folds = 4\n",
    "fold_results = []\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=52)\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(train_df)):\n",
    "    fold_result = []\n",
    "    train_set = train_df.iloc[train_index]\n",
    "    val_set = train_df.iloc[val_index]\n",
    "    \n",
    "    \n",
    "    fewshot_faiss_db = fewshot_db(train_set)\n",
    "    fewshot_faiss_retriever = fewshot_faiss_db.as_retriever(search_kwargs={'k':fewshot_k} )\n",
    "\n",
    "    pred = run(train_ensemble_retriever, test_ensemble_retriever, fewshot_faiss_retriever, val_set, llm, verbose=True)\n",
    "    result = pd.DataFrame()\n",
    "    result['pred'] = [result['Answer'] for result in pred]\n",
    "    val_set.index = range(len(val_set))\n",
    "    result['gt'] = val_set['Answer']\n",
    "        \n",
    "    result = calculate_average_f1_score(result['gt'], result['pred'])\n",
    "    print(result)\n",
    "    fold_results.append(result)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "py pdf 0.663\n",
    "\n",
    "ptmu 0.6740735793987513\n",
    "\n",
    "PDFPlumberLoader 0.6516317411146649 0.655995454987313(0.4), 0.6578699067809993(0.2), 0.6578699067809993(0.1)\n",
    "\n",
    "PDFPlumberLoader NEW:  0.6508706361694363, 0.6545265117992998 (0.4)\n",
    "\n",
    "NEWNEW : 0.6690379419494678(0.5)recursive,chunk1000,overlap200 -> 0.59\n",
    "\n",
    "konlp : 0.639172573877312 chunk 400 \n",
    "\n",
    "0.6502491672498713 chunk 2000 \n",
    "\n",
    "0.6679938815194875 chunk 1000\n",
    "\n",
    "0.6716520173133778 chunk 500 0.5819408919\t\n",
    "\n",
    "====\n",
    "\n",
    "Unstructed \n",
    "\n",
    "0.64 (-)\n",
    "\n",
    "0.6612316276810916 (0.5)\n",
    "\n",
    "0.6578298248291823 (0.3)\n",
    "\n",
    "\n",
    "====\n",
    "\n",
    "Unstructed (New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from save_module import save\n",
    "\n",
    "\n",
    "# weight = [0.5,0.5]\n",
    "# train_faiss_db, train_bm_retrievier = make_db(train_df) \n",
    "# test_faiss_db, test_bm_retrievier = make_db(test_df)\n",
    "# fewshot_faiss_db = fewshot_db(train_df)\n",
    "\n",
    "# train_k = 1\n",
    "# train_bm_retrievier.k = train_k\n",
    "# #knn_retriever.k = train_k\n",
    "# faiss_retriever = train_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':train_k} )\n",
    "# train_ensemble_retriever = EnsembleRetriever(\n",
    "#     retrievers=[train_bm_retrievier, faiss_retriever], weights=weight\n",
    "# )\n",
    "\n",
    "# test_k = 2\n",
    "# test_bm_retrievier.k = test_k\n",
    "# #test_knn_retriever.k = test_k\n",
    "# test_faiss_retriever = test_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':test_k} )\n",
    "# test_ensemble_retriever = EnsembleRetriever(\n",
    "#     retrievers=[test_bm_retrievier, test_faiss_retriever], weights=weight\n",
    "# )\n",
    "\n",
    "# fewshot_k = 3\n",
    "# # fewshot_bm_retrievier.k = fewshot_k\n",
    "# #fewshot_knn_retriever.k = fewshot_k\n",
    "# fewshot_faiss_retriever = fewshot_faiss_db.as_retriever(search_type=\"mmr\",search_kwargs={'k':fewshot_k} )\n",
    "# # fewshot_ensemble_retriever = EnsembleRetriever(\n",
    "# #     retrievers=[fewshot_bm_retrievier, fewshot_faiss_retriever], weights=weight\n",
    "# # )\n",
    "\n",
    "# results = run(train_ensemble_retriever, test_ensemble_retriever, fewshot_faiss_retriever, test_df, llm, verbose=True)\n",
    "# save(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
